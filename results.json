[{"key":"657DUT85","version":1069,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/657DUT85","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/657DUT85","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BQSKVIP7","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"657DUT85","version":1069,"parentItem":"BQSKVIP7","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-03-11T13:03:18Z","url":"https://arxiv.org/abs/2010.16417","note":"","contentType":"text/html","charset":"utf-8","filename":"2010.html","md5":"24f580767864951eab7fe9d1863b7d55","mtime":1647003798000,"tags":[],"relations":{},"dateAdded":"2022-03-11T13:03:18Z","dateModified":"2022-03-11T13:03:18Z"}},{"key":"I27NAXQX","version":1069,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I27NAXQX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I27NAXQX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BQSKVIP7","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"I27NAXQX","version":1069,"parentItem":"BQSKVIP7","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-03-11T13:03:12Z","url":"https://arxiv.org/pdf/2010.16417.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tan et al. - 2020 - MichiGAN Multi-Input-Conditioned Hair Image Gener.pdf","md5":"60ebe78a77d044ec6d6b8b5708e7c6d3","mtime":1647003792000,"tags":[],"relations":{},"dateAdded":"2022-03-11T13:03:12Z","dateModified":"2022-03-11T13:03:12Z"}},{"key":"BQSKVIP7","version":1066,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BQSKVIP7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BQSKVIP7","type":"text/html"}},"meta":{"creatorSummary":"Tan et al.","parsedDate":"2020-10-30","numChildren":3},"data":{"key":"BQSKVIP7","version":1066,"itemType":"journalArticle","title":"MichiGAN: Multi-Input-Conditioned Hair Image Generation for Portrait Editing","creators":[{"creatorType":"author","firstName":"Zhentao","lastName":"Tan"},{"creatorType":"author","firstName":"Menglei","lastName":"Chai"},{"creatorType":"author","firstName":"Dongdong","lastName":"Chen"},{"creatorType":"author","firstName":"Jing","lastName":"Liao"},{"creatorType":"author","firstName":"Qi","lastName":"Chu"},{"creatorType":"author","firstName":"Lu","lastName":"Yuan"},{"creatorType":"author","firstName":"Sergey","lastName":"Tulyakov"},{"creatorType":"author","firstName":"Nenghai","lastName":"Yu"}],"abstractNote":"Despite the recent success of face image generation with GANs, conditional hair editing remains challenging due to the under-explored complexity of its geometry and appearance. In this paper, we present MichiGAN (Multi-Input-Conditioned Hair Image GAN), a novel conditional image generation method for interactive portrait hair manipulation. To provide user control over every major hair visual factor, we explicitly disentangle hair into four orthogonal attributes, including shape, structure, appearance, and background. For each of them, we design a corresponding condition module to represent, process, and convert user inputs, and modulate the image generation pipeline in ways that respect the natures of different visual attributes. All these condition modules are integrated with the backbone generator to form the final end-to-end network, which allows fully-conditioned hair generation from multiple user inputs. Upon it, we also build an interactive portrait hair editing system that enables straightforward manipulation of hair by projecting intuitive and high-level user inputs such as painted masks, guiding strokes, or reference photos to well-defined condition representations. Through extensive experiments and evaluations, we demonstrate the superiority of our method regarding both result quality and user controllability. The code is available at https://github.com/tzt101/MichiGAN.","publicationTitle":"arXiv:2010.16417 [cs]","volume":"","issue":"","pages":"","date":"2020-10-30","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"MichiGAN","url":"http://arxiv.org/abs/2010.16417","accessDate":"2022-03-11T13:02:37Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2010.16417","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-03-11T13:02:37Z","dateModified":"2022-03-11T13:02:37Z"}},{"key":"GJDITDPC","version":1066,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GJDITDPC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GJDITDPC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BQSKVIP7","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GJDITDPC","version":1066,"parentItem":"BQSKVIP7","itemType":"note","note":"Comment: Siggraph 2020, code is available at https://github.com/tzt101/MichiGAN","tags":[],"relations":{},"dateAdded":"2022-03-11T13:02:37Z","dateModified":"2022-03-11T13:02:37Z"}},{"key":"2N5BCYMZ","version":1063,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2N5BCYMZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2N5BCYMZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/C2RR3LNF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2N5BCYMZ","version":1063,"parentItem":"C2RR3LNF","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-03-11T07:27:52Z","url":"https://arxiv.org/abs/2111.15640","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"be00759d4c05b86cd42461e7175df3d6","mtime":1646983672000,"tags":[],"relations":{},"dateAdded":"2022-03-11T07:27:52Z","dateModified":"2022-03-11T07:27:52Z"}},{"key":"Z4QZSTDB","version":1063,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z4QZSTDB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z4QZSTDB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/C2RR3LNF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Z4QZSTDB","version":1063,"parentItem":"C2RR3LNF","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-03-11T07:27:46Z","url":"https://arxiv.org/pdf/2111.15640.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Preechakul et al. - 2022 - Diffusion Autoencoders Toward a Meaningful and De.pdf","md5":"9965c5558098f4ce5bdf2119780164aa","mtime":1646983666000,"tags":[],"relations":{},"dateAdded":"2022-03-11T07:27:46Z","dateModified":"2022-03-11T07:27:46Z"}},{"key":"GNPIQ6CZ","version":1192,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GNPIQ6CZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GNPIQ6CZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/46DDLBL4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GNPIQ6CZ","version":1192,"parentItem":"46DDLBL4","itemType":"attachment","linkMode":"imported_url","title":"Preechakul et al. - 2022 - Diffusion Autoencoders Toward a Meaningful and De.pdf","accessDate":"2022-03-11T07:27:18Z","url":"https://arxiv.org/pdf/2111.15640.pdf?fbclid=IwAR3LEH_6gD8dQLQPDWb65FAXkdsu7bV0sZpMVPRQyjUGa-9PkboFPIc4kDg","note":"","contentType":"application/pdf","charset":"","filename":"Preechakul et al. - 2022 - Diffusion Autoencoders Toward a Meaningful and De.pdf","md5":"9965c5558098f4ce5bdf2119780164aa","mtime":1646983645000,"tags":[],"relations":{},"dateAdded":"2022-03-11T07:27:18Z","dateModified":"2022-06-13T16:20:37Z"}},{"key":"EURKDM4V","version":1192,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EURKDM4V","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EURKDM4V","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/46DDLBL4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EURKDM4V","version":1192,"parentItem":"46DDLBL4","itemType":"note","note":"Comment: Please visit our project page: https://Diff-AE.github.io/","tags":[],"relations":{},"dateAdded":"2022-03-11T07:27:24Z","dateModified":"2022-06-13T16:20:37Z"}},{"key":"FIP3EB3Z","version":1062,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FIP3EB3Z","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FIP3EB3Z","type":"text/html"}},"meta":{"creatorSummary":"Preechakul et al.","parsedDate":"2022-03-09","numChildren":2},"data":{"key":"FIP3EB3Z","version":1062,"itemType":"journalArticle","title":"Diffusion Autoencoders: Toward a Meaningful and Decodable Representation","creators":[{"creatorType":"author","firstName":"Konpat","lastName":"Preechakul"},{"creatorType":"author","firstName":"Nattanat","lastName":"Chatthee"},{"creatorType":"author","firstName":"Suttisak","lastName":"Wizadwongsa"},{"creatorType":"author","firstName":"Supasorn","lastName":"Suwajanakorn"}],"abstractNote":"Diffusion probabilistic models (DPMs) have achieved remarkable quality in image generation that rivals GANs'. But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful representation for other tasks. This paper explores the possibility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level semantics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code, where the first part is semantically meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction. This capability enables challenging applications that currently foil GAN-based methods, such as attribute manipulation on real images. We also show that this two-level encoding improves denoising efficiency and naturally facilitates various downstream tasks including few-shot conditional sampling. Please visit our project page: https://Diff-AE.github.io/","publicationTitle":"arXiv:2111.15640 [cs]","volume":"","issue":"","pages":"","date":"2022-03-09","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"Diffusion Autoencoders","url":"http://arxiv.org/abs/2111.15640","accessDate":"2022-03-11T07:27:24Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.15640","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-03-11T07:27:24Z","dateModified":"2022-03-11T07:27:24Z"}},{"key":"I6AW8RPJ","version":1060,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I6AW8RPJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I6AW8RPJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/C2RR3LNF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"I6AW8RPJ","version":1060,"parentItem":"C2RR3LNF","itemType":"note","note":"Comment: Please visit our project page: https://Diff-AE.github.io/","tags":[],"relations":{},"dateAdded":"2022-03-11T07:25:28Z","dateModified":"2022-03-11T07:25:28Z"}},{"key":"GZRSU6BZ","version":1058,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GZRSU6BZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GZRSU6BZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8LXHEJQB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GZRSU6BZ","version":1058,"parentItem":"8LXHEJQB","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-03-04T05:08:44Z","url":"https://arxiv.org/abs/2202.11833","note":"","contentType":"text/html","charset":"utf-8","filename":"2202.html","md5":"1dada207bfcc77a325e19c9d3f2a0ca4","mtime":1646370524000,"tags":[],"relations":{},"dateAdded":"2022-03-04T05:08:44Z","dateModified":"2022-03-04T05:08:44Z"}},{"key":"K6AQPP6G","version":1058,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/K6AQPP6G","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/K6AQPP6G","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8LXHEJQB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"K6AQPP6G","version":1058,"parentItem":"8LXHEJQB","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-03-04T05:08:35Z","url":"https://arxiv.org/pdf/2202.11833.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Feng et al. - 2022 - Near Perfect GAN Inversion.pdf","md5":"ef22f892940087b894c5087b34a1841e","mtime":1646370515000,"tags":[],"relations":{},"dateAdded":"2022-03-04T05:08:35Z","dateModified":"2022-03-04T05:08:35Z"}},{"key":"8LXHEJQB","version":1056,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8LXHEJQB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8LXHEJQB","type":"text/html"}},"meta":{"creatorSummary":"Feng et al.","parsedDate":"2022-02-23","numChildren":2},"data":{"key":"8LXHEJQB","version":1056,"itemType":"journalArticle","title":"Near Perfect GAN Inversion","creators":[{"creatorType":"author","firstName":"Qianli","lastName":"Feng"},{"creatorType":"author","firstName":"Viraj","lastName":"Shah"},{"creatorType":"author","firstName":"Raghudeep","lastName":"Gadde"},{"creatorType":"author","firstName":"Pietro","lastName":"Perona"},{"creatorType":"author","firstName":"Aleix","lastName":"Martinez"}],"abstractNote":"To edit a real photo using Generative Adversarial Networks (GANs), we need a GAN inversion algorithm to identify the latent vector that perfectly reproduces it. Unfortunately, whereas existing inversion algorithms can synthesize images similar to real photos, they cannot generate the identical clones needed in most applications. Here, we derive an algorithm that achieves near perfect reconstructions of photos. Rather than relying on encoder- or optimization-based methods to find an inverse mapping on a fixed generator $G(\\cdot)$, we derive an approach to locally adjust $G(\\cdot)$ to more optimally represent the photos we wish to synthesize. This is done by locally tweaking the learned mapping $G(\\cdot)$ s.t. $\\| {\\bf x} - G({\\bf z}) \\|<\\epsilon$, with ${\\bf x}$ the photo we wish to reproduce, ${\\bf z}$ the latent vector, $\\|\\cdot\\|$ an appropriate metric, and $\\epsilon > 0$ a small scalar. We show that this approach can not only produce synthetic images that are indistinguishable from the real photos we wish to replicate, but that these images are readily editable. We demonstrate the effectiveness of the derived algorithm on a variety of datasets including human faces, animals, and cars, and discuss its importance for diversity and inclusion.","publicationTitle":"arXiv:2202.11833 [cs]","volume":"","issue":"","pages":"","date":"2022-02-23","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2202.11833","accessDate":"2022-03-04T04:55:23Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2202.11833","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-03-04T04:55:23Z","dateModified":"2022-03-04T04:55:23Z"}},{"key":"NP2XBRT4","version":1053,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NP2XBRT4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NP2XBRT4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LFIP5PLK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NP2XBRT4","version":1053,"parentItem":"LFIP5PLK","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-03-03T09:08:49Z","url":"https://arxiv.org/abs/2112.02475","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"0abb9c3449681952924dd8fe1967469c","mtime":1646298529000,"tags":[],"relations":{},"dateAdded":"2022-03-03T09:08:49Z","dateModified":"2022-03-03T09:08:49Z"}},{"key":"ICWVR6AG","version":1053,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ICWVR6AG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ICWVR6AG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LFIP5PLK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ICWVR6AG","version":1053,"parentItem":"LFIP5PLK","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-03-03T09:08:42Z","url":"https://arxiv.org/pdf/2112.02475.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Whang et al. - 2021 - Deblurring via Stochastic Refinement.pdf","md5":"6466fd4bd0aa1b5d07590d938eb31311","mtime":1646298522000,"tags":[],"relations":{},"dateAdded":"2022-03-03T09:08:42Z","dateModified":"2022-03-03T09:08:42Z"}},{"key":"LFIP5PLK","version":1052,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LFIP5PLK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LFIP5PLK","type":"text/html"}},"meta":{"creatorSummary":"Whang et al.","parsedDate":"2021-12-28","numChildren":2},"data":{"key":"LFIP5PLK","version":1052,"itemType":"journalArticle","title":"Deblurring via Stochastic Refinement","creators":[{"creatorType":"author","firstName":"Jay","lastName":"Whang"},{"creatorType":"author","firstName":"Mauricio","lastName":"Delbracio"},{"creatorType":"author","firstName":"Hossein","lastName":"Talebi"},{"creatorType":"author","firstName":"Chitwan","lastName":"Saharia"},{"creatorType":"author","firstName":"Alexandros G.","lastName":"Dimakis"},{"creatorType":"author","firstName":"Peyman","lastName":"Milanfar"}],"abstractNote":"Image deblurring is an ill-posed problem with multiple plausible solutions for a given input image. However, most existing methods produce a deterministic estimate of the clean image and are trained to minimize pixel-level distortion. These metrics are known to be poorly correlated with human perception, and often lead to unrealistic reconstructions. We present an alternative framework for blind deblurring based on conditional diffusion models. Unlike existing techniques, we train a stochastic sampler that refines the output of a deterministic predictor and is capable of producing a diverse set of plausible reconstructions for a given input. This leads to a significant improvement in perceptual quality over existing state-of-the-art methods across multiple standard benchmarks. Our predict-and-refine approach also enables much more efficient sampling compared to typical diffusion models. Combined with a carefully tuned network architecture and inference procedure, our method is competitive in terms of distortion metrics such as PSNR. These results show clear benefits of our diffusion-based method for deblurring and challenge the widely used strategy of producing a single, deterministic reconstruction.","publicationTitle":"arXiv:2112.02475 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-12-28","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2112.02475","accessDate":"2022-03-03T09:07:46Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.02475","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1}],"collections":[],"relations":{},"dateAdded":"2022-03-03T09:07:46Z","dateModified":"2022-03-03T09:07:46Z"}},{"key":"24Q5IC3H","version":1049,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/24Q5IC3H","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/24Q5IC3H","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/F9DSY2HD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"24Q5IC3H","version":1049,"parentItem":"F9DSY2HD","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-02-18T05:48:40Z","url":"https://arxiv.org/abs/2202.08587","note":"","contentType":"text/html","charset":"utf-8","filename":"2202.html","md5":"f5a6d066e5cb836ac4fa04b7051ccded","mtime":1645163320000,"tags":[],"relations":{},"dateAdded":"2022-02-18T05:48:40Z","dateModified":"2022-02-18T05:48:40Z"}},{"key":"68P45MTL","version":1049,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/68P45MTL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/68P45MTL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/F9DSY2HD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"68P45MTL","version":1049,"parentItem":"F9DSY2HD","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-02-18T05:48:31Z","url":"https://arxiv.org/pdf/2202.08587.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Baydin et al. - 2022 - Gradients without Backpropagation.pdf","md5":"daa9ee81ab383dc4d86cec2a94230c86","mtime":1645163311000,"tags":[],"relations":{},"dateAdded":"2022-02-18T05:48:31Z","dateModified":"2022-02-18T05:48:31Z"}},{"key":"GYEKCIKZ","version":1047,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GYEKCIKZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GYEKCIKZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/F9DSY2HD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GYEKCIKZ","version":1047,"parentItem":"F9DSY2HD","itemType":"note","note":"Comment: 10 pages, 6 figures","tags":[],"relations":{},"dateAdded":"2022-02-18T05:48:06Z","dateModified":"2022-02-18T05:48:06Z"}},{"key":"F9DSY2HD","version":1047,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/F9DSY2HD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/F9DSY2HD","type":"text/html"}},"meta":{"creatorSummary":"Baydin et al.","parsedDate":"2022-02-17","numChildren":3},"data":{"key":"F9DSY2HD","version":1047,"itemType":"journalArticle","title":"Gradients without Backpropagation","creators":[{"creatorType":"author","firstName":"Atılım Güneş","lastName":"Baydin"},{"creatorType":"author","firstName":"Barak A.","lastName":"Pearlmutter"},{"creatorType":"author","firstName":"Don","lastName":"Syme"},{"creatorType":"author","firstName":"Frank","lastName":"Wood"},{"creatorType":"author","firstName":"Philip","lastName":"Torr"}],"abstractNote":"Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differentiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one can compute exactly and efficiently via the forward mode. We call this formulation the forward gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for backpropagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training up to twice as fast in some cases.","publicationTitle":"arXiv:2202.08587 [cs, stat]","volume":"","issue":"","pages":"","date":"2022-02-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2202.08587","accessDate":"2022-02-18T05:48:06Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2202.08587","tags":[{"tag":"68T07","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"I.2.5","type":1},{"tag":"I.2.6","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-02-18T05:48:06Z","dateModified":"2022-02-18T05:48:06Z"}},{"key":"QX44HXHG","version":1044,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QX44HXHG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QX44HXHG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y6BU6MXH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QX44HXHG","version":1044,"parentItem":"Y6BU6MXH","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-02-09T04:20:49Z","url":"https://arxiv.org/abs/2107.02791","note":"","contentType":"text/html","charset":"utf-8","filename":"2107.html","md5":"fd9a3bf6dd46f8ccfc6d307ab5621c63","mtime":1644380449000,"tags":[],"relations":{},"dateAdded":"2022-02-09T04:20:49Z","dateModified":"2022-02-09T04:20:49Z"}},{"key":"W42EQZLQ","version":1044,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/W42EQZLQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/W42EQZLQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y6BU6MXH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"W42EQZLQ","version":1044,"parentItem":"Y6BU6MXH","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-02-09T04:20:41Z","url":"https://arxiv.org/pdf/2107.02791.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Deng et al. - 2021 - Depth-supervised NeRF Fewer Views and Faster Trai.pdf","md5":"d81233fb29f15067789bc907c7dc4057","mtime":1644380441000,"tags":[],"relations":{},"dateAdded":"2022-02-09T04:20:41Z","dateModified":"2022-02-09T04:20:41Z"}},{"key":"JEZZS6DY","version":1040,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JEZZS6DY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JEZZS6DY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MZUIM2PM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JEZZS6DY","version":1040,"parentItem":"MZUIM2PM","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-02-09T04:18:55Z","url":"https://arxiv.org/abs/1907.01341","note":"","contentType":"text/html","charset":"utf-8","filename":"1907.html","md5":"6931cb2ecaec094281f5e3e693d8f3af","mtime":1644380334000,"tags":[],"relations":{},"dateAdded":"2022-02-09T04:18:55Z","dateModified":"2022-02-09T04:18:55Z"}},{"key":"PA29EAJT","version":1040,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PA29EAJT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PA29EAJT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MZUIM2PM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PA29EAJT","version":1040,"parentItem":"MZUIM2PM","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-02-09T04:18:38Z","url":"https://arxiv.org/pdf/1907.01341v3.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Ranftl et al. - 2020 - Towards Robust Monocular Depth Estimation Mixing .pdf","md5":"db79fcfb18b5e04dd98265627900bca9","mtime":1644380318000,"tags":[],"relations":{},"dateAdded":"2022-02-09T04:18:38Z","dateModified":"2022-02-09T04:18:38Z"}},{"key":"A6DR4G2Z","version":1038,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A6DR4G2Z","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A6DR4G2Z","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y6BU6MXH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"A6DR4G2Z","version":1038,"parentItem":"Y6BU6MXH","itemType":"note","note":"Comment: Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub: https://github.com/dunbar12138/DSNeRF","tags":[],"relations":{},"dateAdded":"2022-02-09T04:17:45Z","dateModified":"2022-02-09T04:17:45Z"}},{"key":"Y6BU6MXH","version":1038,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y6BU6MXH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y6BU6MXH","type":"text/html"}},"meta":{"creatorSummary":"Deng et al.","parsedDate":"2021-07-06","numChildren":3},"data":{"key":"Y6BU6MXH","version":1038,"itemType":"journalArticle","title":"Depth-supervised NeRF: Fewer Views and Faster Training for Free","creators":[{"creatorType":"author","firstName":"Kangle","lastName":"Deng"},{"creatorType":"author","firstName":"Andrew","lastName":"Liu"},{"creatorType":"author","firstName":"Jun-Yan","lastName":"Zhu"},{"creatorType":"author","firstName":"Deva","lastName":"Ramanan"}],"abstractNote":"One common failure mode of Neural Radiance Field (NeRF) models is fitting incorrect geometries when given an insufficient number of input views. We propose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning neural radiance fields that takes advantage of readily-available depth supervision. Our key insight is that sparse depth supervision can be used to regularize the learned geometry, a crucial component for effectively rendering novel views using NeRF. We exploit the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as ``free\" depth supervision during training: we simply add a loss to ensure that depth rendered along rays that intersect these 3D points is close to the observed depth. We find that DS-NeRF can render more accurate images given fewer training views while training 2-6x faster. With only two training views on real-world images, DS-NeRF significantly outperforms NeRF as well as other sparse-view variants. We show that our loss is compatible with these NeRF models, demonstrating that depth is a cheap and easily digestible supervisory signal. Finally, we show that DS-NeRF supports other types of depth supervision such as scanned depth sensors and RGBD reconstruction outputs.","publicationTitle":"arXiv:2107.02791 [cs]","volume":"","issue":"","pages":"","date":"2021-07-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Depth-supervised NeRF","url":"http://arxiv.org/abs/2107.02791","accessDate":"2022-02-09T04:17:45Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2107.02791","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-02-09T04:17:45Z","dateModified":"2022-02-09T04:17:45Z"}},{"key":"DNQLKHVS","version":1038,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DNQLKHVS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DNQLKHVS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MZUIM2PM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DNQLKHVS","version":1038,"parentItem":"MZUIM2PM","itemType":"note","note":"Comment: To appear in TPAMI (accepted August 2020)","tags":[],"relations":{},"dateAdded":"2022-02-09T04:17:41Z","dateModified":"2022-02-09T04:17:41Z"}},{"key":"MZUIM2PM","version":1038,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MZUIM2PM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MZUIM2PM","type":"text/html"}},"meta":{"creatorSummary":"Ranftl et al.","parsedDate":"2020-08-25","numChildren":3},"data":{"key":"MZUIM2PM","version":1038,"itemType":"journalArticle","title":"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer","creators":[{"creatorType":"author","firstName":"René","lastName":"Ranftl"},{"creatorType":"author","firstName":"Katrin","lastName":"Lasinger"},{"creatorType":"author","firstName":"David","lastName":"Hafner"},{"creatorType":"author","firstName":"Konrad","lastName":"Schindler"},{"creatorType":"author","firstName":"Vladlen","lastName":"Koltun"}],"abstractNote":"The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer}, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation. Some results are shown in the supplementary video at https://youtu.be/D46FzVyL9I8","publicationTitle":"arXiv:1907.01341 [cs]","volume":"","issue":"","pages":"","date":"2020-08-25","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Towards Robust Monocular Depth Estimation","url":"http://arxiv.org/abs/1907.01341","accessDate":"2022-02-09T04:17:41Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1907.01341\nversion: 3","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-02-09T04:17:41Z","dateModified":"2022-02-09T04:17:41Z"}},{"key":"63D5EUUT","version":1036,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/63D5EUUT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/63D5EUUT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MMEC84GX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"63D5EUUT","version":1036,"parentItem":"MMEC84GX","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-28T06:37:26Z","url":"https://arxiv.org/abs/2008.02401","note":"","contentType":"text/html","charset":"utf-8","filename":"2008.html","md5":"a9e75c2b97fd90e0c574fbd45ba91fad","mtime":1643351846000,"tags":[],"relations":{},"dateAdded":"2022-01-28T06:37:26Z","dateModified":"2022-01-28T06:37:26Z"}},{"key":"2UDXVCXB","version":1036,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2UDXVCXB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2UDXVCXB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MMEC84GX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2UDXVCXB","version":1036,"parentItem":"MMEC84GX","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-28T06:37:18Z","url":"https://arxiv.org/pdf/2008.02401.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Abdal et al. - 2021 - StyleFlow Attribute-conditioned Exploration of St.pdf","md5":"2958733bc1da345c33168dd8ecb43367","mtime":1643351838000,"tags":[],"relations":{},"dateAdded":"2022-01-28T06:37:18Z","dateModified":"2022-01-28T06:37:18Z"}},{"key":"LPU86YBM","version":1033,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LPU86YBM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LPU86YBM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MMEC84GX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LPU86YBM","version":1033,"parentItem":"MMEC84GX","itemType":"note","note":"Comment: \"Project Page https://rameenabdal.github.io/StyleFlow Video: https://youtu.be/LRAUJUn3EqQ \"","tags":[],"relations":{},"dateAdded":"2022-01-28T06:32:00Z","dateModified":"2022-01-28T06:32:00Z"}},{"key":"MMEC84GX","version":1033,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MMEC84GX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MMEC84GX","type":"text/html"}},"meta":{"creatorSummary":"Abdal et al.","parsedDate":"2021-05-06","numChildren":3},"data":{"key":"MMEC84GX","version":1033,"itemType":"journalArticle","title":"StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows","creators":[{"creatorType":"author","firstName":"Rameen","lastName":"Abdal"},{"creatorType":"author","firstName":"Peihao","lastName":"Zhu"},{"creatorType":"author","firstName":"Niloy","lastName":"Mitra"},{"creatorType":"author","firstName":"Peter","lastName":"Wonka"}],"abstractNote":"High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes, while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this paper, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow to other concurrent works.","publicationTitle":"ACM Transactions on Graphics","volume":"40","issue":"3","pages":"1-21","date":"2021-05-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"ACM Trans. Graph.","language":"","DOI":"10.1145/3447648","ISSN":"0730-0301, 1557-7368","shortTitle":"StyleFlow","url":"http://arxiv.org/abs/2008.02401","accessDate":"2022-01-28T06:32:00Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2008.02401","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-28T06:32:00Z","dateModified":"2022-01-28T06:32:00Z"}},{"key":"4AY4TX7N","version":1031,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4AY4TX7N","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4AY4TX7N","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BXRYEFV5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4AY4TX7N","version":1031,"parentItem":"BXRYEFV5","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-27T16:31:25Z","url":"https://arxiv.org/abs/2109.06590","note":"","contentType":"text/html","charset":"utf-8","filename":"2109.html","md5":"edb8bf88327a17745da5947dc381a6ab","mtime":1643301085000,"tags":[],"relations":{},"dateAdded":"2022-01-27T16:31:25Z","dateModified":"2022-01-27T16:31:25Z"}},{"key":"TZWS5A3Q","version":1030,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TZWS5A3Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TZWS5A3Q","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BXRYEFV5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TZWS5A3Q","version":1030,"parentItem":"BXRYEFV5","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-27T16:31:17Z","url":"https://arxiv.org/pdf/2109.06590.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wang et al. - 2021 - High-Fidelity GAN Inversion for Image Attribute Ed.pdf","md5":"4ffedbf993a9a22c7c4d86dfdfa3247d","mtime":1643301077000,"tags":[],"relations":{},"dateAdded":"2022-01-27T16:31:17Z","dateModified":"2022-01-27T16:31:17Z"}},{"key":"Y286G329","version":1183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y286G329","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y286G329","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VGQBFZXW","type":"application/json"}},"meta":{},"data":{"key":"Y286G329","version":1183,"parentItem":"VGQBFZXW","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-27T16:22:43Z","url":"https://arxiv.org/abs/2008.00951","note":"","contentType":"text/html","charset":"utf-8","filename":"2008.html","md5":"dc90b8d5b78db8395bb59e1492f53dc0","mtime":1643300563000,"tags":[],"relations":{},"dateAdded":"2022-01-27T16:22:43Z","dateModified":"2022-06-13T16:19:49Z"}},{"key":"VB9NF52Q","version":1183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VB9NF52Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VB9NF52Q","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VGQBFZXW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VB9NF52Q","version":1183,"parentItem":"VGQBFZXW","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-27T16:22:35Z","url":"https://arxiv.org/pdf/2008.00951.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Richardson et al. - 2021 - Encoding in Style a StyleGAN Encoder for Image-to.pdf","md5":"7b48cea32d95ee3a483af5c078450e30","mtime":1643300555000,"tags":[],"relations":{},"dateAdded":"2022-01-27T16:22:35Z","dateModified":"2022-06-13T16:19:49Z"}},{"key":"JRPBJ5L3","version":1186,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JRPBJ5L3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JRPBJ5L3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/468SDGYY","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JRPBJ5L3","version":1186,"parentItem":"468SDGYY","itemType":"note","note":"Comment: Project Page is at https://tengfei-wang.github.io/HFGI/","tags":[],"relations":{},"dateAdded":"2022-01-27T16:19:41Z","dateModified":"2022-06-13T16:19:47Z"}},{"key":"BXRYEFV5","version":1023,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BXRYEFV5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BXRYEFV5","type":"text/html"}},"meta":{"creatorSummary":"Wang et al.","parsedDate":"2021-09-15","numChildren":3},"data":{"key":"BXRYEFV5","version":1023,"itemType":"journalArticle","title":"High-Fidelity GAN Inversion for Image Attribute Editing","creators":[{"creatorType":"author","firstName":"Tengfei","lastName":"Wang"},{"creatorType":"author","firstName":"Yong","lastName":"Zhang"},{"creatorType":"author","firstName":"Yanbo","lastName":"Fan"},{"creatorType":"author","firstName":"Jue","lastName":"Wang"},{"creatorType":"author","firstName":"Qifeng","lastName":"Chen"}],"abstractNote":"We present a novel high-fidelity generative adversarial network (GAN) inversion framework that enables attribute editing with image-specific details well-preserved (e.g., background, appearance and illumination). We first formulate GAN inversion as a lossy data compression problem and carefully discuss the Rate-Distortion-Edit trade-off. Due to this trade-off, previous works fail to achieve high-fidelity reconstruction while keeping compelling editing ability with a low bit-rate latent code only. In this work, we propose a distortion consultation approach that employs the distortion map as a reference for reconstruction. In the distortion consultation inversion (DCI), the distortion map is first projected to a high-rate latent map, which then complements the basic low-rate latent code with (lost) details via consultation fusion. To achieve high-fidelity editing, we propose an adaptive distortion alignment (ADA) module with a self-supervised training scheme. Extensive experiments in the face and car domains show a clear improvement in terms of both inversion and editing quality.","publicationTitle":"arXiv:2109.06590 [cs]","volume":"","issue":"","pages":"","date":"2021-09-15","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2109.06590","accessDate":"2022-01-27T16:19:41Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2109.06590","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-27T16:19:41Z","dateModified":"2022-01-27T16:19:41Z"}},{"key":"K67SMZS9","version":1183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/K67SMZS9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/K67SMZS9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VGQBFZXW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"K67SMZS9","version":1183,"parentItem":"VGQBFZXW","itemType":"note","note":"Comment: Accepted to CVPR 2021, project page available at https://eladrich.github.io/pixel2style2pixel/","tags":[],"relations":{},"dateAdded":"2022-01-27T16:19:38Z","dateModified":"2022-06-13T16:19:49Z"}},{"key":"DQZYNMYJ","version":1023,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DQZYNMYJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DQZYNMYJ","type":"text/html"}},"meta":{"creatorSummary":"Richardson et al.","parsedDate":"2021-04-21","numChildren":3},"data":{"key":"DQZYNMYJ","version":1023,"itemType":"journalArticle","title":"Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation","creators":[{"creatorType":"author","firstName":"Elad","lastName":"Richardson"},{"creatorType":"author","firstName":"Yuval","lastName":"Alaluf"},{"creatorType":"author","firstName":"Or","lastName":"Patashnik"},{"creatorType":"author","firstName":"Yotam","lastName":"Nitzan"},{"creatorType":"author","firstName":"Yaniv","lastName":"Azar"},{"creatorType":"author","firstName":"Stav","lastName":"Shapiro"},{"creatorType":"author","firstName":"Daniel","lastName":"Cohen-Or"}],"abstractNote":"We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard invert first, edit later methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.","publicationTitle":"arXiv:2008.00951 [cs]","volume":"","issue":"","pages":"","date":"2021-04-21","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Encoding in Style","url":"http://arxiv.org/abs/2008.00951","accessDate":"2022-01-27T16:19:38Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2008.00951","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-27T16:19:38Z","dateModified":"2022-01-27T16:19:38Z"}},{"key":"3DZISXLD","version":1017,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3DZISXLD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3DZISXLD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/25CTNZYU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3DZISXLD","version":1017,"parentItem":"25CTNZYU","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-26T16:09:07Z","url":"https://arxiv.org/abs/2111.15264","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"0be77a7bbebb3ca0a2b8c7182cf984b5","mtime":1643213347000,"tags":[],"relations":{},"dateAdded":"2022-01-26T16:09:07Z","dateModified":"2022-01-26T16:09:07Z"}},{"key":"8NAJLMVR","version":1017,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8NAJLMVR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8NAJLMVR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/25CTNZYU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8NAJLMVR","version":1017,"parentItem":"25CTNZYU","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-26T16:09:02Z","url":"https://arxiv.org/pdf/2111.15264.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Issenhuth et al. - 2021 - EdiBERT, a generative model for image editing.pdf","md5":"20da8443733e68dabf436a0e3c6f332d","mtime":1643213342000,"tags":[],"relations":{},"dateAdded":"2022-01-26T16:09:02Z","dateModified":"2022-01-26T16:09:02Z"}},{"key":"ZJHNI772","version":1186,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZJHNI772","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZJHNI772","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/468SDGYY","type":"application/json"}},"meta":{},"data":{"key":"ZJHNI772","version":1186,"parentItem":"468SDGYY","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-26T16:07:55Z","url":"https://arxiv.org/abs/2109.06590","note":"","contentType":"text/html","charset":"utf-8","filename":"2109.html","md5":"a9b78b9714a43546fbe16a5e1c1839ca","mtime":1643213275000,"tags":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/4AY4TX7N"},"dateAdded":"2022-01-26T16:07:55Z","dateModified":"2022-06-13T16:19:47Z"}},{"key":"SWTW25QL","version":1186,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SWTW25QL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SWTW25QL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/468SDGYY","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SWTW25QL","version":1186,"parentItem":"468SDGYY","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-26T16:07:44Z","url":"https://arxiv.org/pdf/2109.06590.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wang et al. - 2021 - High-Fidelity GAN Inversion for Image Attribute Ed.pdf","md5":"4ffedbf993a9a22c7c4d86dfdfa3247d","mtime":1643213264000,"tags":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/TZWS5A3Q"},"dateAdded":"2022-01-26T16:07:44Z","dateModified":"2022-06-13T16:19:47Z"}},{"key":"25CTNZYU","version":1010,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/25CTNZYU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/25CTNZYU","type":"text/html"}},"meta":{"creatorSummary":"Issenhuth et al.","parsedDate":"2021-11-30","numChildren":2},"data":{"key":"25CTNZYU","version":1010,"itemType":"journalArticle","title":"EdiBERT, a generative model for image editing","creators":[{"creatorType":"author","firstName":"Thibaut","lastName":"Issenhuth"},{"creatorType":"author","firstName":"Ugo","lastName":"Tanielian"},{"creatorType":"author","firstName":"Jérémie","lastName":"Mary"},{"creatorType":"author","firstName":"David","lastName":"Picard"}],"abstractNote":"Advances in computer vision are pushing the limits of im-age manipulation, with generative models sampling detailed images on various tasks. However, a specialized model is often developed and trained for each specific task, even though many image edition tasks share similarities. In denoising, inpainting, or image compositing, one always aims at generating a realistic image from a low-quality one. In this paper, we aim at making a step towards a unified approach for image editing. To do so, we propose EdiBERT, a bi-directional transformer trained in the discrete latent space built by a vector-quantized auto-encoder. We argue that such a bidirectional model is suited for image manipulation since any patch can be re-sampled conditionally to the whole image. Using this unique and straightforward training objective, we show that the resulting model matches state-of-the-art performances on a wide variety of tasks: image denoising, image completion, and image composition.","publicationTitle":"arXiv:2111.15264 [cs]","volume":"","issue":"","pages":"","date":"2021-11-30","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2111.15264","accessDate":"2022-01-26T16:06:17Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.15264","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-26T16:06:18Z","dateModified":"2022-01-26T16:06:18Z"}},{"key":"2799DTHU","version":1010,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2799DTHU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2799DTHU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/468SDGYY","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2799DTHU","version":1010,"parentItem":"468SDGYY","itemType":"note","note":"Comment: Project Page is at https://tengfei-wang.github.io/HFGI/","tags":[],"relations":{},"dateAdded":"2022-01-26T16:06:15Z","dateModified":"2022-01-26T16:06:15Z"}},{"key":"468SDGYY","version":1186,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/468SDGYY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/468SDGYY","type":"text/html"}},"meta":{"creatorSummary":"Wang et al.","parsedDate":"2021-09-15","numChildren":4},"data":{"key":"468SDGYY","version":1186,"itemType":"journalArticle","title":"High-Fidelity GAN Inversion for Image Attribute Editing","creators":[{"creatorType":"author","firstName":"Tengfei","lastName":"Wang"},{"creatorType":"author","firstName":"Yong","lastName":"Zhang"},{"creatorType":"author","firstName":"Yanbo","lastName":"Fan"},{"creatorType":"author","firstName":"Jue","lastName":"Wang"},{"creatorType":"author","firstName":"Qifeng","lastName":"Chen"}],"abstractNote":"We present a novel high-fidelity generative adversarial network (GAN) inversion framework that enables attribute editing with image-specific details well-preserved (e.g., background, appearance and illumination). We first formulate GAN inversion as a lossy data compression problem and carefully discuss the Rate-Distortion-Edit trade-off. Due to this trade-off, previous works fail to achieve high-fidelity reconstruction while keeping compelling editing ability with a low bit-rate latent code only. In this work, we propose a distortion consultation approach that employs the distortion map as a reference for reconstruction. In the distortion consultation inversion (DCI), the distortion map is first projected to a high-rate latent map, which then complements the basic low-rate latent code with (lost) details via consultation fusion. To achieve high-fidelity editing, we propose an adaptive distortion alignment (ADA) module with a self-supervised training scheme. Extensive experiments in the face and car domains show a clear improvement in terms of both inversion and editing quality.","publicationTitle":"arXiv:2109.06590 [cs]","volume":"","issue":"","pages":"","date":"2021-09-15","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2109.06590","accessDate":"2022-01-26T16:06:15Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2109.06590","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/BXRYEFV5"},"dateAdded":"2022-01-26T16:06:15Z","dateModified":"2022-06-13T16:19:47Z"}},{"key":"F3ETVCYF","version":1183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/F3ETVCYF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/F3ETVCYF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZTIX794W","type":"application/json"}},"meta":{},"data":{"key":"F3ETVCYF","version":1183,"parentItem":"ZTIX794W","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-26T15:49:17Z","url":"https://arxiv.org/abs/2011.12799","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"433b49b48f1e16ff2b105211c95c9398","mtime":1643212157000,"tags":[],"relations":{},"dateAdded":"2022-01-26T15:49:17Z","dateModified":"2022-06-13T16:19:55Z"}},{"key":"KSSKCYIV","version":1184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KSSKCYIV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KSSKCYIV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZTIX794W","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KSSKCYIV","version":1184,"parentItem":"ZTIX794W","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-26T15:49:11Z","url":"https://arxiv.org/pdf/2011.12799.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wu et al. - 2020 - StyleSpace Analysis Disentangled Controls for Sty.pdf","md5":"d8675a277d0e0bae263a8ee431f924f9","mtime":1643212151000,"tags":[],"relations":{},"dateAdded":"2022-01-26T15:49:11Z","dateModified":"2022-06-13T16:19:55Z"}},{"key":"LZ62N9WL","version":1184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LZ62N9WL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LZ62N9WL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZTIX794W","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LZ62N9WL","version":1184,"parentItem":"ZTIX794W","itemType":"note","note":"Comment: 25 pages, 21 figures","tags":[],"relations":{},"dateAdded":"2022-01-26T15:45:27Z","dateModified":"2022-06-13T16:19:55Z"}},{"key":"I6AHWQRP","version":1005,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I6AHWQRP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I6AHWQRP","type":"text/html"}},"meta":{"creatorSummary":"Wu et al.","parsedDate":"2020-12-03","numChildren":3},"data":{"key":"I6AHWQRP","version":1005,"itemType":"journalArticle","title":"StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation","creators":[{"creatorType":"author","firstName":"Zongze","lastName":"Wu"},{"creatorType":"author","firstName":"Dani","lastName":"Lischinski"},{"creatorType":"author","firstName":"Eli","lastName":"Shechtman"}],"abstractNote":"We explore and analyze the latent style space of StyleGAN2, a state-of-the-art architecture for image generation, using models pretrained on several different datasets. We first show that StyleSpace, the space of channel-wise style parameters, is significantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to control a distinct visual attribute in a highly localized and disentangled manner. Third, we propose a simple method for identifying style channels that control a specific attribute, using a pretrained classifier or a small number of example images. Manipulation of visual attributes via these StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric. Finally, we demonstrate the applicability of StyleSpace controls to the manipulation of real images. Our findings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces.","publicationTitle":"arXiv:2011.12799 [cs]","volume":"","issue":"","pages":"","date":"2020-12-03","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"StyleSpace Analysis","url":"http://arxiv.org/abs/2011.12799","accessDate":"2022-01-26T15:45:27Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.12799","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-26T15:45:27Z","dateModified":"2022-01-26T15:45:27Z"}},{"key":"39KYF2Z7","version":1005,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/39KYF2Z7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/39KYF2Z7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/I5RXWJNG","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"39KYF2Z7","version":1005,"parentItem":"I5RXWJNG","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-26T15:42:22Z","url":"https://arxiv.org/pdf/2005.09635.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Shen et al. - 2020 - InterFaceGAN Interpreting the Disentangled Face R.pdf","md5":"6b7ace95714348c5b96e65f7e4fefe30","mtime":1643211909000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/FJ4SRLZE"},"dateAdded":"2022-01-26T15:45:08Z","dateModified":"2022-01-26T15:45:08Z"}},{"key":"8XUWLW8D","version":1005,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8XUWLW8D","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8XUWLW8D","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/I5RXWJNG","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8XUWLW8D","version":1005,"parentItem":"I5RXWJNG","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-26T15:42:30Z","url":"https://arxiv.org/abs/2005.09635","note":"","contentType":"text/html","charset":"utf-8","filename":"2005.html","md5":"9b6d192e1b506bfe9af277154a90d855","mtime":1643211909000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/C39MY5AL"},"dateAdded":"2022-01-26T15:45:08Z","dateModified":"2022-01-26T15:45:08Z"}},{"key":"I5RXWJNG","version":1004,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I5RXWJNG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I5RXWJNG","type":"text/html"}},"meta":{"creatorSummary":"Shen et al.","parsedDate":"2020-10-29","numChildren":3},"data":{"key":"I5RXWJNG","version":1004,"itemType":"journalArticle","title":"InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs","creators":[{"creatorType":"author","firstName":"Yujun","lastName":"Shen"},{"creatorType":"author","firstName":"Ceyuan","lastName":"Yang"},{"creatorType":"author","firstName":"Xiaoou","lastName":"Tang"},{"creatorType":"author","firstName":"Bolei","lastName":"Zhou"}],"abstractNote":"Although Generative Adversarial Networks (GANs) have made significant progress in face synthesis, there lacks enough understanding of what GANs have learned in the latent representation to map a random code to a photo-realistic image. In this work, we propose a framework called InterFaceGAN to interpret the disentangled face representation learned by the state-of-the-art GAN models and study the properties of the facial semantics encoded in the latent space. We first find that GANs learn various semantics in some linear subspaces of the latent space. After identifying these subspaces, we can realistically manipulate the corresponding facial attributes without retraining the model. We then conduct a detailed study on the correlation between different semantics and manage to better disentangle them via subspace projection, resulting in more precise control of the attribute manipulation. Besides manipulating the gender, age, expression, and presence of eyeglasses, we can even alter the face pose and fix the artifacts accidentally made by GANs. Furthermore, we perform an in-depth face identity analysis and a layer-wise analysis to evaluate the editing results quantitatively. Finally, we apply our approach to real face editing by employing GAN inversion approaches and explicitly training feed-forward models based on the synthetic data established by InterFaceGAN. Extensive experimental results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable face representation.","publicationTitle":"arXiv:2005.09635 [cs, eess]","volume":"","issue":"","pages":"","date":"2020-10-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"InterFaceGAN","url":"http://arxiv.org/abs/2005.09635","accessDate":"2022-01-26T15:41:42Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2005.09635","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/KGJGKHRV"},"dateAdded":"2022-01-26T15:45:08Z","dateModified":"2022-01-26T15:45:08Z"}},{"key":"2EGPLERR","version":1004,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2EGPLERR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2EGPLERR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/I5RXWJNG","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2EGPLERR","version":1004,"parentItem":"I5RXWJNG","itemType":"note","note":"Comment: Accepted by TPAMI 2020","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/A4JDRQME"},"dateAdded":"2022-01-26T15:45:08Z","dateModified":"2022-01-26T15:45:08Z"}},{"key":"8LMPJ49G","version":999,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8LMPJ49G","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8LMPJ49G","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8I286XFW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8LMPJ49G","version":999,"parentItem":"8I286XFW","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-20T03:55:29Z","url":"https://arxiv.org/abs/2104.07659","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"4c108c097b9414b481f9491b3848aeec","mtime":1642650929000,"tags":[],"relations":{},"dateAdded":"2022-01-20T03:55:29Z","dateModified":"2022-01-20T03:55:29Z"}},{"key":"ZPVNUE9D","version":999,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZPVNUE9D","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZPVNUE9D","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8I286XFW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZPVNUE9D","version":999,"parentItem":"8I286XFW","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-20T03:55:22Z","url":"https://arxiv.org/pdf/2104.07659.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Hao et al. - 2021 - GANcraft Unsupervised 3D Neural Rendering of Mine.pdf","md5":"69be1bb40720750cefa2c6ca4d124c6e","mtime":1642650922000,"tags":[],"relations":{},"dateAdded":"2022-01-20T03:55:22Z","dateModified":"2022-01-20T03:55:22Z"}},{"key":"8I286XFW","version":997,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8I286XFW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8I286XFW","type":"text/html"}},"meta":{"creatorSummary":"Hao et al.","parsedDate":"2021-04-15","numChildren":2},"data":{"key":"8I286XFW","version":997,"itemType":"journalArticle","title":"GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds","creators":[{"creatorType":"author","firstName":"Zekun","lastName":"Hao"},{"creatorType":"author","firstName":"Arun","lastName":"Mallya"},{"creatorType":"author","firstName":"Serge","lastName":"Belongie"},{"creatorType":"author","firstName":"Ming-Yu","lastName":"Liu"}],"abstractNote":"We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ .","publicationTitle":"arXiv:2104.07659 [cs]","volume":"","issue":"","pages":"","date":"2021-04-15","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"GANcraft","url":"http://arxiv.org/abs/2104.07659","accessDate":"2022-01-20T03:54:49Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.07659","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-20T03:54:49Z","dateModified":"2022-01-20T03:54:49Z"}},{"key":"MI6UEZW4","version":995,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MI6UEZW4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MI6UEZW4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6F53TDGI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MI6UEZW4","version":995,"parentItem":"6F53TDGI","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-19T13:32:11Z","url":"https://arxiv.org/abs/2105.08050","note":"","contentType":"text/html","charset":"utf-8","filename":"2105.html","md5":"c9cd3d6c50c4f99b11907b8be383f3f2","mtime":1642599131000,"tags":[],"relations":{},"dateAdded":"2022-01-19T13:32:11Z","dateModified":"2022-01-19T13:32:11Z"}},{"key":"FP6KGL6U","version":995,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FP6KGL6U","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FP6KGL6U","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6F53TDGI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FP6KGL6U","version":995,"parentItem":"6F53TDGI","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-19T13:32:02Z","url":"https://arxiv.org/pdf/2105.08050.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Liu et al. - 2021 - Pay Attention to MLPs.pdf","md5":"5815a2e0a48bf74c797bb235852f30ae","mtime":1642599122000,"tags":[],"relations":{},"dateAdded":"2022-01-19T13:32:02Z","dateModified":"2022-01-19T13:32:02Z"}},{"key":"6F53TDGI","version":992,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6F53TDGI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6F53TDGI","type":"text/html"}},"meta":{"creatorSummary":"Liu et al.","parsedDate":"2021-06-01","numChildren":2},"data":{"key":"6F53TDGI","version":992,"itemType":"journalArticle","title":"Pay Attention to MLPs","creators":[{"creatorType":"author","firstName":"Hanxiao","lastName":"Liu"},{"creatorType":"author","firstName":"Zihang","lastName":"Dai"},{"creatorType":"author","firstName":"David R.","lastName":"So"},{"creatorType":"author","firstName":"Quoc V.","lastName":"Le"}],"abstractNote":"Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.","publicationTitle":"arXiv:2105.08050 [cs]","volume":"","issue":"","pages":"","date":"2021-06-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2105.08050","accessDate":"2022-01-19T13:31:54Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2105.08050","tags":[{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-19T13:31:54Z","dateModified":"2022-01-19T13:31:54Z"}},{"key":"APIJEBRD","version":990,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/APIJEBRD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/APIJEBRD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CSFTSCJG","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"APIJEBRD","version":990,"parentItem":"CSFTSCJG","itemType":"attachment","linkMode":"imported_url","title":"Yang et al. - Progressive Seed Generation Auto-Encoder for Unsup.pdf","accessDate":"2022-01-19T09:25:16Z","url":"https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Progressive_Seed_Generation_Auto-Encoder_for_Unsupervised_Point_Cloud_Learning_ICCV_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Yang et al. - Progressive Seed Generation Auto-Encoder for Unsup.pdf","md5":"b84f20ee7fe43e2632a66b12a6f87db3","mtime":1642584319000,"tags":[],"relations":{},"dateAdded":"2022-01-19T09:25:16Z","dateModified":"2022-01-19T09:25:19Z"}},{"key":"CSFTSCJG","version":989,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CSFTSCJG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CSFTSCJG","type":"text/html"}},"meta":{"creatorSummary":"Yang et al.","numChildren":1},"data":{"key":"CSFTSCJG","version":989,"itemType":"journalArticle","title":"Progressive Seed Generation Auto-Encoder for Unsupervised Point Cloud Learning","creators":[{"creatorType":"author","firstName":"Juyoung","lastName":"Yang"},{"creatorType":"author","firstName":"Pyunghwan","lastName":"Ahn"},{"creatorType":"author","firstName":"Doyeon","lastName":"Kim"},{"creatorType":"author","firstName":"Haeil","lastName":"Lee"},{"creatorType":"author","firstName":"Junmo","lastName":"Kim"}],"abstractNote":"With the development of 3D scanning technologies, 3D vision tasks have become a popular research area. Owing to the large amount of data acquired by sensors, unsupervised learning is essential for understanding and utilizing point clouds without an expensive annotation process. In this paper, we propose a novel framework and an effective auto-encoder architecture named “PSG-Net” for reconstruction-based learning of point clouds. Unlike existing studies that used fixed or random 2D points, our framework generates input-dependent point-wise features for the latent point set. PSG-Net uses the encoded input to produce point-wise features through the seed generation module and extracts richer features in multiple stages with gradually increasing resolution by applying the seed feature propagation module progressively. We prove the effectiveness of PSG-Net experimentally; PSG-Net shows stateof-the-art performances in point cloud reconstruction and unsupervised classification, and achieves comparable performance to counterpart methods in supervised completion.","publicationTitle":"","volume":"","issue":"","pages":"10","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2022-01-19T09:25:19Z","dateModified":"2022-01-19T09:25:19Z"}},{"key":"ETXD22HQ","version":988,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ETXD22HQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ETXD22HQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MN2GZDCC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ETXD22HQ","version":988,"parentItem":"MN2GZDCC","itemType":"attachment","linkMode":"imported_url","title":"Eckart et al. - 2021 - Self-Supervised Learning on 3D Point Clouds by Lea.pdf","accessDate":"2022-01-19T09:24:40Z","url":"https://openaccess.thecvf.com/content/CVPR2021/papers/Eckart_Self-Supervised_Learning_on_3D_Point_Clouds_by_Learning_Discrete_Generative_CVPR_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Eckart et al. - 2021 - Self-Supervised Learning on 3D Point Clouds by Lea.pdf","md5":"5d56903818048fc25f7036bc09a7f51e","mtime":1642584288000,"tags":[],"relations":{},"dateAdded":"2022-01-19T09:24:40Z","dateModified":"2022-01-19T09:24:48Z"}},{"key":"MN2GZDCC","version":987,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MN2GZDCC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MN2GZDCC","type":"text/html"}},"meta":{"creatorSummary":"Eckart et al.","parsedDate":"2021","numChildren":1},"data":{"key":"MN2GZDCC","version":987,"itemType":"conferencePaper","title":"Self-Supervised Learning on 3D Point Clouds by Learning Discrete Generative Models","creators":[{"creatorType":"author","firstName":"Benjamin","lastName":"Eckart"},{"creatorType":"author","firstName":"Wentao","lastName":"Yuan"},{"creatorType":"author","firstName":"Chao","lastName":"Liu"},{"creatorType":"author","firstName":"Jan","lastName":"Kautz"}],"abstractNote":"While recent pre-training tasks on 2D images have proven very successful for transfer learning, pre-training for 3D data remains challenging. In this work, we introduce a general method for 3D self-supervised representation learning that 1) remains agnostic to the underlying neural network architecture, and 2) speciﬁcally leverages the geometric nature of 3D point cloud data. The proposed task softly segments 3D points into a discrete number of geometric partitions. A self-supervised loss is formed under the interpretation that these soft partitions implicitly parameterize a latent Gaussian Mixture Model (GMM), and that this generative model establishes a data likelihood function. Our pretext task can therefore be viewed in terms of an encoder-decoder paradigm that squeezes learned representations through an implicitly deﬁned parametric discrete generative model bottleneck. We show that any existing neural network architecture designed for supervised point cloud segmentation can be repurposed for the proposed unsupervised pretext task. By maximizing data likelihood with respect to the soft partitions formed by the unsupervised point-wise segmentation network, learned representations are encouraged to contain compositionally rich geometric information. In tests, we show that our method naturally induces semantic separation in feature space, resulting in state-of-the-art performance on downstream applications like model classiﬁcation and semantic segmentation.","date":"6/2021","proceedingsTitle":"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","place":"Nashville, TN, USA","publisher":"IEEE","volume":"","pages":"8244-8253","series":"","language":"en","DOI":"10.1109/CVPR46437.2021.00815","ISBN":"978-1-66544-509-2","shortTitle":"","url":"https://ieeexplore.ieee.org/document/9578161/","accessDate":"2022-01-19T09:24:47Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2022-01-19T09:24:47Z","dateModified":"2022-01-19T09:24:47Z"}},{"key":"L4FZZCG6","version":985,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/L4FZZCG6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/L4FZZCG6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DH7WKPRJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"L4FZZCG6","version":985,"parentItem":"DH7WKPRJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-19T09:07:31Z","url":"https://arxiv.org/abs/2112.07945","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"56924755aa0d769d9e89b2a61acb59ac","mtime":1642583251000,"tags":[],"relations":{},"dateAdded":"2022-01-19T09:07:31Z","dateModified":"2022-01-19T09:07:31Z"}},{"key":"QDYV5YGF","version":985,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QDYV5YGF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QDYV5YGF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DH7WKPRJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QDYV5YGF","version":985,"parentItem":"DH7WKPRJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-19T09:07:05Z","url":"https://arxiv.org/pdf/2112.07945.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chan et al. - 2021 - Efficient Geometry-aware 3D Generative Adversarial.pdf","md5":"a602e7e7917c4f748b9a5a7fbf6bfd71","mtime":1642583225000,"tags":[],"relations":{},"dateAdded":"2022-01-19T09:07:05Z","dateModified":"2022-01-19T09:07:05Z"}},{"key":"PKEPSAWE","version":982,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PKEPSAWE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PKEPSAWE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DH7WKPRJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PKEPSAWE","version":982,"parentItem":"DH7WKPRJ","itemType":"note","note":"Comment: Project page: https://matthew-a-chan.github.io/EG3D","tags":[],"relations":{},"dateAdded":"2022-01-19T09:03:18Z","dateModified":"2022-01-19T09:03:18Z"}},{"key":"DH7WKPRJ","version":982,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DH7WKPRJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DH7WKPRJ","type":"text/html"}},"meta":{"creatorSummary":"Chan et al.","parsedDate":"2021-12-15","numChildren":3},"data":{"key":"DH7WKPRJ","version":982,"itemType":"journalArticle","title":"Efficient Geometry-aware 3D Generative Adversarial Networks","creators":[{"creatorType":"author","firstName":"Eric R.","lastName":"Chan"},{"creatorType":"author","firstName":"Connor Z.","lastName":"Lin"},{"creatorType":"author","firstName":"Matthew A.","lastName":"Chan"},{"creatorType":"author","firstName":"Koki","lastName":"Nagano"},{"creatorType":"author","firstName":"Boxiao","lastName":"Pan"},{"creatorType":"author","firstName":"Shalini","lastName":"De Mello"},{"creatorType":"author","firstName":"Orazio","lastName":"Gallo"},{"creatorType":"author","firstName":"Leonidas","lastName":"Guibas"},{"creatorType":"author","firstName":"Jonathan","lastName":"Tremblay"},{"creatorType":"author","firstName":"Sameh","lastName":"Khamis"},{"creatorType":"author","firstName":"Tero","lastName":"Karras"},{"creatorType":"author","firstName":"Gordon","lastName":"Wetzstein"}],"abstractNote":"Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. For this purpose, we introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.","publicationTitle":"arXiv:2112.07945 [cs]","volume":"","issue":"","pages":"","date":"2021-12-15","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2112.07945","accessDate":"2022-01-19T09:03:18Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.07945","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-19T09:03:18Z","dateModified":"2022-01-19T09:03:18Z"}},{"key":"XGRW9TJ3","version":980,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XGRW9TJ3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XGRW9TJ3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5NB6ZAH4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XGRW9TJ3","version":980,"parentItem":"5NB6ZAH4","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-19T08:59:31Z","url":"https://arxiv.org/abs/2101.02691","note":"","contentType":"text/html","charset":"utf-8","filename":"2101.html","md5":"16bb07a4160a06a0b59f3ac5510bfb0a","mtime":1642582771000,"tags":[],"relations":{},"dateAdded":"2022-01-19T08:59:31Z","dateModified":"2022-01-19T08:59:31Z"}},{"key":"C3VNU2PP","version":980,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/C3VNU2PP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/C3VNU2PP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5NB6ZAH4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"C3VNU2PP","version":980,"parentItem":"5NB6ZAH4","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-19T08:59:23Z","url":"https://arxiv.org/pdf/2101.02691.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhang et al. - 2021 - Self-Supervised Pretraining of 3D Features on any .pdf","md5":"56262ea20029c121ec7f6d0090269176","mtime":1642582763000,"tags":[],"relations":{},"dateAdded":"2022-01-19T08:59:23Z","dateModified":"2022-01-19T08:59:23Z"}},{"key":"5NB6ZAH4","version":979,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5NB6ZAH4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5NB6ZAH4","type":"text/html"}},"meta":{"creatorSummary":"Zhang et al.","parsedDate":"2021-01-07","numChildren":2},"data":{"key":"5NB6ZAH4","version":979,"itemType":"journalArticle","title":"Self-Supervised Pretraining of 3D Features on any Point-Cloud","creators":[{"creatorType":"author","firstName":"Zaiwei","lastName":"Zhang"},{"creatorType":"author","firstName":"Rohit","lastName":"Girdhar"},{"creatorType":"author","firstName":"Armand","lastName":"Joulin"},{"creatorType":"author","firstName":"Ishan","lastName":"Misra"}],"abstractNote":"Pretraining on large labeled datasets is a prerequisite to achieve good performance in many computer vision tasks like 2D object recognition, video classification etc. However, pretraining is not widely used for 3D recognition tasks where state-of-the-art methods train models from scratch. A primary reason is the lack of large annotated datasets because 3D data is both difficult to acquire and time consuming to label. We present a simple self-supervised pertaining method that can work with any 3D data - single or multiview, indoor or outdoor, acquired by varied sensors, without 3D registration. We pretrain standard point cloud and voxel based model architectures, and show that joint pretraining further improves performance. We evaluate our models on 9 benchmarks for object detection, semantic segmentation, and object classification, where they achieve state-of-the-art results and can outperform supervised pretraining. We set a new state-of-the-art for object detection on ScanNet (69.0% mAP) and SUNRGBD (63.5% mAP). Our pretrained models are label efficient and improve performance for classes with few examples.","publicationTitle":"arXiv:2101.02691 [cs]","volume":"","issue":"","pages":"","date":"2021-01-07","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2101.02691","accessDate":"2022-01-19T08:59:05Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2101.02691","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-19T08:59:07Z","dateModified":"2022-01-19T08:59:07Z"}},{"key":"P2VDFJJH","version":977,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/P2VDFJJH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/P2VDFJJH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UJNXV3HT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"P2VDFJJH","version":977,"parentItem":"UJNXV3HT","itemType":"attachment","linkMode":"imported_url","title":"Chen et al. - Shape Self-Correction for Unsupervised Point Cloud.pdf","accessDate":"2022-01-19T08:55:36Z","url":"https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Shape_Self-Correction_for_Unsupervised_Point_Cloud_Understanding_ICCV_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chen et al. - Shape Self-Correction for Unsupervised Point Cloud.pdf","md5":"ec2d6dc668b5393e213eb0a8f6c97a20","mtime":1642582545000,"tags":[],"relations":{},"dateAdded":"2022-01-19T08:55:36Z","dateModified":"2022-01-19T08:55:46Z"}},{"key":"UJNXV3HT","version":976,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UJNXV3HT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UJNXV3HT","type":"text/html"}},"meta":{"creatorSummary":"Chen et al.","numChildren":1},"data":{"key":"UJNXV3HT","version":976,"itemType":"journalArticle","title":"Shape Self-Correction for Unsupervised Point Cloud Understanding","creators":[{"creatorType":"author","firstName":"Ye","lastName":"Chen"},{"creatorType":"author","firstName":"Jinxian","lastName":"Liu"},{"creatorType":"author","firstName":"Bingbing","lastName":"Ni"},{"creatorType":"author","firstName":"Hang","lastName":"Wang"},{"creatorType":"author","firstName":"Jiancheng","lastName":"Yang"},{"creatorType":"author","firstName":"Ning","lastName":"Liu"},{"creatorType":"author","firstName":"Teng","lastName":"Li"},{"creatorType":"author","firstName":"Qi","lastName":"Tian"}],"abstractNote":"We develop a novel self-supervised learning method named Shape Self-Correction for point cloud analysis. Our method is motivated by the principle that a good shape representation should be able to find distorted parts of a shape and correct them. To learn strong shape representations in an unsupervised manner, we first design a shapedisorganizing module to destroy certain local shape parts of an object. Then the destroyed shape and the normal shape are sent into a point cloud network to get representations, which are employed to segment points that belong to distorted parts and further reconstruct them to restore the shape to normal. To perform better in these two associated pretext tasks, the network is constrained to capture useful shape features from the object, which indicates that the point cloud network encodes rich geometric and contextual information. The learned feature extractor transfers well to downstream classification and segmentation tasks. Experimental results on ModelNet, ScanNet and ShapeNetPart demonstrate that our method achieves state-of-the-art performance among unsupervised methods. Our framework can be applied to a wide range of deep learning networks for point cloud analysis and we show experimentally that pre-training with our framework significantly boosts the performance of supervised models.","publicationTitle":"","volume":"","issue":"","pages":"10","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2022-01-19T08:55:44Z","dateModified":"2022-01-19T08:55:44Z"}},{"key":"4JTJFRE5","version":973,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4JTJFRE5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4JTJFRE5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4LIEUS6Z","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4JTJFRE5","version":973,"parentItem":"4LIEUS6Z","itemType":"attachment","linkMode":"imported_url","title":"Peng et al. - 2020 - Convolutional Occupancy Networks.pdf","accessDate":"2022-01-19T08:52:33Z","url":"https://arxiv.org/pdf/2003.04618.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Peng et al. - 2020 - Convolutional Occupancy Networks.pdf","md5":"3f9b5382c705d77077aadb9cabb868f9","mtime":1642582366000,"tags":[],"relations":{},"dateAdded":"2022-01-19T08:52:33Z","dateModified":"2022-01-19T08:52:48Z"}},{"key":"4LIEUS6Z","version":971,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4LIEUS6Z","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4LIEUS6Z","type":"text/html"}},"meta":{"creatorSummary":"Peng et al.","parsedDate":"2020-08-01","numChildren":2},"data":{"key":"4LIEUS6Z","version":971,"itemType":"journalArticle","title":"Convolutional Occupancy Networks","creators":[{"creatorType":"author","firstName":"Songyou","lastName":"Peng"},{"creatorType":"author","firstName":"Michael","lastName":"Niemeyer"},{"creatorType":"author","firstName":"Lars","lastName":"Mescheder"},{"creatorType":"author","firstName":"Marc","lastName":"Pollefeys"},{"creatorType":"author","firstName":"Andreas","lastName":"Geiger"}],"abstractNote":"Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fullyconnected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more ﬂexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the eﬀectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically ﬁnd that our method enables the ﬁne-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.","publicationTitle":"arXiv:2003.04618 [cs]","volume":"","issue":"","pages":"","date":"2020-08-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2003.04618","accessDate":"2022-01-19T08:52:38Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2003.04618","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-19T08:52:38Z","dateModified":"2022-01-19T08:52:42Z"}},{"key":"KJJH5TXR","version":971,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KJJH5TXR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KJJH5TXR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4LIEUS6Z","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KJJH5TXR","version":971,"parentItem":"4LIEUS6Z","itemType":"note","note":"Comment: ECCV 2020 (Spotlight). Project page with supplementary material and code: https://pengsongyou.github.io/conv_onet","tags":[],"relations":{},"dateAdded":"2022-01-19T08:52:38Z","dateModified":"2022-01-19T08:52:38Z"}},{"key":"Q35GDZHM","version":968,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Q35GDZHM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Q35GDZHM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/F6FIWD87","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Q35GDZHM","version":968,"parentItem":"F6FIWD87","itemType":"attachment","linkMode":"imported_url","title":"Müller et al. - Instant Neural Graphics Primitives with a Multires.pdf","accessDate":"2022-01-19T08:36:28Z","url":"https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Müller et al. - Instant Neural Graphics Primitives with a Multires.pdf","md5":"483803e96ddae23c068eb5dede9736a8","mtime":1642581399000,"tags":[],"relations":{},"dateAdded":"2022-01-19T08:36:28Z","dateModified":"2022-01-19T08:36:40Z"}},{"key":"F6FIWD87","version":967,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/F6FIWD87","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/F6FIWD87","type":"text/html"}},"meta":{"creatorSummary":"Müller et al.","numChildren":1},"data":{"key":"F6FIWD87","version":967,"itemType":"journalArticle","title":"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding","creators":[{"creatorType":"author","firstName":"Thomas","lastName":"Müller"},{"creatorType":"author","firstName":"Alex","lastName":"Evans"},{"creatorType":"author","firstName":"Christoph","lastName":"Schied"},{"creatorType":"author","firstName":"Alexander","lastName":"Keller"}],"abstractNote":"","publicationTitle":"","volume":"","issue":"","pages":"13","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2022-01-19T08:36:37Z","dateModified":"2022-01-19T08:36:37Z"}},{"key":"VX29AC7E","version":965,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VX29AC7E","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VX29AC7E","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/H6VLYDMW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VX29AC7E","version":965,"parentItem":"H6VLYDMW","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-19T07:30:40Z","url":"https://arxiv.org/abs/2111.05826","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"e5ac523a7b583e72d95a2ab191f961ec","mtime":1642577440000,"tags":[],"relations":{},"dateAdded":"2022-01-19T07:30:40Z","dateModified":"2022-01-19T07:30:40Z"}},{"key":"7C6ZJMVP","version":965,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7C6ZJMVP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7C6ZJMVP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/H6VLYDMW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7C6ZJMVP","version":965,"parentItem":"H6VLYDMW","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-19T07:30:32Z","url":"https://arxiv.org/pdf/2111.05826.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Saharia et al. - 2021 - Palette Image-to-Image Diffusion Models.pdf","md5":"d683ec7e1c204faefb97135ec676ab85","mtime":1642577432000,"tags":[],"relations":{},"dateAdded":"2022-01-19T07:30:32Z","dateModified":"2022-01-19T07:30:32Z"}},{"key":"H6VLYDMW","version":962,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/H6VLYDMW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/H6VLYDMW","type":"text/html"}},"meta":{"creatorSummary":"Saharia et al.","parsedDate":"2021-11-10","numChildren":2},"data":{"key":"H6VLYDMW","version":962,"itemType":"journalArticle","title":"Palette: Image-to-Image Diffusion Models","creators":[{"creatorType":"author","firstName":"Chitwan","lastName":"Saharia"},{"creatorType":"author","firstName":"William","lastName":"Chan"},{"creatorType":"author","firstName":"Huiwen","lastName":"Chang"},{"creatorType":"author","firstName":"Chris A.","lastName":"Lee"},{"creatorType":"author","firstName":"Jonathan","lastName":"Ho"},{"creatorType":"author","firstName":"Tim","lastName":"Salimans"},{"creatorType":"author","firstName":"David J.","lastName":"Fleet"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"}],"abstractNote":"We introduce Palette, a simple and general framework for image-to-image translation using conditional diffusion models. On four challenging image-to-image translation tasks (colorization, inpainting, uncropping, and JPEG decompression), Palette outperforms strong GAN and regression baselines, and establishes a new state of the art. This is accomplished without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss, demonstrating a desirable degree of generality and flexibility. We uncover the impact of using $L_2$ vs. $L_1$ loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention through empirical architecture studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, and report several sample quality scores including FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against reference images for various baselines. We expect this standardized evaluation protocol to play a critical role in advancing image-to-image translation research. Finally, we show that a single generalist Palette model trained on 3 tasks (colorization, inpainting, JPEG decompression) performs as well or better than task-specific specialist counterparts.","publicationTitle":"arXiv:2111.05826 [cs]","volume":"","issue":"","pages":"","date":"2021-11-10","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Palette","url":"http://arxiv.org/abs/2111.05826","accessDate":"2022-01-19T07:30:01Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.05826","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-19T07:30:01Z","dateModified":"2022-01-19T07:30:01Z"}},{"key":"CNZHIFCA","version":960,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CNZHIFCA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CNZHIFCA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/YT4EWW5Q","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CNZHIFCA","version":960,"parentItem":"YT4EWW5Q","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-19T05:47:16Z","url":"https://arxiv.org/abs/2111.14822","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"6aee6114fb4ae24f306aaad54beb2df0","mtime":1642571236000,"tags":[],"relations":{},"dateAdded":"2022-01-19T05:47:16Z","dateModified":"2022-01-19T05:47:16Z"}},{"key":"H4KYLAXH","version":960,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/H4KYLAXH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/H4KYLAXH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/YT4EWW5Q","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"H4KYLAXH","version":960,"parentItem":"YT4EWW5Q","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-19T05:47:09Z","url":"https://arxiv.org/pdf/2111.14822.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Gu et al. - 2021 - Vector Quantized Diffusion Model for Text-to-Image.pdf","md5":"f13e5fb70bfda2f13013eb1b9ef88586","mtime":1642571229000,"tags":[],"relations":{},"dateAdded":"2022-01-19T05:47:09Z","dateModified":"2022-01-19T05:47:09Z"}},{"key":"YT4EWW5Q","version":957,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YT4EWW5Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YT4EWW5Q","type":"text/html"}},"meta":{"creatorSummary":"Gu et al.","parsedDate":"2021-12-20","numChildren":2},"data":{"key":"YT4EWW5Q","version":957,"itemType":"journalArticle","title":"Vector Quantized Diffusion Model for Text-to-Image Synthesis","creators":[{"creatorType":"author","firstName":"Shuyang","lastName":"Gu"},{"creatorType":"author","firstName":"Dong","lastName":"Chen"},{"creatorType":"author","firstName":"Jianmin","lastName":"Bao"},{"creatorType":"author","firstName":"Fang","lastName":"Wen"},{"creatorType":"author","firstName":"Bo","lastName":"Zhang"},{"creatorType":"author","firstName":"Dongdong","lastName":"Chen"},{"creatorType":"author","firstName":"Lu","lastName":"Yuan"},{"creatorType":"author","firstName":"Baining","lastName":"Guo"}],"abstractNote":"We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.","publicationTitle":"arXiv:2111.14822 [cs]","volume":"","issue":"","pages":"","date":"2021-12-20","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2111.14822","accessDate":"2022-01-19T05:46:50Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.14822","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-19T05:46:50Z","dateModified":"2022-01-19T05:46:50Z"}},{"key":"C4YUR6H6","version":955,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/C4YUR6H6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/C4YUR6H6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2APRGQCV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"C4YUR6H6","version":955,"parentItem":"2APRGQCV","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-19T05:18:55Z","url":"https://arxiv.org/abs/2104.03670","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"ee07d380960f08877e08ccdcd13069d8","mtime":1642569535000,"tags":[],"relations":{},"dateAdded":"2022-01-19T05:18:55Z","dateModified":"2022-01-19T05:18:55Z"}},{"key":"MWEIXH99","version":955,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MWEIXH99","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MWEIXH99","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2APRGQCV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MWEIXH99","version":955,"parentItem":"2APRGQCV","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-19T05:18:48Z","url":"https://arxiv.org/pdf/2104.03670.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhou et al. - 2021 - 3D Shape Generation and Completion through Point-V.pdf","md5":"332c179b7f6c0cc74e65e2089ce007cb","mtime":1642569528000,"tags":[],"relations":{},"dateAdded":"2022-01-19T05:18:48Z","dateModified":"2022-01-19T05:18:48Z"}},{"key":"27PW8SCK","version":951,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/27PW8SCK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/27PW8SCK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RRCC4H77","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"27PW8SCK","version":951,"parentItem":"RRCC4H77","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-19T05:15:10Z","url":"https://arxiv.org/abs/2008.06520","note":"","contentType":"text/html","charset":"utf-8","filename":"2008.html","md5":"1dc763c38281bd4c8ed75bba24b19cac","mtime":1642569310000,"tags":[],"relations":{},"dateAdded":"2022-01-19T05:15:10Z","dateModified":"2022-01-19T05:15:10Z"}},{"key":"LXV475SL","version":951,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LXV475SL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LXV475SL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RRCC4H77","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LXV475SL","version":951,"parentItem":"RRCC4H77","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-19T05:15:03Z","url":"https://arxiv.org/pdf/2008.06520.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Cai et al. - 2020 - Learning Gradient Fields for Shape Generation.pdf","md5":"bf94eee3dead116d7eaf2d1ce2d3f5bf","mtime":1642569303000,"tags":[],"relations":{},"dateAdded":"2022-01-19T05:15:03Z","dateModified":"2022-01-19T05:15:03Z"}},{"key":"XW8Q6IXJ","version":950,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XW8Q6IXJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XW8Q6IXJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NB96B96D","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XW8Q6IXJ","version":950,"parentItem":"NB96B96D","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-19T05:14:51Z","url":"https://arxiv.org/abs/2103.01458","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"224448b7b518b897da6167318370fa80","mtime":1642569291000,"tags":[],"relations":{},"dateAdded":"2022-01-19T05:14:51Z","dateModified":"2022-01-19T05:14:51Z"}},{"key":"XHR2VNGC","version":950,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XHR2VNGC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XHR2VNGC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NB96B96D","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XHR2VNGC","version":950,"parentItem":"NB96B96D","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-19T05:14:41Z","url":"https://arxiv.org/pdf/2103.01458.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Luo and Hu - 2021 - Diffusion Probabilistic Models for 3D Point Cloud .pdf","md5":"83273077d1fdcdf394439c308e9290cf","mtime":1642569281000,"tags":[],"relations":{},"dateAdded":"2022-01-19T05:14:41Z","dateModified":"2022-01-19T05:14:41Z"}},{"key":"LBVHR7V7","version":947,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LBVHR7V7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LBVHR7V7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2APRGQCV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LBVHR7V7","version":947,"parentItem":"2APRGQCV","itemType":"note","note":"Comment: Project page: https://alexzhou907.github.io/pvd","tags":[],"relations":{},"dateAdded":"2022-01-19T05:14:10Z","dateModified":"2022-01-19T05:14:10Z"}},{"key":"2APRGQCV","version":947,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2APRGQCV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2APRGQCV","type":"text/html"}},"meta":{"creatorSummary":"Zhou et al.","parsedDate":"2021-08-29","numChildren":3},"data":{"key":"2APRGQCV","version":947,"itemType":"journalArticle","title":"3D Shape Generation and Completion through Point-Voxel Diffusion","creators":[{"creatorType":"author","firstName":"Linqi","lastName":"Zhou"},{"creatorType":"author","firstName":"Yilun","lastName":"Du"},{"creatorType":"author","firstName":"Jiajun","lastName":"Wu"}],"abstractNote":"We propose a novel approach for probabilistic generative modeling of 3D shapes. Unlike most existing models that learn to deterministically translate a latent vector to a shape, our model, Point-Voxel Diffusion (PVD), is a unified, probabilistic formulation for unconditional shape generation and conditional, multi-modal shape completion. PVD marries denoising diffusion models with the hybrid, point-voxel representation of 3D shapes. It can be viewed as a series of denoising steps, reversing the diffusion process from observed point cloud data to Gaussian noise, and is trained by optimizing a variational lower bound to the (conditional) likelihood function. Experiments demonstrate that PVD is capable of synthesizing high-fidelity shapes, completing partial point clouds, and generating multiple completion results from single-view depth scans of real objects.","publicationTitle":"arXiv:2104.03670 [cs]","volume":"","issue":"","pages":"","date":"2021-08-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2104.03670","accessDate":"2022-01-19T05:14:10Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.03670","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-19T05:14:10Z","dateModified":"2022-01-19T05:14:10Z"}},{"key":"NB96B96D","version":947,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NB96B96D","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NB96B96D","type":"text/html"}},"meta":{"creatorSummary":"Luo and Hu","parsedDate":"2021-06-13","numChildren":3},"data":{"key":"NB96B96D","version":947,"itemType":"journalArticle","title":"Diffusion Probabilistic Models for 3D Point Cloud Generation","creators":[{"creatorType":"author","firstName":"Shitong","lastName":"Luo"},{"creatorType":"author","firstName":"Wei","lastName":"Hu"}],"abstractNote":"We present a probabilistic model for point cloud generation, which is fundamental for various 3D vision tasks such as shape completion, upsampling, synthesis and data augmentation. Inspired by the diffusion process in non-equilibrium thermodynamics, we view points in point clouds as particles in a thermodynamic system in contact with a heat bath, which diffuse from the original distribution to a noise distribution. Point cloud generation thus amounts to learning the reverse diffusion process that transforms the noise distribution to the distribution of a desired shape. Specifically, we propose to model the reverse diffusion process for point clouds as a Markov chain conditioned on certain shape latent. We derive the variational bound in closed form for training and provide implementations of the model. Experimental results demonstrate that our model achieves competitive performance in point cloud generation and auto-encoding. The code is available at \\url{https://github.com/luost26/diffusion-point-cloud}.","publicationTitle":"arXiv:2103.01458 [cs]","volume":"","issue":"","pages":"","date":"2021-06-13","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2103.01458","accessDate":"2022-01-19T05:14:06Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.01458","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-19T05:14:06Z","dateModified":"2022-01-19T05:14:06Z"}},{"key":"GXSJCF34","version":947,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GXSJCF34","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GXSJCF34","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NB96B96D","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GXSJCF34","version":947,"parentItem":"NB96B96D","itemType":"note","note":"Comment: Accepted to CVPR 2021","tags":[],"relations":{},"dateAdded":"2022-01-19T05:14:06Z","dateModified":"2022-01-19T05:14:06Z"}},{"key":"3FSRAAHZ","version":947,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3FSRAAHZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3FSRAAHZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RRCC4H77","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3FSRAAHZ","version":947,"parentItem":"RRCC4H77","itemType":"note","note":"Comment: Published in ECCV 2020 (Spotlight); Project page: https://www.cs.cornell.edu/~ruojin/ShapeGF/","tags":[],"relations":{},"dateAdded":"2022-01-19T05:14:03Z","dateModified":"2022-01-19T05:14:03Z"}},{"key":"RRCC4H77","version":947,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RRCC4H77","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RRCC4H77","type":"text/html"}},"meta":{"creatorSummary":"Cai et al.","parsedDate":"2020-08-18","numChildren":3},"data":{"key":"RRCC4H77","version":947,"itemType":"journalArticle","title":"Learning Gradient Fields for Shape Generation","creators":[{"creatorType":"author","firstName":"Ruojin","lastName":"Cai"},{"creatorType":"author","firstName":"Guandao","lastName":"Yang"},{"creatorType":"author","firstName":"Hadar","lastName":"Averbuch-Elor"},{"creatorType":"author","firstName":"Zekun","lastName":"Hao"},{"creatorType":"author","firstName":"Serge","lastName":"Belongie"},{"creatorType":"author","firstName":"Noah","lastName":"Snavely"},{"creatorType":"author","firstName":"Bharath","lastName":"Hariharan"}],"abstractNote":"In this work, we propose a novel technique to generate shapes from point cloud data. A point cloud can be viewed as samples from a distribution of 3D points whose density is concentrated near the surface of the shape. Point cloud generation thus amounts to moving randomly sampled points to high-density areas. We generate point clouds by performing stochastic gradient ascent on an unnormalized probability density, thereby moving sampled points toward the high-likelihood regions. Our model directly predicts the gradient of the log density field and can be trained with a simple objective adapted from score-based generative models. We show that our method can reach state-of-the-art performance for point cloud auto-encoding and generation, while also allowing for extraction of a high-quality implicit surface. Code is available at https://github.com/RuojinCai/ShapeGF.","publicationTitle":"arXiv:2008.06520 [cs]","volume":"","issue":"","pages":"","date":"2020-08-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2008.06520","accessDate":"2022-01-19T05:14:03Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2008.06520","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-19T05:14:03Z","dateModified":"2022-01-19T05:14:03Z"}},{"key":"CL5RB8Z4","version":946,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CL5RB8Z4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CL5RB8Z4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/A955B987","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CL5RB8Z4","version":946,"parentItem":"A955B987","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-12T07:09:51Z","url":"https://arxiv.org/abs/2201.03529","note":"","contentType":"text/html","charset":"utf-8","filename":"2201.html","md5":"e94c131a896db4640ee42715561433e3","mtime":1641971391000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/P9DXVF5X"},"dateAdded":"2022-01-12T07:09:51Z","dateModified":"2022-01-12T07:09:51Z"}},{"key":"XSCDI2NF","version":946,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XSCDI2NF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XSCDI2NF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/A955B987","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XSCDI2NF","version":946,"parentItem":"A955B987","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-12T07:09:43Z","url":"https://arxiv.org/pdf/2201.03529.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Evci et al. - 2022 - Head2Toe Utilizing Intermediate Representations f.pdf","md5":"2f7b558b14205908e0e964b1958b452b","mtime":1641971383000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/JMAV3DTC"},"dateAdded":"2022-01-12T07:09:43Z","dateModified":"2022-01-12T07:09:43Z"}},{"key":"A955B987","version":946,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A955B987","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A955B987","type":"text/html"}},"meta":{"creatorSummary":"Evci et al.","parsedDate":"2022-01-10","numChildren":2},"data":{"key":"A955B987","version":946,"itemType":"journalArticle","title":"Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning","creators":[{"creatorType":"author","firstName":"Utku","lastName":"Evci"},{"creatorType":"author","firstName":"Vincent","lastName":"Dumoulin"},{"creatorType":"author","firstName":"Hugo","lastName":"Larochelle"},{"creatorType":"author","firstName":"Michael C.","lastName":"Mozer"}],"abstractNote":"Transfer-learning methods aim to improve performance in a data-scarce target domain using a model pretrained on a data-rich source domain. A cost-efficient strategy, linear probing, involves freezing the source model and training a new classification head for the target domain. This strategy is outperformed by a more costly but state-of-the-art method -- fine-tuning all parameters of the source model to the target domain -- possibly because fine-tuning allows the model to leverage useful information from intermediate layers which is otherwise discarded by the later pretrained layers. We explore the hypothesis that these intermediate layers might be directly exploited. We propose a method, Head-to-Toe probing (Head2Toe), that selects features from all layers of the source model to train a classification head for the target-domain. In evaluations on the VTAB-1k, Head2Toe matches performance obtained with fine-tuning on average while reducing training and storage cost hundred folds or more, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning.","publicationTitle":"arXiv:2201.03529 [cs]","volume":"","issue":"","pages":"","date":"2022-01-10","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Head2Toe","url":"http://arxiv.org/abs/2201.03529","accessDate":"2022-01-12T07:09:33Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2201.03529","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/8K5RIVZV"},"dateAdded":"2022-01-12T07:09:33Z","dateModified":"2022-01-12T07:09:33Z"}},{"key":"9RI3E9BC","version":934,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9RI3E9BC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9RI3E9BC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DADP6AVI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9RI3E9BC","version":934,"parentItem":"DADP6AVI","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-12T04:29:11Z","url":"https://arxiv.org/abs/2112.10752","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"db9728477a9b8419e83250946f24a4b9","mtime":1641961751000,"tags":[],"relations":{},"dateAdded":"2022-01-12T04:29:11Z","dateModified":"2022-01-12T04:29:11Z"}},{"key":"AF38HE78","version":934,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AF38HE78","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AF38HE78","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DADP6AVI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AF38HE78","version":934,"parentItem":"DADP6AVI","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-12T04:29:03Z","url":"https://arxiv.org/pdf/2112.10752.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Rombach et al. - 2021 - High-Resolution Image Synthesis with Latent Diffus.pdf","md5":"af495e5684eb5f51a4aa4d6b5f006b94","mtime":1641961743000,"tags":[],"relations":{},"dateAdded":"2022-01-12T04:29:03Z","dateModified":"2022-01-12T04:29:03Z"}},{"key":"DADP6AVI","version":930,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DADP6AVI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DADP6AVI","type":"text/html"}},"meta":{"creatorSummary":"Rombach et al.","parsedDate":"2021-12-20","numChildren":2},"data":{"key":"DADP6AVI","version":930,"itemType":"journalArticle","title":"High-Resolution Image Synthesis with Latent Diffusion Models","creators":[{"creatorType":"author","firstName":"Robin","lastName":"Rombach"},{"creatorType":"author","firstName":"Andreas","lastName":"Blattmann"},{"creatorType":"author","firstName":"Dominik","lastName":"Lorenz"},{"creatorType":"author","firstName":"Patrick","lastName":"Esser"},{"creatorType":"author","firstName":"Björn","lastName":"Ommer"}],"abstractNote":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .","publicationTitle":"arXiv:2112.10752 [cs]","volume":"","issue":"","pages":"","date":"2021-12-20","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2112.10752","accessDate":"2022-01-12T04:23:47Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.10752","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-12T04:23:47Z","dateModified":"2022-01-12T04:23:47Z"}},{"key":"2TEIQ77N","version":925,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2TEIQ77N","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2TEIQ77N","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/FUVVAF8X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2TEIQ77N","version":925,"parentItem":"FUVVAF8X","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-12T03:43:05Z","url":"https://arxiv.org/abs/2112.03126","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"dcca827d47420864d5f72c0c5d00c4eb","mtime":1641958985000,"tags":[],"relations":{},"dateAdded":"2022-01-12T03:43:05Z","dateModified":"2022-01-12T04:05:18Z"}},{"key":"HHNP7JIK","version":925,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HHNP7JIK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HHNP7JIK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/FUVVAF8X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HHNP7JIK","version":925,"parentItem":"FUVVAF8X","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-12T03:42:59Z","url":"https://arxiv.org/pdf/2112.03126.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Baranchuk et al. - 2021 - Label-Efficient Semantic Segmentation with Diffusi.pdf","md5":"d5f5374b86d5fdb6a0d400c8d2d2957d","mtime":1641958979000,"tags":[],"relations":{},"dateAdded":"2022-01-12T03:42:59Z","dateModified":"2022-01-12T04:05:18Z"}},{"key":"FUVVAF8X","version":925,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FUVVAF8X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FUVVAF8X","type":"text/html"}},"meta":{"creatorSummary":"Baranchuk et al.","parsedDate":"2021-12-06","numChildren":6},"data":{"key":"FUVVAF8X","version":925,"itemType":"journalArticle","title":"Label-Efficient Semantic Segmentation with Diffusion Models","creators":[{"creatorType":"author","firstName":"Dmitry","lastName":"Baranchuk"},{"creatorType":"author","firstName":"Ivan","lastName":"Rubachev"},{"creatorType":"author","firstName":"Andrey","lastName":"Voynov"},{"creatorType":"author","firstName":"Valentin","lastName":"Khrulkov"},{"creatorType":"author","firstName":"Artem","lastName":"Babenko"}],"abstractNote":"Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic editing. In this paper, we demonstrate that diffusion models can also serve as an instrument for semantic segmentation, especially in the setup when labeled data is scarce. In particular, for several pretrained diffusion models, we investigate the intermediate activations from the networks that perform the Markov step of the reverse diffusion process. We show that these activations effectively capture the semantic information from an input image and appear to be excellent pixel-level representations for the segmentation problem. Based on these observations, we describe a simple segmentation method, which can work even if only a few training images are provided. Our approach significantly outperforms the existing alternatives on several datasets for the same amount of human supervision.","publicationTitle":"arXiv:2112.03126 [cs]","volume":"","issue":"","pages":"","date":"2021-12-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2112.03126","accessDate":"2021-12-08T04:27:37Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.03126\nversion: 1","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/KWUA2MZG","dc:replaces":["http://zotero.org/users/7902311/items/XG4HXCPE","http://zotero.org/users/7902311/items/P4QQG986"]},"dateAdded":"2021-12-08T04:27:37Z","dateModified":"2022-01-12T04:05:18Z"}},{"key":"2A2UWYP5","version":925,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2A2UWYP5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2A2UWYP5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E95EE656","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2A2UWYP5","version":925,"parentItem":"E95EE656","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-12T03:42:15Z","url":"https://arxiv.org/pdf/2112.03145.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wolleb et al. - 2021 - Diffusion Models for Implicit Image Segmentation E.pdf","md5":"efe4aa9790cd67dfbfdee1d9e4b11760","mtime":1641958935000,"tags":[],"relations":{},"dateAdded":"2022-01-12T03:42:15Z","dateModified":"2022-01-12T04:05:17Z"}},{"key":"3EJQNLLT","version":925,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3EJQNLLT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3EJQNLLT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E95EE656","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3EJQNLLT","version":925,"parentItem":"E95EE656","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-12T03:42:25Z","url":"https://arxiv.org/abs/2112.03145","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"279b3416106dcd222a2637dbd6b23a3d","mtime":1641958945000,"tags":[],"relations":{},"dateAdded":"2022-01-12T03:42:25Z","dateModified":"2022-01-12T04:05:17Z"}},{"key":"4XZZXRMC","version":925,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4XZZXRMC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4XZZXRMC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E95EE656","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4XZZXRMC","version":925,"parentItem":"E95EE656","itemType":"note","note":"Comment: In this version, we updated the results section with more detailed evaluations","tags":[],"relations":{},"dateAdded":"2022-01-12T03:42:03Z","dateModified":"2022-01-12T04:05:17Z"}},{"key":"E95EE656","version":925,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/E95EE656","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/E95EE656","type":"text/html"}},"meta":{"creatorSummary":"Wolleb et al.","parsedDate":"2021-12-06","numChildren":5},"data":{"key":"E95EE656","version":925,"itemType":"journalArticle","title":"Diffusion Models for Implicit Image Segmentation Ensembles","creators":[{"creatorType":"author","firstName":"Julia","lastName":"Wolleb"},{"creatorType":"author","firstName":"Robin","lastName":"Sandkühler"},{"creatorType":"author","firstName":"Florentin","lastName":"Bieder"},{"creatorType":"author","firstName":"Philippe","lastName":"Valmaggia"},{"creatorType":"author","firstName":"Philippe C.","lastName":"Cattin"}],"abstractNote":"Diffusion models have shown impressive performance for generative modelling of images. In this paper, we present a novel semantic segmentation method based on diffusion models. By modifying the training and sampling scheme, we show that diffusion models can perform lesion segmentation of medical images. To generate an image specific segmentation, we train the model on the ground truth segmentation, and use the image as a prior during training and in every step during the sampling process. With the given stochastic sampling process, we can generate a distribution of segmentation masks. This property allows us to compute pixel-wise uncertainty maps of the segmentation, and allows an implicit ensemble of segmentations that increases the segmentation performance. We evaluate our method on the BRATS2020 dataset for brain tumor segmentation. Compared to state-of-the-art segmentation models, our approach yields good segmentation results and, additionally, meaningful uncertainty maps.","publicationTitle":"arXiv:2112.03145 [cs]","volume":"","issue":"","pages":"","date":"2021-12-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2112.03145","accessDate":"2021-12-08T13:45:24Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.03145\nversion: 1","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/PK7FF5DG"},"dateAdded":"2021-12-08T13:45:29Z","dateModified":"2022-01-12T04:05:17Z"}},{"key":"P83PPYNH","version":926,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/P83PPYNH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/P83PPYNH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IBG26726","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"P83PPYNH","version":926,"parentItem":"IBG26726","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-12T03:42:55Z","url":"https://arxiv.org/abs/2112.05149","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"dee7f3c7e9086bd1256a8ac593f0bb7d","mtime":1641958975000,"tags":[],"relations":{},"dateAdded":"2022-01-12T03:42:55Z","dateModified":"2022-01-12T04:05:16Z"}},{"key":"IYTSRSDW","version":926,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IYTSRSDW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IYTSRSDW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IBG26726","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"IYTSRSDW","version":926,"parentItem":"IBG26726","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-12T03:42:50Z","url":"https://arxiv.org/pdf/2112.05149.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kim et al. - 2021 - DiffuseMorph Unsupervised Deformable Image Regist.pdf","md5":"6b4e9ec87627fa4cd6753ea7017ee1c7","mtime":1641958970000,"tags":[],"relations":{},"dateAdded":"2022-01-12T03:42:50Z","dateModified":"2022-01-12T04:05:16Z"}},{"key":"IBG26726","version":926,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IBG26726","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IBG26726","type":"text/html"}},"meta":{"creatorSummary":"Kim et al.","parsedDate":"2021-12-09","numChildren":4},"data":{"key":"IBG26726","version":926,"itemType":"journalArticle","title":"DiffuseMorph: Unsupervised Deformable Image Registration Along Continuous Trajectory Using Diffusion Models","creators":[{"creatorType":"author","firstName":"Boah","lastName":"Kim"},{"creatorType":"author","firstName":"Inhwa","lastName":"Han"},{"creatorType":"author","firstName":"Jong Chul","lastName":"Ye"}],"abstractNote":"Deformable image registration is one of the fundamental tasks for medical imaging and computer vision. Classical registration algorithms usually rely on iterative optimization approaches to provide accurate deformation, which requires high computational cost. Although many deep-learning-based methods have been developed to carry out fast image registration, it is still challenging to estimate the deformation field with less topological folding problem. Furthermore, these approaches only enable registration to a single fixed image, and it is not possible to obtain continuously varying registration results between the moving and fixed images. To address this, here we present a novel approach of diffusion model-based probabilistic image registration, called DiffuseMorph. Specifically, our model learns the score function of the deformation between moving and fixed images. Similar to the existing diffusion models, DiffuseMorph not only provides synthetic deformed images through a reverse diffusion process, but also enables various levels of deformation of the moving image along with the latent space. Experimental results on 2D face expression image and 3D brain image registration tasks demonstrate that our method can provide flexible and accurate deformation with a capability of topology preservation.","publicationTitle":"arXiv:2112.05149 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-12-09","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"DiffuseMorph","url":"http://arxiv.org/abs/2112.05149","accessDate":"2021-12-20T05:52:19Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.05149\nversion: 1","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/ASH3ZHT5"},"dateAdded":"2021-12-20T05:52:19Z","dateModified":"2022-01-12T04:05:16Z"}},{"key":"7HEHTWU3","version":926,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7HEHTWU3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7HEHTWU3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GH9CK93P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7HEHTWU3","version":926,"parentItem":"GH9CK93P","itemType":"note","note":"Comment: Project page https://xh-liu.github.io/sdg/","tags":[],"relations":{},"dateAdded":"2022-01-12T03:43:22Z","dateModified":"2022-01-12T04:05:15Z"}},{"key":"A3Q9K8HE","version":925,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A3Q9K8HE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A3Q9K8HE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GH9CK93P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"A3Q9K8HE","version":925,"parentItem":"GH9CK93P","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-12T03:44:04Z","url":"https://arxiv.org/abs/2112.05744","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"5dc2408742ce7a672da8dc1f5220f6c4","mtime":1641959044000,"tags":[],"relations":{},"dateAdded":"2022-01-12T03:44:04Z","dateModified":"2022-01-12T04:05:15Z"}},{"key":"Y8D7GXSL","version":925,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y8D7GXSL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y8D7GXSL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GH9CK93P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Y8D7GXSL","version":925,"parentItem":"GH9CK93P","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-12T03:43:58Z","url":"https://arxiv.org/pdf/2112.05744.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Liu et al. - 2021 - More Control for Free! Image Synthesis with Semant.pdf","md5":"cf0b89c22bdbfbdae1737350044c4c1e","mtime":1641959038000,"tags":[],"relations":{},"dateAdded":"2022-01-12T03:43:58Z","dateModified":"2022-01-12T04:05:15Z"}},{"key":"GH9CK93P","version":925,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GH9CK93P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GH9CK93P","type":"text/html"}},"meta":{"creatorSummary":"Liu et al.","parsedDate":"2021-12-14","numChildren":6},"data":{"key":"GH9CK93P","version":925,"itemType":"journalArticle","title":"More Control for Free! Image Synthesis with Semantic Diffusion Guidance","creators":[{"creatorType":"author","firstName":"Xihui","lastName":"Liu"},{"creatorType":"author","firstName":"Dong Huk","lastName":"Park"},{"creatorType":"author","firstName":"Samaneh","lastName":"Azadi"},{"creatorType":"author","firstName":"Gong","lastName":"Zhang"},{"creatorType":"author","firstName":"Arman","lastName":"Chopikyan"},{"creatorType":"author","firstName":"Yuxiao","lastName":"Hu"},{"creatorType":"author","firstName":"Humphrey","lastName":"Shi"},{"creatorType":"author","firstName":"Anna","lastName":"Rohrbach"},{"creatorType":"author","firstName":"Trevor","lastName":"Darrell"}],"abstractNote":"Controllable image synthesis models allow creation of diverse images based on text instructions or guidance from an example image. Recently, denoising diffusion probabilistic models have been shown to generate more realistic imagery than prior methods, and have been successfully demonstrated in unconditional and class-conditional settings. We explore fine-grained, continuous control of this model class, and introduce a novel unified framework for semantic diffusion guidance, which allows either language or image guidance, or both. Guidance is injected into a pretrained unconditional diffusion model using the gradient of image-text or image matching scores. We explore CLIP-based textual guidance as well as both content and style-based image guidance in a unified form. Our text-guided synthesis approach can be applied to datasets without associated text annotations. We conduct experiments on FFHQ and LSUN datasets, and show results on fine-grained text-guided image synthesis, synthesis of images related to a style or content example image, and examples with both textual and image guidance.","publicationTitle":"arXiv:2112.05744 [cs]","volume":"","issue":"","pages":"","date":"2021-12-14","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2112.05744","accessDate":"2021-12-20T05:46:17Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.05744\nversion: 2","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/N55Q6EF5","dc:replaces":"http://zotero.org/users/7902311/items/987HDN58"},"dateAdded":"2021-12-20T05:46:17Z","dateModified":"2022-01-12T04:05:15Z"}},{"key":"BMI4V5ZU","version":920,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BMI4V5ZU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BMI4V5ZU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BNAD7E38","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BMI4V5ZU","version":920,"parentItem":"BNAD7E38","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-01-12T03:43:09Z","url":"https://arxiv.org/abs/2112.05146","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"dd08fb1e2a03b3377bae157ab582db10","mtime":1641958989000,"tags":[],"relations":{},"dateAdded":"2022-01-12T03:43:09Z","dateModified":"2022-01-12T03:43:09Z"}},{"key":"RC5FQRPF","version":920,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RC5FQRPF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RC5FQRPF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BNAD7E38","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RC5FQRPF","version":920,"parentItem":"BNAD7E38","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-01-12T03:43:03Z","url":"https://arxiv.org/pdf/2112.05146.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chung et al. - 2021 - Come-Closer-Diffuse-Faster Accelerating Condition.pdf","md5":"052d86ca5beba33ce94554efb5c6a425","mtime":1641958983000,"tags":[],"relations":{},"dateAdded":"2022-01-12T03:43:03Z","dateModified":"2022-01-12T03:43:03Z"}},{"key":"BNAD7E38","version":912,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BNAD7E38","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BNAD7E38","type":"text/html"}},"meta":{"creatorSummary":"Chung et al.","parsedDate":"2021-12-08","numChildren":2},"data":{"key":"BNAD7E38","version":912,"itemType":"journalArticle","title":"Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction","creators":[{"creatorType":"author","firstName":"Hyungjin","lastName":"Chung"},{"creatorType":"author","firstName":"Byeongsu","lastName":"Sim"},{"creatorType":"author","firstName":"Jong Chul","lastName":"Ye"}],"abstractNote":"Diffusion models have recently attained significant interest within the community owing to their strong performance as generative models. Furthermore, its application to inverse problems have demonstrated state-of-the-art performance. Unfortunately, diffusion models have a critical downside - they are inherently slow to sample from, needing few thousand steps of iteration to generate images from pure Gaussian noise. In this work, we show that starting from Gaussian noise is unnecessary. Instead, starting from a single forward diffusion with better initialization significantly reduces the number of sampling steps in the reverse conditional diffusion. This phenomenon is formally explained by the contraction theory of the stochastic difference equations like our conditional diffusion strategy - the alternating applications of reverse diffusion followed by a non-expansive data consistency step. The new sampling strategy, dubbed Come-Closer-Diffuse-Faster (CCDF), also reveals a new insight on how the existing feed-forward neural network approaches for inverse problems can be synergistically combined with the diffusion models. Experimental results with super-resolution, image inpainting, and compressed sensing MRI demonstrate that our method can achieve state-of-the-art reconstruction performance at significantly reduced sampling steps.","publicationTitle":"arXiv:2112.05146 [cs, eess, stat]","volume":"","issue":"","pages":"","date":"2021-12-08","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Come-Closer-Diffuse-Faster","url":"http://arxiv.org/abs/2112.05146","accessDate":"2022-01-12T03:42:07Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.05146","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-01-12T03:42:07Z","dateModified":"2022-01-12T03:42:07Z"}},{"key":"CIV93AGA","version":907,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CIV93AGA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CIV93AGA","type":"text/html"}},"meta":{"numChildren":0},"data":{"key":"CIV93AGA","version":907,"itemType":"webpage","title":"Yang Song | Generative Modeling by Estimating Gradients of the Data Distribution","creators":[],"abstractNote":"","websiteTitle":"","websiteType":"","date":"","shortTitle":"","url":"https://yang-song.github.io/blog/2021/score/?fbclid=IwAR1Pffx89cXKvuwTvZXUXm0lt5iLJ_FMLUHdogOqZm3wI_R46ELWKIDAwbY","accessDate":"2021-12-24T18:05:42Z","language":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-12-24T18:05:42Z","dateModified":"2021-12-24T18:05:42Z"}},{"key":"G4QHHYPR","version":897,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/G4QHHYPR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/G4QHHYPR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GCAWP9ZZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"G4QHHYPR","version":897,"parentItem":"GCAWP9ZZ","itemType":"note","note":"Comment: Project page: https://matthew-a-chan.github.io/EG3D","tags":[],"relations":{},"dateAdded":"2021-12-22T07:37:58Z","dateModified":"2021-12-22T07:42:02Z"}},{"key":"6KGKLRID","version":898,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6KGKLRID","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6KGKLRID","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E8Q7JRHQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6KGKLRID","version":898,"parentItem":"E8Q7JRHQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-22T07:41:34Z","url":"https://arxiv.org/abs/2110.09788","note":"","contentType":"text/html","charset":"utf-8","filename":"2110.html","md5":"67b9d149762b4c2a9efcc16af351e1f2","mtime":1640158894000,"tags":[],"relations":{},"dateAdded":"2021-12-22T07:41:34Z","dateModified":"2021-12-22T07:41:34Z"}},{"key":"3IUAERKW","version":898,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3IUAERKW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3IUAERKW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E8Q7JRHQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3IUAERKW","version":898,"parentItem":"E8Q7JRHQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-22T07:41:30Z","url":"https://arxiv.org/pdf/2110.09788.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhou et al. - 2021 - CIPS-3D A 3D-Aware Generator of GANs Based on Con.pdf","md5":"045ec9e25b382a0249e56f72912b4655","mtime":1640158890000,"tags":[],"relations":{},"dateAdded":"2021-12-22T07:41:30Z","dateModified":"2021-12-22T07:41:30Z"}},{"key":"FPZPWLXV","version":897,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FPZPWLXV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FPZPWLXV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2UZAZ3TR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FPZPWLXV","version":897,"parentItem":"2UZAZ3TR","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-22T07:40:57Z","url":"https://arxiv.org/abs/2112.08867","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"f75fee54d00259d2ed513c1b28982a86","mtime":1640158857000,"tags":[],"relations":{},"dateAdded":"2021-12-22T07:40:57Z","dateModified":"2021-12-22T07:40:57Z"}},{"key":"E8Q7JRHQ","version":896,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/E8Q7JRHQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/E8Q7JRHQ","type":"text/html"}},"meta":{"creatorSummary":"Zhou et al.","parsedDate":"2021-10-19","numChildren":3},"data":{"key":"E8Q7JRHQ","version":896,"itemType":"journalArticle","title":"CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis","creators":[{"creatorType":"author","firstName":"Peng","lastName":"Zhou"},{"creatorType":"author","firstName":"Lingxi","lastName":"Xie"},{"creatorType":"author","firstName":"Bingbing","lastName":"Ni"},{"creatorType":"author","firstName":"Qi","lastName":"Tian"}],"abstractNote":"The style-based GAN (StyleGAN) architecture achieved state-of-the-art results for generating high-quality images, but it lacks explicit and precise control over camera poses. The recently proposed NeRF-based GANs made great progress towards 3D-aware generators, but they are unable to generate high-quality images yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that is composed of a shallow NeRF network and a deep implicit neural representation (INR) network. The generator synthesizes each pixel value independently without any spatial convolution or upsampling operation. In addition, we diagnose the problem of mirror symmetry that implies a suboptimal solution and solve it by introducing an auxiliary discriminator. Trained on raw, single-view images, CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of 6.97 for images at the $256\\times256$ resolution on FFHQ. We also demonstrate several interesting directions for CIPS-3D such as transfer learning and 3D-aware face stylization. The synthesis results are best viewed as videos, so we recommend the readers to check our github project at https://github.com/PeterouZh/CIPS-3D","publicationTitle":"arXiv:2110.09788 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-10-19","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"CIPS-3D","url":"http://arxiv.org/abs/2110.09788","accessDate":"2021-12-22T07:40:54Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2110.09788","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-22T07:40:54Z","dateModified":"2021-12-22T07:40:54Z"}},{"key":"GQ8P3EPV","version":896,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GQ8P3EPV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GQ8P3EPV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E8Q7JRHQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GQ8P3EPV","version":896,"parentItem":"E8Q7JRHQ","itemType":"note","note":"Comment: 3D-aware GANs based on NeRF, https://github.com/PeterouZh/CIPS-3D","tags":[],"relations":{},"dateAdded":"2021-12-22T07:40:54Z","dateModified":"2021-12-22T07:40:54Z"}},{"key":"DW7LFTJC","version":897,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DW7LFTJC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DW7LFTJC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2UZAZ3TR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DW7LFTJC","version":897,"parentItem":"2UZAZ3TR","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-22T07:40:50Z","url":"https://arxiv.org/pdf/2112.08867.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Deng et al. - 2021 - GRAM Generative Radiance Manifolds for 3D-Aware I.pdf","md5":"c74fb1b0fa0d7cae7fc78c9f7c02edc8","mtime":1640158850000,"tags":[],"relations":{},"dateAdded":"2021-12-22T07:40:50Z","dateModified":"2021-12-22T07:40:50Z"}},{"key":"PDRXHPNY","version":896,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PDRXHPNY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PDRXHPNY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/3XRYJRBB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PDRXHPNY","version":896,"parentItem":"3XRYJRBB","itemType":"attachment","linkMode":"imported_url","title":"Kim - SuperStyleNet Deep Image Synthesis with Superpixe.pdf","accessDate":"2021-12-22T07:39:08Z","url":"https://www.bmvc2021-virtualconference.com/assets/papers/0051.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kim - SuperStyleNet Deep Image Synthesis with Superpixe.pdf","md5":"e96bb7d25cf05606a625d9483126b2c8","mtime":1640158750000,"tags":[],"relations":{},"dateAdded":"2021-12-22T07:39:08Z","dateModified":"2021-12-22T07:39:10Z"}},{"key":"3XRYJRBB","version":895,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3XRYJRBB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3XRYJRBB","type":"text/html"}},"meta":{"creatorSummary":"Kim","numChildren":1},"data":{"key":"3XRYJRBB","version":895,"itemType":"journalArticle","title":"SuperStyleNet: Deep Image Synthesis with Superpixel Based Style Encoder","creators":[{"creatorType":"author","firstName":"Jonghyun","lastName":"Kim"}],"abstractNote":"","publicationTitle":"","volume":"","issue":"","pages":"14","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-12-22T07:39:10Z","dateModified":"2021-12-22T07:39:10Z"}},{"key":"2UZAZ3TR","version":892,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2UZAZ3TR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2UZAZ3TR","type":"text/html"}},"meta":{"creatorSummary":"Deng et al.","parsedDate":"2021-12-17","numChildren":3},"data":{"key":"2UZAZ3TR","version":892,"itemType":"journalArticle","title":"GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation","creators":[{"creatorType":"author","firstName":"Yu","lastName":"Deng"},{"creatorType":"author","firstName":"Jiaolong","lastName":"Yang"},{"creatorType":"author","firstName":"Jianfeng","lastName":"Xiang"},{"creatorType":"author","firstName":"Xin","lastName":"Tong"}],"abstractNote":"3D-aware image generative modeling aims to generate 3D-consistent images with explicitly controllable camera poses. Recent works have shown promising results by training neural radiance field (NeRF) generators on unstructured 2D images, but still can not generate highly-realistic images with fine details. A critical reason is that the high memory and computation cost of volumetric representation learning greatly restricts the number of point samples for radiance integration during training. Deficient sampling not only limits the expressive power of the generator to handle fine details but also impedes effective GAN training due to the noise caused by unstable Monte Carlo sampling. We propose a novel approach that regulates point sampling and radiance field learning on 2D manifolds, embodied as a set of learned implicit surfaces in the 3D volume. For each viewing ray, we calculate ray-surface intersections and accumulate their radiance generated by the network. By training and rendering such radiance manifolds, our generator can produce high quality images with realistic fine details and strong visual 3D consistency.","publicationTitle":"arXiv:2112.08867 [cs]","volume":"","issue":"","pages":"","date":"2021-12-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"GRAM","url":"http://arxiv.org/abs/2112.08867","accessDate":"2021-12-22T07:36:46Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.08867","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-22T07:36:47Z","dateModified":"2021-12-22T07:36:47Z"}},{"key":"5ND7PY2L","version":892,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5ND7PY2L","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5ND7PY2L","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2UZAZ3TR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5ND7PY2L","version":892,"parentItem":"2UZAZ3TR","itemType":"note","note":"Comment: Add project page link: https://yudeng.github.io/GRAM/","tags":[],"relations":{},"dateAdded":"2021-12-22T07:36:47Z","dateModified":"2021-12-22T07:36:47Z"}},{"key":"XJ9QHTBB","version":893,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XJ9QHTBB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XJ9QHTBB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GCAWP9ZZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XJ9QHTBB","version":893,"parentItem":"GCAWP9ZZ","itemType":"attachment","linkMode":"imported_url","title":"Chan et al. - Efﬁcient Geometry-aware 3D Generative Adversarial .pdf","accessDate":"2021-12-22T07:36:35Z","url":"https://matthew-a-chan.github.io/EG3D/media/eg3d.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chan et al. - Efﬁcient Geometry-aware 3D Generative Adversarial .pdf","md5":"8dcc00a260df3260fd97cb4b5c47ad42","mtime":1640158603000,"tags":[],"relations":{},"dateAdded":"2021-12-22T07:36:35Z","dateModified":"2021-12-22T07:36:46Z"}},{"key":"GCAWP9ZZ","version":892,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GCAWP9ZZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GCAWP9ZZ","type":"text/html"}},"meta":{"creatorSummary":"Chan et al.","numChildren":2},"data":{"key":"GCAWP9ZZ","version":892,"itemType":"journalArticle","title":"Efﬁcient Geometry-aware 3D Generative Adversarial Networks","creators":[{"creatorType":"author","firstName":"Eric R","lastName":"Chan"},{"creatorType":"author","firstName":"Connor Z","lastName":"Lin"},{"creatorType":"author","firstName":"Matthew A","lastName":"Chan"},{"creatorType":"author","firstName":"Koki","lastName":"Nagano"},{"creatorType":"author","firstName":"Boxiao","lastName":"Pan"},{"creatorType":"author","firstName":"Shalini","lastName":"De"},{"creatorType":"author","firstName":"Orazio","lastName":"Gallo"},{"creatorType":"author","firstName":"Leonidas","lastName":"Guibas"},{"creatorType":"author","firstName":"Jonathan","lastName":"Tremblay"},{"creatorType":"author","firstName":"Sameh","lastName":"Khamis"},{"creatorType":"author","firstName":"Tero","lastName":"Karras"},{"creatorType":"author","firstName":"Gordon","lastName":"Wetzstein"}],"abstractNote":"Unsupervised generation of high-quality multi-viewconsistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efﬁciency and image quality of 3D GANs without overly relying on these approximations. For this purpose, we introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efﬁciency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.","publicationTitle":"","volume":"","issue":"","pages":"28","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-12-22T07:36:41Z","dateModified":"2021-12-22T07:36:41Z"}},{"key":"EJ79WUHH","version":891,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EJ79WUHH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EJ79WUHH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GZPHR4HC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EJ79WUHH","version":891,"parentItem":"GZPHR4HC","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-22T06:39:34Z","url":"https://arxiv.org/abs/2112.11427","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"3fd05c49a98610ac400e610155a5f6e5","mtime":1640155195000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/BC73CJMG"},"dateAdded":"2021-12-22T06:39:55Z","dateModified":"2021-12-22T06:39:55Z"}},{"key":"5VHMNILK","version":891,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5VHMNILK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5VHMNILK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GZPHR4HC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5VHMNILK","version":891,"parentItem":"GZPHR4HC","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-22T06:39:02Z","url":"https://arxiv.org/pdf/2112.11427.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Or-El et al. - 2021 - StyleSDF High-Resolution 3D-Consistent Image and .pdf","md5":"34bdcecea01c3b07da62c67062300caf","mtime":1640155195000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/FDDMHM2I"},"dateAdded":"2021-12-22T06:39:55Z","dateModified":"2021-12-22T06:39:55Z"}},{"key":"PXHZRAZ8","version":890,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PXHZRAZ8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PXHZRAZ8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GZPHR4HC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PXHZRAZ8","version":890,"parentItem":"GZPHR4HC","itemType":"note","note":"Comment: Project Webpage: https://stylesdf.github.io/","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/AJF32N2V"},"dateAdded":"2021-12-22T06:39:55Z","dateModified":"2021-12-22T06:39:55Z"}},{"key":"GZPHR4HC","version":890,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GZPHR4HC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GZPHR4HC","type":"text/html"}},"meta":{"creatorSummary":"Or-El et al.","parsedDate":"2021-12-21","numChildren":3},"data":{"key":"GZPHR4HC","version":890,"itemType":"journalArticle","title":"StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation","creators":[{"creatorType":"author","firstName":"Roy","lastName":"Or-El"},{"creatorType":"author","firstName":"Xuan","lastName":"Luo"},{"creatorType":"author","firstName":"Mengyi","lastName":"Shan"},{"creatorType":"author","firstName":"Eli","lastName":"Shechtman"},{"creatorType":"author","firstName":"Jeong Joon","lastName":"Park"},{"creatorType":"author","firstName":"Ira","lastName":"Kemelmacher-Shlizerman"}],"abstractNote":"We introduce a high resolution, 3D-consistent image and shape generation technique which we call StyleSDF. Our method is trained on single-view RGB data only, and stands on the shoulders of StyleGAN2 for image generation, while solving two main challenges in 3D-aware GANs: 1) high-resolution, view-consistent generation of the RGB images, and 2) detailed 3D shape. We achieve this by merging a SDF-based 3D representation with a style-based 2D generator. Our 3D implicit network renders low-resolution feature maps, from which the style-based network generates view-consistent, 1024x1024 images. Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to consistent volume rendering. Our method shows higher quality results compared to state of the art in terms of visual and geometric quality.","publicationTitle":"arXiv:2112.11427 [cs]","volume":"","issue":"","pages":"","date":"2021-12-21","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"StyleSDF","url":"http://arxiv.org/abs/2112.11427","accessDate":"2021-12-22T06:34:22Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.11427","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/68FJJ95Q"},"dateAdded":"2021-12-22T06:39:55Z","dateModified":"2021-12-22T06:39:55Z"}},{"key":"CCXVFAHS","version":885,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CCXVFAHS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CCXVFAHS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8DQKV223","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CCXVFAHS","version":885,"parentItem":"8DQKV223","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-21T13:55:09Z","url":"https://arxiv.org/abs/2112.10741","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"5d3e1e1e1174335d5449f5c207d3d9a0","mtime":1640094909000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/MU4NMK87"},"dateAdded":"2021-12-21T13:55:09Z","dateModified":"2021-12-21T13:55:09Z"}},{"key":"LMSBTYBW","version":885,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LMSBTYBW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LMSBTYBW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8DQKV223","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LMSBTYBW","version":885,"parentItem":"8DQKV223","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-21T13:55:01Z","url":"https://arxiv.org/pdf/2112.10741.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Nichol et al. - 2021 - GLIDE Towards Photorealistic Image Generation and.pdf","md5":"80ed3a9a1fa21679946169b9cd19679a","mtime":1640094901000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/EWEI8TQL"},"dateAdded":"2021-12-21T13:55:01Z","dateModified":"2021-12-21T13:55:01Z"}},{"key":"IY5HLTJS","version":885,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IY5HLTJS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IY5HLTJS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8DQKV223","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"IY5HLTJS","version":885,"parentItem":"8DQKV223","itemType":"note","note":"Comment: 20 pages, 18 figures","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/3QJTXN88"},"dateAdded":"2021-12-21T13:53:01Z","dateModified":"2021-12-21T13:53:01Z"}},{"key":"8DQKV223","version":885,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8DQKV223","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8DQKV223","type":"text/html"}},"meta":{"creatorSummary":"Nichol et al.","parsedDate":"2021-12-20","numChildren":3},"data":{"key":"8DQKV223","version":885,"itemType":"journalArticle","title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models","creators":[{"creatorType":"author","firstName":"Alex","lastName":"Nichol"},{"creatorType":"author","firstName":"Prafulla","lastName":"Dhariwal"},{"creatorType":"author","firstName":"Aditya","lastName":"Ramesh"},{"creatorType":"author","firstName":"Pranav","lastName":"Shyam"},{"creatorType":"author","firstName":"Pamela","lastName":"Mishkin"},{"creatorType":"author","firstName":"Bob","lastName":"McGrew"},{"creatorType":"author","firstName":"Ilya","lastName":"Sutskever"},{"creatorType":"author","firstName":"Mark","lastName":"Chen"}],"abstractNote":"Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.","publicationTitle":"arXiv:2112.10741 [cs]","volume":"","issue":"","pages":"","date":"2021-12-20","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"GLIDE","url":"http://arxiv.org/abs/2112.10741","accessDate":"2021-12-21T13:53:01Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.10741","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/A53S8Z4C"},"dateAdded":"2021-12-21T13:53:01Z","dateModified":"2021-12-21T13:53:01Z"}},{"key":"3BTVS77J","version":1184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3BTVS77J","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3BTVS77J","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WU6KTIRS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3BTVS77J","version":1184,"parentItem":"WU6KTIRS","itemType":"attachment","linkMode":"imported_url","title":"Saharia et al. - 2021 - Palette Image-to-Image Diffusion Models.pdf","accessDate":"2021-12-17T04:42:32Z","url":"https://arxiv.org/pdf/2111.05826.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Saharia et al. - 2021 - Palette Image-to-Image Diffusion Models.pdf","md5":"d683ec7e1c204faefb97135ec676ab85","mtime":1639716152000,"tags":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/7C6ZJMVP"},"dateAdded":"2021-12-17T04:42:32Z","dateModified":"2022-06-13T16:19:37Z"}},{"key":"WU6KTIRS","version":1184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WU6KTIRS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WU6KTIRS","type":"text/html"}},"meta":{"creatorSummary":"Saharia et al.","parsedDate":"2021-11-10","numChildren":4},"data":{"key":"WU6KTIRS","version":1184,"itemType":"journalArticle","title":"Palette: Image-to-Image Diffusion Models","creators":[{"creatorType":"author","firstName":"Chitwan","lastName":"Saharia"},{"creatorType":"author","firstName":"William","lastName":"Chan"},{"creatorType":"author","firstName":"Huiwen","lastName":"Chang"},{"creatorType":"author","firstName":"Chris A.","lastName":"Lee"},{"creatorType":"author","firstName":"Jonathan","lastName":"Ho"},{"creatorType":"author","firstName":"Tim","lastName":"Salimans"},{"creatorType":"author","firstName":"David J.","lastName":"Fleet"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"}],"abstractNote":"We introduce Palette, a simple and general framework for image-to-image translation using conditional diffusion models. On four challenging image-to-image translation tasks (colorization, inpainting, uncropping, and JPEG decompression), Palette outperforms strong GAN and regression baselines, and establishes a new state of the art. This is accomplished without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss, demonstrating a desirable degree of generality and flexibility. We uncover the impact of using $L_2$ vs. $L_1$ loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention through empirical architecture studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, and report several sample quality scores including FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against reference images for various baselines. We expect this standardized evaluation protocol to play a critical role in advancing image-to-image translation research. Finally, we show that a single generalist Palette model trained on 3 tasks (colorization, inpainting, JPEG decompression) performs as well or better than task-specific specialist counterparts.","publicationTitle":"arXiv:2111.05826 [cs]","volume":"","issue":"","pages":"","date":"2021-11-10","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Palette","url":"http://arxiv.org/abs/2111.05826","accessDate":"2021-12-08T14:32:02Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.05826","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/RFDEFCAV","dc:replaces":["http://zotero.org/users/7902311/items/DGZCQMR7","http://zotero.org/users/7902311/items/7K7WXD7V","http://zotero.org/users/7902311/items/H6VLYDMW"]},"dateAdded":"2021-12-08T14:34:10Z","dateModified":"2022-06-13T16:19:37Z"}},{"key":"QDK8BYBH","version":876,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QDK8BYBH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QDK8BYBH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9SFGJX2G","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QDK8BYBH","version":876,"parentItem":"9SFGJX2G","itemType":"attachment","linkMode":"imported_url","title":"Yao et al. - Leveraging Batch Normalization for Vision Transfor.pdf","accessDate":"2021-12-20T09:04:48Z","url":"https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Yao_Leveraging_Batch_Normalization_for_Vision_Transformers_ICCVW_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Yao et al. - Leveraging Batch Normalization for Vision Transfor.pdf","md5":"dbc2da46da04ba49475b8876e5e0416d","mtime":1639991154000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/3UANZGEB"},"dateAdded":"2021-12-20T09:05:54Z","dateModified":"2021-12-20T09:05:54Z"}},{"key":"9SFGJX2G","version":875,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9SFGJX2G","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9SFGJX2G","type":"text/html"}},"meta":{"creatorSummary":"Yao et al.","numChildren":1},"data":{"key":"9SFGJX2G","version":875,"itemType":"journalArticle","title":"Leveraging Batch Normalization for Vision Transformers","creators":[{"creatorType":"author","firstName":"Zhuliang","lastName":"Yao"},{"creatorType":"author","firstName":"Yue","lastName":"Cao"},{"creatorType":"author","firstName":"Yutong","lastName":"Lin"},{"creatorType":"author","firstName":"Ze","lastName":"Liu"},{"creatorType":"author","firstName":"Zheng","lastName":"Zhang"},{"creatorType":"author","firstName":"Han","lastName":"Hu"}],"abstractNote":"Transformer-based vision architectures have attracted great attention because of the strong performance over the convolutional neural networks (CNNs). Inherited from the NLP tasks, the architectures take Layer Normalization (LN) as a default normalization technique. On the other side, previous vision models, i.e., CNNs, treat Batch Normalization (BN) as a de facto standard, with the merits of faster inference than other normalization layers due to an avoidance of calculating the mean and variance statistics during inference, as well as better regularization effects during training. In this paper, we aim to introduce Batch Normalization to Transformer-based vision architectures. Our initial exploration reveals frequent crashes in model training when directly replacing all LN layers with BN, contributing to the un-normalized feed forward network (FFN) blocks. We therefore propose to add a BN layer in-between the two linear layers in the FFN block where stabilized training statistics are observed, resulting in a pure BN-based architecture. Our experiments proved that our resulting approach is as effective as the LN-based counterpart and is about 20% faster.","publicationTitle":"","volume":"","issue":"","pages":"10","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/GRJEA6IT"},"dateAdded":"2021-12-20T09:05:54Z","dateModified":"2021-12-20T09:05:54Z"}},{"key":"YID8TZKC","version":874,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YID8TZKC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YID8TZKC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/L93JNV23","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YID8TZKC","version":874,"parentItem":"L93JNV23","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-20T08:49:54Z","url":"https://arxiv.org/abs/2105.01601","note":"","contentType":"text/html","charset":"utf-8","filename":"2105.html","md5":"edbacb3747ca0f6393e353eceba7ff7a","mtime":1639990194000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/73V7AR6W"},"dateAdded":"2021-12-20T08:49:54Z","dateModified":"2021-12-20T08:49:54Z"}},{"key":"EU8663KD","version":874,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EU8663KD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EU8663KD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/L93JNV23","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EU8663KD","version":874,"parentItem":"L93JNV23","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-20T08:49:44Z","url":"https://arxiv.org/pdf/2105.01601.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tolstikhin et al. - 2021 - MLP-Mixer An all-MLP Architecture for Vision.pdf","md5":"306d532b00ac6db1df94bd8f91b37b4e","mtime":1639990184000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/NQTDDF5S"},"dateAdded":"2021-12-20T08:49:44Z","dateModified":"2021-12-20T08:49:44Z"}},{"key":"NNRTU96W","version":874,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NNRTU96W","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NNRTU96W","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/L93JNV23","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NNRTU96W","version":874,"parentItem":"L93JNV23","itemType":"note","note":"Comment: v2: Fixed parameter counts in Table 1. v3: Added results on JFT-3B in Figure 2(right); Added Section 3.4 on the input permutations. v4: Updated the x label in Figure 2(right)","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/8Z82ALYS"},"dateAdded":"2021-12-20T08:48:57Z","dateModified":"2021-12-20T08:48:57Z"}},{"key":"L93JNV23","version":874,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/L93JNV23","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/L93JNV23","type":"text/html"}},"meta":{"creatorSummary":"Tolstikhin et al.","parsedDate":"2021-06-11","numChildren":3},"data":{"key":"L93JNV23","version":874,"itemType":"journalArticle","title":"MLP-Mixer: An all-MLP Architecture for Vision","creators":[{"creatorType":"author","firstName":"Ilya","lastName":"Tolstikhin"},{"creatorType":"author","firstName":"Neil","lastName":"Houlsby"},{"creatorType":"author","firstName":"Alexander","lastName":"Kolesnikov"},{"creatorType":"author","firstName":"Lucas","lastName":"Beyer"},{"creatorType":"author","firstName":"Xiaohua","lastName":"Zhai"},{"creatorType":"author","firstName":"Thomas","lastName":"Unterthiner"},{"creatorType":"author","firstName":"Jessica","lastName":"Yung"},{"creatorType":"author","firstName":"Andreas","lastName":"Steiner"},{"creatorType":"author","firstName":"Daniel","lastName":"Keysers"},{"creatorType":"author","firstName":"Jakob","lastName":"Uszkoreit"},{"creatorType":"author","firstName":"Mario","lastName":"Lucic"},{"creatorType":"author","firstName":"Alexey","lastName":"Dosovitskiy"}],"abstractNote":"Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. \"mixing\" the per-location features), and one with MLPs applied across patches (i.e. \"mixing\" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.","publicationTitle":"arXiv:2105.01601 [cs]","volume":"","issue":"","pages":"","date":"2021-06-11","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"MLP-Mixer","url":"http://arxiv.org/abs/2105.01601","accessDate":"2021-12-20T08:48:57Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2105.01601","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/6VRWELTI"},"dateAdded":"2021-12-20T08:48:57Z","dateModified":"2021-12-20T08:48:57Z"}},{"key":"9MMKRKFW","version":868,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9MMKRKFW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9MMKRKFW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BH3JAGRV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9MMKRKFW","version":868,"parentItem":"BH3JAGRV","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-20T08:27:15Z","url":"https://arxiv.org/pdf/2010.11929.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf","md5":"7f1ba6e93e08ec7fc3f4fa1c8a255ac0","mtime":1639988853000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/ZGNY8IJ6"},"dateAdded":"2021-12-20T08:27:33Z","dateModified":"2021-12-20T08:27:33Z"}},{"key":"6IQ8TJ59","version":868,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6IQ8TJ59","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6IQ8TJ59","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BH3JAGRV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6IQ8TJ59","version":868,"parentItem":"BH3JAGRV","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-20T08:27:21Z","url":"https://arxiv.org/abs/2010.11929","note":"","contentType":"text/html","charset":"utf-8","filename":"2010.html","md5":"0ffb9ec49ebd9908bf3c8faab34d43d5","mtime":1639988853000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/8FCF3XSI"},"dateAdded":"2021-12-20T08:27:33Z","dateModified":"2021-12-20T08:27:33Z"}},{"key":"BH3JAGRV","version":867,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BH3JAGRV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BH3JAGRV","type":"text/html"}},"meta":{"creatorSummary":"Dosovitskiy et al.","parsedDate":"2021-06-03","numChildren":3},"data":{"key":"BH3JAGRV","version":867,"itemType":"journalArticle","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","creators":[{"creatorType":"author","firstName":"Alexey","lastName":"Dosovitskiy"},{"creatorType":"author","firstName":"Lucas","lastName":"Beyer"},{"creatorType":"author","firstName":"Alexander","lastName":"Kolesnikov"},{"creatorType":"author","firstName":"Dirk","lastName":"Weissenborn"},{"creatorType":"author","firstName":"Xiaohua","lastName":"Zhai"},{"creatorType":"author","firstName":"Thomas","lastName":"Unterthiner"},{"creatorType":"author","firstName":"Mostafa","lastName":"Dehghani"},{"creatorType":"author","firstName":"Matthias","lastName":"Minderer"},{"creatorType":"author","firstName":"Georg","lastName":"Heigold"},{"creatorType":"author","firstName":"Sylvain","lastName":"Gelly"},{"creatorType":"author","firstName":"Jakob","lastName":"Uszkoreit"},{"creatorType":"author","firstName":"Neil","lastName":"Houlsby"}],"abstractNote":"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","publicationTitle":"arXiv:2010.11929 [cs]","volume":"","issue":"","pages":"","date":"2021-06-03","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"An Image is Worth 16x16 Words","url":"http://arxiv.org/abs/2010.11929","accessDate":"2021-12-20T08:26:48Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2010.11929","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/K3LF6VPC"},"dateAdded":"2021-12-20T08:27:33Z","dateModified":"2021-12-20T08:27:33Z"}},{"key":"AZ86WRCD","version":867,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AZ86WRCD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AZ86WRCD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BH3JAGRV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AZ86WRCD","version":867,"parentItem":"BH3JAGRV","itemType":"note","note":"Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/YUZBNGHU"},"dateAdded":"2021-12-20T08:27:33Z","dateModified":"2021-12-20T08:27:33Z"}},{"key":"BYFZYP3E","version":866,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BYFZYP3E","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BYFZYP3E","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S93FIYXZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BYFZYP3E","version":866,"parentItem":"S93FIYXZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-20T06:39:00Z","url":"https://arxiv.org/abs/2112.09687","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"861e2499a6d82e18a83024aaa1a3f313","mtime":1639983303000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/S8UMGWQ3"},"dateAdded":"2021-12-20T06:55:03Z","dateModified":"2021-12-20T06:55:03Z"}},{"key":"MDKTPNFL","version":866,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MDKTPNFL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MDKTPNFL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S93FIYXZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MDKTPNFL","version":866,"parentItem":"S93FIYXZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-20T06:38:53Z","url":"https://arxiv.org/pdf/2112.09687.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Suhail et al. - 2021 - Light Field Neural Rendering.pdf","md5":"b84616e5c8eb0406cbfedd4ffc5a3f7c","mtime":1639983303000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/KYINDQHC"},"dateAdded":"2021-12-20T06:55:03Z","dateModified":"2021-12-20T06:55:03Z"}},{"key":"7P6WTT85","version":865,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7P6WTT85","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7P6WTT85","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S93FIYXZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7P6WTT85","version":865,"parentItem":"S93FIYXZ","itemType":"note","note":"Comment: Project page with code and videos at https://light-field-neural-rendering.github.io","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/695EFHHC"},"dateAdded":"2021-12-20T06:55:03Z","dateModified":"2021-12-20T06:55:03Z"}},{"key":"S93FIYXZ","version":865,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/S93FIYXZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/S93FIYXZ","type":"text/html"}},"meta":{"creatorSummary":"Suhail et al.","parsedDate":"2021-12-17","numChildren":3},"data":{"key":"S93FIYXZ","version":865,"itemType":"journalArticle","title":"Light Field Neural Rendering","creators":[{"creatorType":"author","firstName":"Mohammed","lastName":"Suhail"},{"creatorType":"author","firstName":"Carlos","lastName":"Esteves"},{"creatorType":"author","firstName":"Leonid","lastName":"Sigal"},{"creatorType":"author","firstName":"Ameesh","lastName":"Makadia"}],"abstractNote":"Classical light field rendering for novel view synthesis can accurately reproduce view-dependent effects such as reflection, refraction, and translucency, but requires a dense view sampling of the scene. Methods based on geometric reconstruction need only sparse views, but cannot accurately model non-Lambertian effects. We introduce a model that combines the strengths and mitigates the limitations of these two directions. By operating on a four-dimensional representation of the light field, our model learns to represent view-dependent effects accurately. By enforcing geometric constraints during training and inference, the scene geometry is implicitly learned from a sparse set of views. Concretely, we introduce a two-stage transformer-based model that first aggregates features along epipolar lines, then aggregates features along reference views to produce the color of a target ray. Our model outperforms the state-of-the-art on multiple forward-facing and 360{\\deg} datasets, with larger margins on scenes with severe view-dependent variations.","publicationTitle":"arXiv:2112.09687 [cs]","volume":"","issue":"","pages":"","date":"2021-12-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2112.09687","accessDate":"2021-12-20T06:36:28Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.09687","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"aek"}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/TW5DUQNB"},"dateAdded":"2021-12-20T06:55:03Z","dateModified":"2021-12-20T06:55:03Z"}},{"key":"USU77U7F","version":862,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/USU77U7F","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/USU77U7F","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IBG26726","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"USU77U7F","version":862,"parentItem":"IBG26726","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-20T05:53:03Z","url":"https://arxiv.org/abs/2112.05149","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"7eb2029ccbcaac798b95dba0d7b2169e","mtime":1639979583000,"tags":[],"relations":{},"dateAdded":"2021-12-20T05:53:03Z","dateModified":"2021-12-20T05:53:03Z"}},{"key":"3ACBMA5V","version":862,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3ACBMA5V","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3ACBMA5V","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IBG26726","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3ACBMA5V","version":862,"parentItem":"IBG26726","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-20T05:52:58Z","url":"https://arxiv.org/pdf/2112.05149v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kim et al. - 2021 - DiffuseMorph Unsupervised Deformable Image Regist.pdf","md5":"6b4e9ec87627fa4cd6753ea7017ee1c7","mtime":1639979578000,"tags":[],"relations":{},"dateAdded":"2021-12-20T05:52:58Z","dateModified":"2021-12-20T05:52:58Z"}},{"key":"IQE367ZQ","version":857,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IQE367ZQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IQE367ZQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BT67HPUK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"IQE367ZQ","version":857,"parentItem":"BT67HPUK","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-20T05:48:43Z","url":"https://arxiv.org/abs/2112.07068","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"e6531bbc4785da0cc8ebf9556651dc94","mtime":1639979323000,"tags":[],"relations":{},"dateAdded":"2021-12-20T05:48:43Z","dateModified":"2021-12-20T05:48:43Z"}},{"key":"RW2SSKL5","version":857,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RW2SSKL5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RW2SSKL5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BT67HPUK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RW2SSKL5","version":857,"parentItem":"BT67HPUK","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-20T05:48:38Z","url":"https://arxiv.org/pdf/2112.07068v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Dockhorn et al. - 2021 - Score-Based Generative Modeling with Critically-Da.pdf","md5":"265a15191959b9fbc2e656fb68579938","mtime":1639979318000,"tags":[],"relations":{},"dateAdded":"2021-12-20T05:48:38Z","dateModified":"2021-12-20T05:48:38Z"}},{"key":"BT67HPUK","version":855,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BT67HPUK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BT67HPUK","type":"text/html"}},"meta":{"creatorSummary":"Dockhorn et al.","parsedDate":"2021-12-13","numChildren":2},"data":{"key":"BT67HPUK","version":855,"itemType":"journalArticle","title":"Score-Based Generative Modeling with Critically-Damped Langevin Diffusion","creators":[{"creatorType":"author","firstName":"Tim","lastName":"Dockhorn"},{"creatorType":"author","firstName":"Arash","lastName":"Vahdat"},{"creatorType":"author","firstName":"Karsten","lastName":"Kreis"}],"abstractNote":"Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered \"velocities\" that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. Project page and code: https://nv-tlabs.github.io/CLD-SGM.","publicationTitle":"arXiv:2112.07068 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-12-13","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2112.07068","accessDate":"2021-12-20T05:47:56Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.07068\nversion: 1","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-20T05:47:56Z","dateModified":"2021-12-20T05:47:56Z"}},{"key":"HW3GPJJJ","version":864,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HW3GPJJJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HW3GPJJJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GH9CK93P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HW3GPJJJ","version":864,"parentItem":"GH9CK93P","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-20T05:46:59Z","url":"https://arxiv.org/abs/2112.05744","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"71bf1f12a09ccc433d8b0db15011bc53","mtime":1639979219000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/N39ZV7ST"},"dateAdded":"2021-12-20T05:46:59Z","dateModified":"2021-12-20T05:46:59Z"}},{"key":"52Q4CIU4","version":864,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/52Q4CIU4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/52Q4CIU4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GH9CK93P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"52Q4CIU4","version":864,"parentItem":"GH9CK93P","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-20T05:46:51Z","url":"https://arxiv.org/pdf/2112.05744v2.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Liu et al. - 2021 - More Control for Free! Image Synthesis with Semant.pdf","md5":"cf0b89c22bdbfbdae1737350044c4c1e","mtime":1639979211000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/G7N84NSY"},"dateAdded":"2021-12-20T05:46:51Z","dateModified":"2021-12-20T05:46:51Z"}},{"key":"PABIH86D","version":864,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PABIH86D","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PABIH86D","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GH9CK93P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PABIH86D","version":864,"parentItem":"GH9CK93P","itemType":"note","note":"Comment: Project page https://xh-liu.github.io/sdg/","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/YWCV2IYC"},"dateAdded":"2021-12-20T05:46:17Z","dateModified":"2021-12-20T05:46:17Z"}},{"key":"2DVES98J","version":845,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2DVES98J","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2DVES98J","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RSCSBIBN","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2DVES98J","version":845,"parentItem":"RSCSBIBN","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-16T05:56:16Z","url":"https://arxiv.org/abs/2111.15174","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"23629b5525eefd1635c7fef89f1c8d82","mtime":1639634176000,"tags":[],"relations":{},"dateAdded":"2021-12-16T05:56:16Z","dateModified":"2021-12-16T05:56:16Z"}},{"key":"56EBVPJI","version":844,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/56EBVPJI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/56EBVPJI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RSCSBIBN","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"56EBVPJI","version":844,"parentItem":"RSCSBIBN","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-16T05:56:12Z","url":"https://arxiv.org/pdf/2111.15174.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wang et al. - 2021 - CRIS CLIP-Driven Referring Image Segmentation.pdf","md5":"769b3e1c699c4a48e6773229e182eaa0","mtime":1639634172000,"tags":[],"relations":{},"dateAdded":"2021-12-16T05:56:12Z","dateModified":"2021-12-16T05:56:12Z"}},{"key":"YZCIIVMM","version":843,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YZCIIVMM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YZCIIVMM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CYJ9QZ29","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YZCIIVMM","version":843,"parentItem":"CYJ9QZ29","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-16T05:55:59Z","url":"https://arxiv.org/abs/2107.12518","note":"","contentType":"text/html","charset":"utf-8","filename":"2107.html","md5":"1d878af9bd20bcfd973926c7297f46da","mtime":1639634159000,"tags":[],"relations":{},"dateAdded":"2021-12-16T05:55:59Z","dateModified":"2021-12-16T05:55:59Z"}},{"key":"MBCHPVB2","version":843,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MBCHPVB2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MBCHPVB2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CYJ9QZ29","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MBCHPVB2","version":843,"parentItem":"CYJ9QZ29","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-16T05:55:52Z","url":"https://arxiv.org/pdf/2107.12518.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Pakhomov et al. - 2021 - Segmentation in Style Unsupervised Semantic Image.pdf","md5":"d33907b76376b6293aeb10a09a41dd9e","mtime":1639634152000,"tags":[],"relations":{},"dateAdded":"2021-12-16T05:55:52Z","dateModified":"2021-12-16T05:55:52Z"}},{"key":"Z2GWCKLX","version":840,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z2GWCKLX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z2GWCKLX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RSCSBIBN","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Z2GWCKLX","version":840,"parentItem":"RSCSBIBN","itemType":"note","note":"Comment: 15 pages, 6 figures","tags":[],"relations":{},"dateAdded":"2021-12-16T05:55:45Z","dateModified":"2021-12-16T05:55:45Z"}},{"key":"RSCSBIBN","version":840,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RSCSBIBN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RSCSBIBN","type":"text/html"}},"meta":{"creatorSummary":"Wang et al.","parsedDate":"2021-11-30","numChildren":3},"data":{"key":"RSCSBIBN","version":840,"itemType":"journalArticle","title":"CRIS: CLIP-Driven Referring Image Segmentation","creators":[{"creatorType":"author","firstName":"Zhaoqing","lastName":"Wang"},{"creatorType":"author","firstName":"Yu","lastName":"Lu"},{"creatorType":"author","firstName":"Qiang","lastName":"Li"},{"creatorType":"author","firstName":"Xunqiang","lastName":"Tao"},{"creatorType":"author","firstName":"Yandong","lastName":"Guo"},{"creatorType":"author","firstName":"Mingming","lastName":"Gong"},{"creatorType":"author","firstName":"Tongliang","lastName":"Liu"}],"abstractNote":"Referring image segmentation aims to segment a referent via a natural linguistic expression.Due to the distinct data properties between text and image, it is challenging for a network to well align text and pixel-level features. Existing approaches use pretrained models to facilitate learning, yet separately transfer the language/vision knowledge from pretrained models, ignoring the multi-modal corresponding information. Inspired by the recent advance in Contrastive Language-Image Pretraining (CLIP), in this paper, we propose an end-to-end CLIP-Driven Referring Image Segmentation framework (CRIS). To transfer the multi-modal knowledge effectively, CRIS resorts to vision-language decoding and contrastive learning for achieving the text-to-pixel alignment. More specifically, we design a vision-language decoder to propagate fine-grained semantic information from textual representations to each pixel-level activation, which promotes consistency between the two modalities. In addition, we present text-to-pixel contrastive learning to explicitly enforce the text feature similar to the related pixel-level features and dissimilar to the irrelevances. The experimental results on three benchmark datasets demonstrate that our proposed framework significantly outperforms the state-of-the-art performance without any post-processing. The code will be released.","publicationTitle":"arXiv:2111.15174 [cs]","volume":"","issue":"","pages":"","date":"2021-11-30","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"CRIS","url":"http://arxiv.org/abs/2111.15174","accessDate":"2021-12-16T05:55:45Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.15174","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-16T05:55:45Z","dateModified":"2021-12-16T05:55:45Z"}},{"key":"9BDNNNY4","version":840,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9BDNNNY4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9BDNNNY4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CYJ9QZ29","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9BDNNNY4","version":840,"parentItem":"CYJ9QZ29","itemType":"note","note":"Comment: https://segmentation-in-style.github.io/","tags":[],"relations":{},"dateAdded":"2021-12-16T05:55:42Z","dateModified":"2021-12-16T05:55:42Z"}},{"key":"CYJ9QZ29","version":840,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CYJ9QZ29","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CYJ9QZ29","type":"text/html"}},"meta":{"creatorSummary":"Pakhomov et al.","parsedDate":"2021-11-18","numChildren":3},"data":{"key":"CYJ9QZ29","version":840,"itemType":"journalArticle","title":"Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP","creators":[{"creatorType":"author","firstName":"Daniil","lastName":"Pakhomov"},{"creatorType":"author","firstName":"Sanchit","lastName":"Hira"},{"creatorType":"author","firstName":"Narayani","lastName":"Wagle"},{"creatorType":"author","firstName":"Kemar E.","lastName":"Green"},{"creatorType":"author","firstName":"Nassir","lastName":"Navab"}],"abstractNote":"We introduce a method that allows to automatically segment images into semantically meaningful regions without human supervision. Derived regions are consistent across different images and coincide with human-defined semantic classes on some datasets. In cases where semantic regions might be hard for human to define and consistently label, our method is still able to find meaningful and consistent semantic classes. In our work, we use pretrained StyleGAN2 generative model: clustering in the feature space of the generative model allows to discover semantic classes. Once classes are discovered, a synthetic dataset with generated images and corresponding segmentation masks can be created. After that a segmentation model is trained on the synthetic dataset and is able to generalize to real images. Additionally, by using CLIP we are able to use prompts defined in a natural language to discover some desired semantic classes. We test our method on publicly available datasets and show state-of-the-art results.","publicationTitle":"arXiv:2107.12518 [cs]","volume":"","issue":"","pages":"","date":"2021-11-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Segmentation in Style","url":"http://arxiv.org/abs/2107.12518","accessDate":"2021-12-16T05:55:42Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2107.12518","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-16T05:55:42Z","dateModified":"2021-12-16T05:55:42Z"}},{"key":"UULLGRS2","version":838,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UULLGRS2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UULLGRS2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RI4Y6S7X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UULLGRS2","version":838,"parentItem":"RI4Y6S7X","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-11T17:02:57Z","url":"https://arxiv.org/abs/2112.01573","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"5330eab7bf37640b649ecf2c1390ca77","mtime":1639242177000,"tags":[],"relations":{},"dateAdded":"2021-12-11T17:02:57Z","dateModified":"2021-12-11T17:02:57Z"}},{"key":"CZBXW4L4","version":837,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CZBXW4L4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CZBXW4L4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RI4Y6S7X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CZBXW4L4","version":837,"parentItem":"RI4Y6S7X","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-11T17:02:35Z","url":"https://arxiv.org/pdf/2112.01573.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Liu et al. - 2021 - FuseDream Training-Free Text-to-Image Generation .pdf","md5":"5bda9e5b36dae75f2d0a267d56f23aef","mtime":1639242155000,"tags":[],"relations":{},"dateAdded":"2021-12-11T17:02:35Z","dateModified":"2021-12-11T17:02:35Z"}},{"key":"F32BYZDU","version":834,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/F32BYZDU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/F32BYZDU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4WL9L9VQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"F32BYZDU","version":834,"parentItem":"4WL9L9VQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-11T17:01:13Z","url":"https://arxiv.org/abs/2112.01518","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"fd39dfb17e2181c9a2679a38e5a65bef","mtime":1639242073000,"tags":[],"relations":{},"dateAdded":"2021-12-11T17:01:13Z","dateModified":"2021-12-11T17:01:13Z"}},{"key":"4L4T7Y6C","version":834,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4L4T7Y6C","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4L4T7Y6C","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4WL9L9VQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4L4T7Y6C","version":834,"parentItem":"4WL9L9VQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-11T17:01:06Z","url":"https://arxiv.org/pdf/2112.01518.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Rao et al. - 2021 - DenseCLIP Language-Guided Dense Prediction with C.pdf","md5":"62151f198dd8048fb8892f0702da9dda","mtime":1639242066000,"tags":[],"relations":{},"dateAdded":"2021-12-11T17:01:06Z","dateModified":"2021-12-11T17:01:06Z"}},{"key":"WRS6LG4U","version":831,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WRS6LG4U","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WRS6LG4U","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4WL9L9VQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WRS6LG4U","version":831,"parentItem":"4WL9L9VQ","itemType":"note","note":"Comment: Project page: https://denseclip.ivg-research.xyz","tags":[],"relations":{},"dateAdded":"2021-12-11T17:00:55Z","dateModified":"2021-12-11T17:00:55Z"}},{"key":"4WL9L9VQ","version":831,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4WL9L9VQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4WL9L9VQ","type":"text/html"}},"meta":{"creatorSummary":"Rao et al.","parsedDate":"2021-12-02","numChildren":3},"data":{"key":"4WL9L9VQ","version":831,"itemType":"journalArticle","title":"DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting","creators":[{"creatorType":"author","firstName":"Yongming","lastName":"Rao"},{"creatorType":"author","firstName":"Wenliang","lastName":"Zhao"},{"creatorType":"author","firstName":"Guangyi","lastName":"Chen"},{"creatorType":"author","firstName":"Yansong","lastName":"Tang"},{"creatorType":"author","firstName":"Zheng","lastName":"Zhu"},{"creatorType":"author","firstName":"Guan","lastName":"Huang"},{"creatorType":"author","firstName":"Jie","lastName":"Zhou"},{"creatorType":"author","firstName":"Jiwen","lastName":"Lu"}],"abstractNote":"Recent progress has shown that large-scale pre-training using contrastive image-text pairs can be a promising alternative for high-quality visual representation learning from natural language supervision. Benefiting from a broader source of supervision, this new paradigm exhibits impressive transferability to downstream classification tasks and datasets. However, the problem of transferring the knowledge learned from image-text pairs to more complex dense prediction tasks has barely been visited. In this work, we present a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP. Specifically, we convert the original image-text matching problem in CLIP to a pixel-text matching problem and use the pixel-text score maps to guide the learning of dense prediction models. By further using the contextual information from the image to prompt the language model, we are able to facilitate our model to better exploit the pre-trained knowledge. Our method is model-agnostic, which can be applied to arbitrary dense prediction systems and various pre-trained visual backbones including both CLIP models and ImageNet pre-trained models. Extensive experiments demonstrate the superior performance of our methods on semantic segmentation, object detection, and instance segmentation tasks. Code is available at https://github.com/raoyongming/DenseCLIP","publicationTitle":"arXiv:2112.01518 [cs]","volume":"","issue":"","pages":"","date":"2021-12-02","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"DenseCLIP","url":"http://arxiv.org/abs/2112.01518","accessDate":"2021-12-11T17:00:55Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.01518","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-11T17:00:55Z","dateModified":"2021-12-11T17:00:55Z"}},{"key":"RI4Y6S7X","version":830,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RI4Y6S7X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RI4Y6S7X","type":"text/html"}},"meta":{"creatorSummary":"Liu et al.","parsedDate":"2021-12-02","numChildren":2},"data":{"key":"RI4Y6S7X","version":830,"itemType":"journalArticle","title":"FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization","creators":[{"creatorType":"author","firstName":"Xingchao","lastName":"Liu"},{"creatorType":"author","firstName":"Chengyue","lastName":"Gong"},{"creatorType":"author","firstName":"Lemeng","lastName":"Wu"},{"creatorType":"author","firstName":"Shujian","lastName":"Zhang"},{"creatorType":"author","firstName":"Hao","lastName":"Su"},{"creatorType":"author","firstName":"Qiang","lastName":"Liu"}],"abstractNote":"Generating images from natural language instructions is an intriguing yet highly challenging task. We approach text-to-image generation by combining the power of the retrained CLIP representation with an off-the-shelf image generator (GANs), optimizing in the latent space of GAN to find images that achieve maximum CLIP score with the given input text. Compared to traditional methods that train generative models from text to image starting from scratch, the CLIP+GAN approach is training-free, zero shot and can be easily customized with different generators. However, optimizing CLIP score in the GAN space casts a highly challenging optimization problem and off-the-shelf optimizers such as Adam fail to yield satisfying results. In this work, we propose a FuseDream pipeline, which improves the CLIP+GAN approach with three key techniques: 1) an AugCLIP score which robustifies the CLIP objective by introducing random augmentation on image. 2) a novel initialization and over-parameterization strategy for optimization which allows us to efficiently navigate the non-convex landscape in GAN space. 3) a composed generation technique which, by leveraging a novel bi-level optimization formulation, can compose multiple images to extend the GAN space and overcome the data-bias. When promoted by different input text, FuseDream can generate high-quality images with varying objects, backgrounds, artistic styles, even novel counterfactual concepts that do not appear in the training data of the GAN we use. Quantitatively, the images generated by FuseDream yield top-level Inception score and FID score on MS COCO dataset, without additional architecture design or training. Our code is publicly available at \\url{https://github.com/gnobitab/FuseDream}.","publicationTitle":"arXiv:2112.01573 [cs]","volume":"","issue":"","pages":"","date":"2021-12-02","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"FuseDream","url":"http://arxiv.org/abs/2112.01573","accessDate":"2021-12-11T17:00:43Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.01573","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-11T17:00:43Z","dateModified":"2021-12-11T17:00:43Z"}},{"key":"DI3IZ3S3","version":827,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DI3IZ3S3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DI3IZ3S3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/3RTX9SZR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DI3IZ3S3","version":827,"parentItem":"3RTX9SZR","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-11T16:35:50Z","url":"https://arxiv.org/abs/2103.17249","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"7b8d660c5ff6caffa555e9ae8e20054b","mtime":1639240549000,"tags":[],"relations":{},"dateAdded":"2021-12-11T16:35:50Z","dateModified":"2021-12-11T16:35:50Z"}},{"key":"5XPQ26IA","version":827,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5XPQ26IA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5XPQ26IA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/3RTX9SZR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5XPQ26IA","version":827,"parentItem":"3RTX9SZR","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-11T16:35:33Z","url":"https://arxiv.org/pdf/2103.17249.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Patashnik et al. - 2021 - StyleCLIP Text-Driven Manipulation of StyleGAN Im.pdf","md5":"355221bc9b4175a2cb6767b78a66c3af","mtime":1639240533000,"tags":[],"relations":{},"dateAdded":"2021-12-11T16:35:33Z","dateModified":"2021-12-11T16:35:33Z"}},{"key":"5J54XAIG","version":823,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5J54XAIG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5J54XAIG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5L2SAIKV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5J54XAIG","version":823,"parentItem":"5L2SAIKV","itemType":"attachment","linkMode":"imported_url","title":"Deng et al. - 2020 - Disentangled and Controllable Face Image Generatio.pdf","accessDate":"2021-12-10T09:01:25Z","url":"https://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_Disentangled_and_Controllable_Face_Image_Generation_via_3D_Imitative-Contrastive_Learning_CVPR_2020_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Deng et al. - 2020 - Disentangled and Controllable Face Image Generatio.pdf","md5":"4cc8f175a61b313e855949188a013647","mtime":1639126890000,"tags":[],"relations":{},"dateAdded":"2021-12-10T09:01:25Z","dateModified":"2021-12-11T16:33:32Z"}},{"key":"5L2SAIKV","version":823,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5L2SAIKV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5L2SAIKV","type":"text/html"}},"meta":{"creatorSummary":"Deng et al.","parsedDate":"2020","numChildren":2},"data":{"key":"5L2SAIKV","version":823,"itemType":"conferencePaper","title":"Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning","creators":[{"creatorType":"author","firstName":"Yu","lastName":"Deng"},{"creatorType":"author","firstName":"Jiaolong","lastName":"Yang"},{"creatorType":"author","firstName":"Dong","lastName":"Chen"},{"creatorType":"author","firstName":"Fang","lastName":"Wen"},{"creatorType":"author","firstName":"Xin","lastName":"Tong"}],"abstractNote":"We propose an approach for face image generation of virtual people with disentangled, precisely-controllable latent representations for identity of non-existing people, expression, pose, and illumination. We embed 3D priors into adversarial learning and train the network to imitate the image formation of an analytic 3D face deformation and rendering process. To deal with the generation freedom induced by the domain gap between real and rendered faces, we further introduce contrastive learning to promote disentanglement by comparing pairs of generated images. Experiments show that through our imitative-contrastive learning, the factor variations are very well disentangled and the properties of a generated face can be precisely controlled. We also analyze the learned latent space and present several meaningful properties supporting factor disentanglement. Our method can also be used to embed real images into the disentangled latent space. We hope our method could provide new understandings of the relationship between physical properties and deep image synthesis.","date":"6/2020","proceedingsTitle":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","place":"Seattle, WA, USA","publisher":"IEEE","volume":"","pages":"5153-5162","series":"","language":"en","DOI":"10.1109/CVPR42600.2020.00520","ISBN":"978-1-72817-168-5","shortTitle":"","url":"https://ieeexplore.ieee.org/document/9156396/","accessDate":"2021-10-06T13:32:09Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/4IPEBFWY"},"dateAdded":"2021-10-06T13:32:09Z","dateModified":"2021-12-11T16:33:32Z"}},{"key":"TWQD9XLY","version":823,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TWQD9XLY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TWQD9XLY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/YK9AS7DJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TWQD9XLY","version":823,"parentItem":"YK9AS7DJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-10T09:04:35Z","url":"https://arxiv.org/abs/2005.08925","note":"","contentType":"text/html","charset":"utf-8","filename":"2005.html","md5":"bd30424d9e7c97c4672e48e7430b707d","mtime":1639127071000,"tags":[],"relations":{},"dateAdded":"2021-12-10T09:04:35Z","dateModified":"2021-12-11T16:33:30Z"}},{"key":"D6DF78VD","version":823,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/D6DF78VD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/D6DF78VD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/YK9AS7DJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"D6DF78VD","version":823,"parentItem":"YK9AS7DJ","itemType":"note","note":"Comment: (updated version); SIGGRAPH 2020;Project webpage: https://people.eecs.berkeley.edu/~cecilia77/project-pages/portrait Video: https://youtu.be/M_qYTXhzyac","tags":[],"relations":{},"dateAdded":"2021-12-10T09:03:07Z","dateModified":"2021-12-11T16:33:30Z"}},{"key":"YK9AS7DJ","version":823,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YK9AS7DJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YK9AS7DJ","type":"text/html"}},"meta":{"creatorSummary":"Zhang et al.","parsedDate":"2020-07-08","numChildren":4},"data":{"key":"YK9AS7DJ","version":823,"itemType":"journalArticle","title":"Portrait shadow manipulation","creators":[{"creatorType":"author","firstName":"Xuaner (Cecilia)","lastName":"Zhang"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Yun-Ta","lastName":"Tsai"},{"creatorType":"author","firstName":"Rohit","lastName":"Pandey"},{"creatorType":"author","firstName":"Xiuming","lastName":"Zhang"},{"creatorType":"author","firstName":"Ren","lastName":"Ng"},{"creatorType":"author","firstName":"David E.","lastName":"Jacobs"}],"abstractNote":"","publicationTitle":"ACM Transactions on Graphics","volume":"39","issue":"4","pages":"","date":"2020-07-08","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"ACM Trans. Graph.","language":"en","DOI":"10.1145/3386569.3392390","ISSN":"0730-0301, 1557-7368","shortTitle":"","url":"https://dl.acm.org/doi/10.1145/3386569.3392390","accessDate":"2021-12-10T09:02:59Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/A9W2R5MP"},"dateAdded":"2021-12-10T09:03:06Z","dateModified":"2021-12-11T16:33:30Z"}},{"key":"UN5PCSCZ","version":823,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UN5PCSCZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UN5PCSCZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/YK9AS7DJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UN5PCSCZ","version":823,"parentItem":"YK9AS7DJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-10T09:04:17Z","url":"https://arxiv.org/pdf/2005.08925.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhang et al. - 2020 - Portrait Shadow Manipulation.pdf","md5":"9f0994be6e636ef850dec20bb8500e1b","mtime":1639127057000,"tags":[],"relations":{},"dateAdded":"2021-12-10T09:04:17Z","dateModified":"2021-12-11T16:33:30Z"}},{"key":"M5BLJBLM","version":824,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/M5BLJBLM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/M5BLJBLM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/H8GP5UHU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"M5BLJBLM","version":824,"parentItem":"H8GP5UHU","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-11T13:18:29Z","url":"https://arxiv.org/pdf/2108.12841.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Jo et al. - 2021 - Rethinking Deep Image Prior for Denoising.pdf","md5":"6a68c69a7e49b76ce639823fd96f7a45","mtime":1639228709000,"tags":[],"relations":{},"dateAdded":"2021-12-11T13:18:29Z","dateModified":"2021-12-11T16:33:28Z"}},{"key":"WJLUDN6A","version":824,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WJLUDN6A","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WJLUDN6A","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/H8GP5UHU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WJLUDN6A","version":824,"parentItem":"H8GP5UHU","itemType":"note","note":"Comment: ICCV 2021","tags":[],"relations":{},"dateAdded":"2021-12-11T13:17:24Z","dateModified":"2021-12-11T16:33:28Z"}},{"key":"H8GP5UHU","version":823,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/H8GP5UHU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/H8GP5UHU","type":"text/html"}},"meta":{"creatorSummary":"Jo et al.","numChildren":4},"data":{"key":"H8GP5UHU","version":823,"itemType":"journalArticle","title":"Rethinking Deep Image Prior for Denoising","creators":[{"creatorType":"author","firstName":"Yeonsik","lastName":"Jo"},{"creatorType":"author","firstName":"LG AI","lastName":"Research"},{"creatorType":"author","firstName":"Se Young","lastName":"Chun"},{"creatorType":"author","firstName":"Jonghyun","lastName":"Choi"}],"abstractNote":"Deep image prior (DIP) serves as a good inductive bias for diverse inverse problems. Among them, denoising is known to be particularly challenging for the DIP due to noise fitting with the requirement of an early stopping. To address the issue, we first analyze the DIP by the notion of effective degrees of freedom (DF) to monitor the optimization progress and propose a principled stopping criterion before fitting to noise without access of a paired ground truth image for Gaussian noise. We also propose the ‘stochastic temporal ensemble (STE)’ method for incorporating techniques to further improve DIP’s performance for denoising. We additionally extend our method to Poisson noise. Our empirical validations show that given a single noisy image, our method denoises the image while preserving rich textual details. Further, our approach outperforms prior arts in LPIPS by large margins with comparable PSNR and SSIM on seven different datasets.","publicationTitle":"","volume":"","issue":"","pages":"10","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/PZE3X7LJ"},"dateAdded":"2021-12-11T13:17:19Z","dateModified":"2021-12-11T16:33:28Z"}},{"key":"YQR8SXQY","version":823,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YQR8SXQY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YQR8SXQY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/H8GP5UHU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YQR8SXQY","version":823,"parentItem":"H8GP5UHU","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-11T13:18:46Z","url":"https://arxiv.org/abs/2108.12841","note":"","contentType":"text/html","charset":"utf-8","filename":"2108.html","md5":"9c6b2a8d02598ba698161b62736e11a3","mtime":1639228726000,"tags":[],"relations":{},"dateAdded":"2021-12-11T13:18:46Z","dateModified":"2021-12-11T16:33:28Z"}},{"key":"CEEQ28U2","version":824,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CEEQ28U2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CEEQ28U2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/3RTX9SZR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CEEQ28U2","version":824,"parentItem":"3RTX9SZR","itemType":"note","note":"Comment: 18 pages, 24 figures, code and video may be found here: https://github.com/orpatashnik/StyleCLIP","tags":[],"relations":{},"dateAdded":"2021-12-11T16:31:36Z","dateModified":"2021-12-11T16:33:26Z"}},{"key":"3RTX9SZR","version":824,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3RTX9SZR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3RTX9SZR","type":"text/html"}},"meta":{"creatorSummary":"Patashnik et al.","parsedDate":"2021-03-31","numChildren":4},"data":{"key":"3RTX9SZR","version":824,"itemType":"journalArticle","title":"StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery","creators":[{"creatorType":"author","firstName":"Or","lastName":"Patashnik"},{"creatorType":"author","firstName":"Zongze","lastName":"Wu"},{"creatorType":"author","firstName":"Eli","lastName":"Shechtman"},{"creatorType":"author","firstName":"Daniel","lastName":"Cohen-Or"},{"creatorType":"author","firstName":"Dani","lastName":"Lischinski"}],"abstractNote":"Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.","publicationTitle":"arXiv:2103.17249 [cs]","volume":"","issue":"","pages":"","date":"2021-03-31","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"StyleCLIP","url":"http://arxiv.org/abs/2103.17249","accessDate":"2021-12-11T16:30:51Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.17249","tags":[{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/UHBH5755"},"dateAdded":"2021-12-11T16:30:51Z","dateModified":"2021-12-11T16:33:26Z"}},{"key":"ZZIII55E","version":821,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZZIII55E","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZZIII55E","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/3RTX9SZR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZZIII55E","version":821,"parentItem":"3RTX9SZR","itemType":"note","note":"Comment: 18 pages, 24 figures, code and video may be found here: https://github.com/orpatashnik/StyleCLIP","tags":[],"relations":{},"dateAdded":"2021-12-11T16:30:51Z","dateModified":"2021-12-11T16:30:51Z"}},{"key":"YJHRGH6Y","version":818,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YJHRGH6Y","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YJHRGH6Y","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Z7RER2RJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YJHRGH6Y","version":818,"parentItem":"Z7RER2RJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-11T13:18:15Z","url":"https://arxiv.org/abs/2107.12085","note":"","contentType":"text/html","charset":"utf-8","filename":"2107.html","md5":"9850630b124afdfc9fb3348f06810567","mtime":1639228695000,"tags":[],"relations":{},"dateAdded":"2021-12-11T13:18:15Z","dateModified":"2021-12-11T13:18:15Z"}},{"key":"5IFMDRH3","version":818,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5IFMDRH3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5IFMDRH3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Z7RER2RJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5IFMDRH3","version":818,"parentItem":"Z7RER2RJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-11T13:17:59Z","url":"https://arxiv.org/pdf/2107.12085.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Guo et al. - 2021 - Learning to Adversarially Blur Visual Object Track.pdf","md5":"7b5fb14a537e14faebc955d4dddb7e7c","mtime":1639228674000,"tags":[],"relations":{},"dateAdded":"2021-12-11T13:17:59Z","dateModified":"2021-12-11T13:17:59Z"}},{"key":"AGEP6M7H","version":817,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AGEP6M7H","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AGEP6M7H","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/H8GP5UHU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AGEP6M7H","version":817,"parentItem":"H8GP5UHU","itemType":"attachment","linkMode":"imported_url","title":"Jo et al. - Rethinking Deep Image Prior for Denoising.pdf","accessDate":"2021-12-11T13:17:08Z","url":"https://arxiv.org/pdf/2108.12841.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Jo et al. - Rethinking Deep Image Prior for Denoising.pdf","md5":"6a68c69a7e49b76ce639823fd96f7a45","mtime":1639228649000,"tags":[],"relations":{},"dateAdded":"2021-12-11T13:17:08Z","dateModified":"2021-12-11T13:17:33Z"}},{"key":"6RHUNZ3Y","version":817,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6RHUNZ3Y","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6RHUNZ3Y","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E2KN2R2M","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6RHUNZ3Y","version":817,"parentItem":"E2KN2R2M","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-11T13:17:23Z","url":"https://arxiv.org/abs/2108.09034","note":"","contentType":"text/html","charset":"utf-8","filename":"2108.html","md5":"bb65721074a4102fedb56c14bdcf7b7f","mtime":1639228629000,"tags":[],"relations":{},"dateAdded":"2021-12-11T13:17:23Z","dateModified":"2021-12-11T13:17:23Z"}},{"key":"RXRH3XBQ","version":816,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RXRH3XBQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RXRH3XBQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Z7RER2RJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RXRH3XBQ","version":816,"parentItem":"Z7RER2RJ","itemType":"note","note":"Comment: This work has been accepted to ICCV 2021","tags":[],"relations":{},"dateAdded":"2021-12-11T13:17:09Z","dateModified":"2021-12-11T13:17:09Z"}},{"key":"Z7RER2RJ","version":816,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z7RER2RJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z7RER2RJ","type":"text/html"}},"meta":{"creatorSummary":"Guo et al.","parsedDate":"2021-10-28","numChildren":3},"data":{"key":"Z7RER2RJ","version":816,"itemType":"journalArticle","title":"Learning to Adversarially Blur Visual Object Tracking","creators":[{"creatorType":"author","firstName":"Qing","lastName":"Guo"},{"creatorType":"author","firstName":"Ziyi","lastName":"Cheng"},{"creatorType":"author","firstName":"Felix","lastName":"Juefei-Xu"},{"creatorType":"author","firstName":"Lei","lastName":"Ma"},{"creatorType":"author","firstName":"Xiaofei","lastName":"Xie"},{"creatorType":"author","firstName":"Yang","lastName":"Liu"},{"creatorType":"author","firstName":"Jianjun","lastName":"Zhao"}],"abstractNote":"Motion blur caused by the moving of the object or camera during the exposure can be a key challenge for visual object tracking, affecting tracking accuracy significantly. In this work, we explore the robustness of visual object trackers against motion blur from a new angle, i.e., adversarial blur attack (ABA). Our main objective is to online transfer input frames to their natural motion-blurred counterparts while misleading the state-of-the-art trackers during the tracking process. To this end, we first design the motion blur synthesizing method for visual tracking based on the generation principle of motion blur, considering the motion information and the light accumulation process. With this synthetic method, we propose optimization-based ABA (OP-ABA) by iteratively optimizing an adversarial objective function against the tracking w.r.t. the motion and light accumulation parameters. The OP-ABA is able to produce natural adversarial examples but the iteration can cause heavy time cost, making it unsuitable for attacking real-time trackers. To alleviate this issue, we further propose one-step ABA (OS-ABA) where we design and train a joint adversarial motion and accumulation predictive network (JAMANet) with the guidance of OP-ABA, which is able to efficiently estimate the adversarial motion and accumulation parameters in a one-step way. The experiments on four popular datasets (e.g., OTB100, VOT2018, UAV123, and LaSOT) demonstrate that our methods are able to cause significant accuracy drops on four state-of-the-art trackers with high transferability. Please find the source code at \\url{https://github.com/tsingqguo/ABA}.","publicationTitle":"arXiv:2107.12085 [cs]","volume":"","issue":"","pages":"","date":"2021-10-28","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2107.12085","accessDate":"2021-12-11T13:17:07Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2107.12085","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-11T13:17:09Z","dateModified":"2021-12-11T13:17:09Z"}},{"key":"PIXAQD95","version":817,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PIXAQD95","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PIXAQD95","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E2KN2R2M","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PIXAQD95","version":817,"parentItem":"E2KN2R2M","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-11T13:16:50Z","url":"https://arxiv.org/pdf/2108.09034.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Duan et al. - 2021 - AdvDrop Adversarial Attack to DNNs by Dropping In.pdf","md5":"d1124a4a592eb44874f8ad9366ef7c57","mtime":1639228610000,"tags":[],"relations":{},"dateAdded":"2021-12-11T13:16:50Z","dateModified":"2021-12-11T13:16:50Z"}},{"key":"E2KN2R2M","version":814,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/E2KN2R2M","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/E2KN2R2M","type":"text/html"}},"meta":{"creatorSummary":"Duan et al.","parsedDate":"2021-08-20","numChildren":3},"data":{"key":"E2KN2R2M","version":814,"itemType":"journalArticle","title":"AdvDrop: Adversarial Attack to DNNs by Dropping Information","creators":[{"creatorType":"author","firstName":"Ranjie","lastName":"Duan"},{"creatorType":"author","firstName":"Yuefeng","lastName":"Chen"},{"creatorType":"author","firstName":"Dantong","lastName":"Niu"},{"creatorType":"author","firstName":"Yun","lastName":"Yang"},{"creatorType":"author","firstName":"A. K.","lastName":"Qin"},{"creatorType":"author","firstName":"Yuan","lastName":"He"}],"abstractNote":"Human can easily recognize visual objects with lost information: even losing most details with only contour reserved, e.g. cartoon. However, in terms of visual perception of Deep Neural Networks (DNNs), the ability for recognizing abstract objects (visual objects with lost information) is still a challenge. In this work, we investigate this issue from an adversarial viewpoint: will the performance of DNNs decrease even for the images only losing a little information? Towards this end, we propose a novel adversarial attack, named \\textit{AdvDrop}, which crafts adversarial examples by dropping existing information of images. Previously, most adversarial attacks add extra disturbing information on clean images explicitly. Opposite to previous works, our proposed work explores the adversarial robustness of DNN models in a novel perspective by dropping imperceptible details to craft adversarial examples. We demonstrate the effectiveness of \\textit{AdvDrop} by extensive experiments, and show that this new type of adversarial examples is more difficult to be defended by current defense systems.","publicationTitle":"arXiv:2108.09034 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-08-20","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"AdvDrop","url":"http://arxiv.org/abs/2108.09034","accessDate":"2021-12-11T13:16:21Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2108.09034","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Cryptography and Security","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-11T13:16:21Z","dateModified":"2021-12-11T13:16:21Z"}},{"key":"5NR73GWZ","version":814,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5NR73GWZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5NR73GWZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E2KN2R2M","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5NR73GWZ","version":814,"parentItem":"E2KN2R2M","itemType":"note","note":"Comment: Accepted to ICCV 2021","tags":[],"relations":{},"dateAdded":"2021-12-11T13:16:21Z","dateModified":"2021-12-11T13:16:21Z"}},{"key":"78JC5JDN","version":812,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/78JC5JDN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/78JC5JDN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IZDVMKE4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"78JC5JDN","version":812,"parentItem":"IZDVMKE4","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-11T12:30:11Z","url":"https://arxiv.org/abs/2112.01071","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"e44438a2fa134afc1e9d968624a40c4f","mtime":1639225811000,"tags":[],"relations":{},"dateAdded":"2021-12-11T12:30:11Z","dateModified":"2021-12-11T12:30:11Z"}},{"key":"MLKU4HND","version":812,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MLKU4HND","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MLKU4HND","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IZDVMKE4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MLKU4HND","version":812,"parentItem":"IZDVMKE4","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-11T12:30:02Z","url":"https://arxiv.org/pdf/2112.01071.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhou et al. - 2021 - DenseCLIP Extract Free Dense Labels from CLIP.pdf","md5":"ce8060147982f589a48698a9cfe8715f","mtime":1639225802000,"tags":[],"relations":{},"dateAdded":"2021-12-11T12:30:02Z","dateModified":"2021-12-11T12:30:02Z"}},{"key":"IZDVMKE4","version":809,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IZDVMKE4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IZDVMKE4","type":"text/html"}},"meta":{"creatorSummary":"Zhou et al.","parsedDate":"2021-12-02","numChildren":3},"data":{"key":"IZDVMKE4","version":809,"itemType":"journalArticle","title":"DenseCLIP: Extract Free Dense Labels from CLIP","creators":[{"creatorType":"author","firstName":"Chong","lastName":"Zhou"},{"creatorType":"author","firstName":"Chen Change","lastName":"Loy"},{"creatorType":"author","firstName":"Bo","lastName":"Dai"}],"abstractNote":"Contrastive Language-Image Pre-training (CLIP) has made a remarkable breakthrough in open-vocabulary zero-shot image recognition. Many recent studies leverage the pre-trained CLIP models for image-level classification and manipulation. In this paper, we further explore the potentials of CLIP for pixel-level dense prediction, specifically in semantic segmentation. Our method, DenseCLIP, in the absence of annotations and fine-tuning, yields reasonable segmentation results on open concepts across various datasets. By adding pseudo labeling and self-training, DenseCLIP+ surpasses SOTA transductive zero-shot semantic segmentation methods by large margins, e.g., mIoUs of unseen classes on PASCAL VOC/PASCAL Context/COCO Stuff are improved from 35.6/20.7/30.3 to 86.1/66.7/54.7. We also test the robustness of DenseCLIP under input corruption and evaluate its capability in discriminating fine-grained objects and novel concepts. Our finding suggests that DenseCLIP can serve as a new reliable source of supervision for dense prediction tasks to achieve annotation-free segmentation.","publicationTitle":"arXiv:2112.01071 [cs]","volume":"","issue":"","pages":"","date":"2021-12-02","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"DenseCLIP","url":"http://arxiv.org/abs/2112.01071","accessDate":"2021-12-11T12:26:16Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.01071","tags":[{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-11T12:26:18Z","dateModified":"2021-12-11T12:26:18Z"}},{"key":"BZHRKXR6","version":809,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BZHRKXR6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BZHRKXR6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IZDVMKE4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BZHRKXR6","version":809,"parentItem":"IZDVMKE4","itemType":"note","note":"Comment: Tech report, 12 pages, 6 figures","tags":[],"relations":{},"dateAdded":"2021-12-11T12:26:18Z","dateModified":"2021-12-11T12:26:18Z"}},{"key":"9APQ48QX","version":1185,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9APQ48QX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9APQ48QX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/G3EK5DHW","type":"application/json"}},"meta":{},"data":{"key":"9APQ48QX","version":1185,"parentItem":"G3EK5DHW","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-10T18:02:51Z","url":"https://arxiv.org/abs/2112.05143","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"3dc0fe87660e0f8cc5152d7885b0a740","mtime":1639159371000,"tags":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/SEWFH344"},"dateAdded":"2021-12-10T18:02:51Z","dateModified":"2022-06-13T16:19:43Z"}},{"key":"9N3JXUJ2","version":1185,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9N3JXUJ2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9N3JXUJ2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/G3EK5DHW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9N3JXUJ2","version":1185,"parentItem":"G3EK5DHW","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-10T18:02:31Z","url":"https://arxiv.org/pdf/2112.05143.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Peebles et al. - 2021 - GAN-Supervised Dense Visual Alignment.pdf","md5":"18fc54d508c326bd134cf1cbe42dfe9f","mtime":1639159351000,"tags":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/PYEHP8HT"},"dateAdded":"2021-12-10T18:02:31Z","dateModified":"2022-06-13T16:19:43Z"}},{"key":"HGKRNJ7Q","version":804,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HGKRNJ7Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HGKRNJ7Q","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/G3EK5DHW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HGKRNJ7Q","version":804,"parentItem":"G3EK5DHW","itemType":"note","note":"Comment: Code available at https://www.github.com/wpeebles/gangealing . Project page and videos available at https://www.wpeebles.com/gangealing","tags":[],"relations":{},"dateAdded":"2021-12-10T17:57:57Z","dateModified":"2021-12-10T17:57:57Z"}},{"key":"G3EK5DHW","version":1185,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/G3EK5DHW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/G3EK5DHW","type":"text/html"}},"meta":{"creatorSummary":"Peebles et al.","parsedDate":"2021-12-09","numChildren":4},"data":{"key":"G3EK5DHW","version":1185,"itemType":"journalArticle","title":"GAN-Supervised Dense Visual Alignment","creators":[{"creatorType":"author","firstName":"William","lastName":"Peebles"},{"creatorType":"author","firstName":"Jun-Yan","lastName":"Zhu"},{"creatorType":"author","firstName":"Richard","lastName":"Zhang"},{"creatorType":"author","firstName":"Antonio","lastName":"Torralba"},{"creatorType":"author","firstName":"Alexei","lastName":"Efros"},{"creatorType":"author","firstName":"Eli","lastName":"Shechtman"}],"abstractNote":"We propose GAN-Supervised Learning, a framework for learning discriminative models and their GAN-generated training data jointly end-to-end. We apply our framework to the dense visual alignment problem. Inspired by the classic Congealing method, our GANgealing algorithm trains a Spatial Transformer to map random samples from a GAN trained on unaligned data to a common, jointly-learned target mode. We show results on eight datasets, all of which demonstrate our method successfully aligns complex data and discovers dense correspondences. GANgealing significantly outperforms past self-supervised correspondence algorithms and performs on-par with (and sometimes exceeds) state-of-the-art supervised correspondence algorithms on several datasets -- without making use of any correspondence supervision or data augmentation and despite being trained exclusively on GAN-generated data. For precise correspondence, we improve upon state-of-the-art supervised methods by as much as $3\\times$. We show applications of our method for augmented reality, image editing and automated pre-processing of image datasets for downstream GAN training.","publicationTitle":"arXiv:2112.05143 [cs]","volume":"","issue":"","pages":"","date":"2021-12-09","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2112.05143","accessDate":"2021-12-10T17:57:57Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.05143","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/S4AZE83Q"},"dateAdded":"2021-12-10T17:57:57Z","dateModified":"2022-06-13T16:19:43Z"}},{"key":"3IVFGAEW","version":801,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3IVFGAEW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3IVFGAEW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/YK9AS7DJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3IVFGAEW","version":801,"parentItem":"YK9AS7DJ","itemType":"attachment","linkMode":"imported_url","title":"Zhang et al. - 2020 - Portrait shadow manipulation.pdf","accessDate":"2021-12-10T09:02:50Z","url":"https://dl.acm.org/doi/pdf/10.1145/3386569.3392390","note":"","contentType":"application/pdf","charset":"","filename":"Zhang et al. - 2020 - Portrait shadow manipulation.pdf","md5":"d31dbd571617ee2036ee775d975d60e0","mtime":1639126999000,"tags":[],"relations":{},"dateAdded":"2021-12-10T09:02:50Z","dateModified":"2021-12-10T09:03:20Z"}},{"key":"MRZ9YX6H","version":796,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MRZ9YX6H","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MRZ9YX6H","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QWNQTYBC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MRZ9YX6H","version":796,"parentItem":"QWNQTYBC","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-10T05:02:55Z","url":"https://arxiv.org/pdf/2110.13040.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Biloš et al. - 2021 - Neural Flows Efficient Alternative to Neural ODEs.pdf","md5":"1ab0429e26f085753b38c85e6e62df65","mtime":1639112597000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/KK58C48Y"},"dateAdded":"2021-12-10T05:03:16Z","dateModified":"2021-12-10T05:03:16Z"}},{"key":"USEPLFVT","version":796,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/USEPLFVT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/USEPLFVT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QWNQTYBC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"USEPLFVT","version":796,"parentItem":"QWNQTYBC","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-10T05:03:04Z","url":"https://arxiv.org/abs/2110.13040","note":"","contentType":"text/html","charset":"utf-8","filename":"2110.html","md5":"d7c9a85f5867621e08b9c33fcc22c186","mtime":1639112597000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/27AJC5QA"},"dateAdded":"2021-12-10T05:03:16Z","dateModified":"2021-12-10T05:03:16Z"}},{"key":"QWNQTYBC","version":795,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QWNQTYBC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QWNQTYBC","type":"text/html"}},"meta":{"creatorSummary":"Biloš et al.","parsedDate":"2021-10-25","numChildren":3},"data":{"key":"QWNQTYBC","version":795,"itemType":"journalArticle","title":"Neural Flows: Efficient Alternative to Neural ODEs","creators":[{"creatorType":"author","firstName":"Marin","lastName":"Biloš"},{"creatorType":"author","firstName":"Johanna","lastName":"Sommer"},{"creatorType":"author","firstName":"Syama Sundar","lastName":"Rangapuram"},{"creatorType":"author","firstName":"Tim","lastName":"Januschowski"},{"creatorType":"author","firstName":"Stephan","lastName":"Günnemann"}],"abstractNote":"Neural ordinary differential equations describe how values change in time. This is the reason why they gained importance in modeling sequential data, especially when the observations are made at irregular intervals. In this paper we propose an alternative by directly modeling the solution curves - the flow of an ODE - with a neural network. This immediately eliminates the need for expensive numerical solvers while still maintaining the modeling capability of neural ODEs. We propose several flow architectures suitable for different applications by establishing precise conditions on when a function defines a valid flow. Apart from computational efficiency, we also provide empirical evidence of favorable generalization performance via applications in time series modeling, forecasting, and density estimation.","publicationTitle":"arXiv:2110.13040 [cs, math]","volume":"","issue":"","pages":"","date":"2021-10-25","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Neural Flows","url":"http://arxiv.org/abs/2110.13040","accessDate":"2021-12-10T05:02:47Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2110.13040","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Mathematics - Numerical Analysis","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/75XYTGP5"},"dateAdded":"2021-12-10T05:03:16Z","dateModified":"2021-12-10T05:03:16Z"}},{"key":"TLHYXF3W","version":795,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TLHYXF3W","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TLHYXF3W","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QWNQTYBC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TLHYXF3W","version":795,"parentItem":"QWNQTYBC","itemType":"note","note":"Comment: Conference on Neural Information Processing Systems (NeurIPS 2021)","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/HAAVFQA8"},"dateAdded":"2021-12-10T05:03:16Z","dateModified":"2021-12-10T05:03:16Z"}},{"key":"3G8WCGNN","version":793,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3G8WCGNN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3G8WCGNN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4N2DBFDS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3G8WCGNN","version":793,"parentItem":"4N2DBFDS","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-09T10:12:47Z","url":"https://arxiv.org/abs/2007.15651","note":"","contentType":"text/html","charset":"utf-8","filename":"2007.html","md5":"f9115fa7e9162ed5d72b0c4ef98b1314","mtime":1639044767000,"tags":[],"relations":{},"dateAdded":"2021-12-09T10:12:47Z","dateModified":"2021-12-09T10:12:47Z"}},{"key":"T6BHK84P","version":793,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/T6BHK84P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/T6BHK84P","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4N2DBFDS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"T6BHK84P","version":793,"parentItem":"4N2DBFDS","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-09T10:11:11Z","url":"https://arxiv.org/pdf/2007.15651.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Park et al. - 2020 - Contrastive Learning for Unpaired Image-to-Image T.pdf","md5":"c754ec773d6b555dce159a00c4f05514","mtime":1639044666000,"tags":[],"relations":{},"dateAdded":"2021-12-09T10:11:11Z","dateModified":"2021-12-09T10:11:11Z"}},{"key":"SWAW7X3V","version":791,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SWAW7X3V","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SWAW7X3V","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4N2DBFDS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SWAW7X3V","version":791,"parentItem":"4N2DBFDS","itemType":"note","note":"Comment: ECCV 2020. Please visit https://taesungp.github.io/ContrastiveUnpairedTranslation/ for introduction videos and more. v3 contains typo fixes and citation update","tags":[],"relations":{},"dateAdded":"2021-12-09T10:09:29Z","dateModified":"2021-12-09T10:09:29Z"}},{"key":"4N2DBFDS","version":791,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4N2DBFDS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4N2DBFDS","type":"text/html"}},"meta":{"creatorSummary":"Park et al.","parsedDate":"2020-08-20","numChildren":3},"data":{"key":"4N2DBFDS","version":791,"itemType":"journalArticle","title":"Contrastive Learning for Unpaired Image-to-Image Translation","creators":[{"creatorType":"author","firstName":"Taesung","lastName":"Park"},{"creatorType":"author","firstName":"Alexei A.","lastName":"Efros"},{"creatorType":"author","firstName":"Richard","lastName":"Zhang"},{"creatorType":"author","firstName":"Jun-Yan","lastName":"Zhu"}],"abstractNote":"In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so -- maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each \"domain\" is only a single image.","publicationTitle":"arXiv:2007.15651 [cs]","volume":"","issue":"","pages":"","date":"2020-08-20","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2007.15651","accessDate":"2021-12-09T10:09:29Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2007.15651","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-09T10:09:29Z","dateModified":"2021-12-09T10:09:29Z"}},{"key":"MJMYMMT2","version":789,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MJMYMMT2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MJMYMMT2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CV9FMQBZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MJMYMMT2","version":789,"parentItem":"CV9FMQBZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T16:28:04Z","url":"https://arxiv.org/abs/2104.07636","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"dea5a1411e705ca052cc892b7a0c69e3","mtime":1638980884000,"tags":[],"relations":{},"dateAdded":"2021-12-08T16:28:04Z","dateModified":"2021-12-08T16:28:04Z"}},{"key":"R76GTCPY","version":789,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/R76GTCPY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/R76GTCPY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CV9FMQBZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"R76GTCPY","version":789,"parentItem":"CV9FMQBZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T16:27:56Z","url":"https://arxiv.org/pdf/2104.07636.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Saharia et al. - 2021 - Image Super-Resolution via Iterative Refinement.pdf","md5":"8344d5977ae4e957b21c92cab830fe73","mtime":1638980876000,"tags":[],"relations":{},"dateAdded":"2021-12-08T16:27:56Z","dateModified":"2021-12-08T16:27:56Z"}},{"key":"CV9FMQBZ","version":786,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CV9FMQBZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CV9FMQBZ","type":"text/html"}},"meta":{"creatorSummary":"Saharia et al.","parsedDate":"2021-06-30","numChildren":2},"data":{"key":"CV9FMQBZ","version":786,"itemType":"journalArticle","title":"Image Super-Resolution via Iterative Refinement","creators":[{"creatorType":"author","firstName":"Chitwan","lastName":"Saharia"},{"creatorType":"author","firstName":"Jonathan","lastName":"Ho"},{"creatorType":"author","firstName":"William","lastName":"Chan"},{"creatorType":"author","firstName":"Tim","lastName":"Salimans"},{"creatorType":"author","firstName":"David J.","lastName":"Fleet"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"}],"abstractNote":"We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.","publicationTitle":"arXiv:2104.07636 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-06-30","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2104.07636","accessDate":"2021-12-08T16:25:49Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.07636","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-08T16:25:49Z","dateModified":"2021-12-08T16:25:49Z"}},{"key":"WHZUK999","version":1184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WHZUK999","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WHZUK999","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WU6KTIRS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WHZUK999","version":1184,"parentItem":"WU6KTIRS","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T14:32:36Z","url":"https://arxiv.org/pdf/2111.05826.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Saharia et al. - 2021 - Palette Image-to-Image Diffusion Models.pdf","md5":"d683ec7e1c204faefb97135ec676ab85","mtime":1638974051000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/PVSWVJ39","dc:replaces":"http://zotero.org/users/7902311/items/7C6ZJMVP"},"dateAdded":"2021-12-08T14:34:10Z","dateModified":"2022-06-13T16:19:37Z"}},{"key":"L4GLXLI5","version":1184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/L4GLXLI5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/L4GLXLI5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WU6KTIRS","type":"application/json"}},"meta":{},"data":{"key":"L4GLXLI5","version":1184,"parentItem":"WU6KTIRS","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T14:32:42Z","url":"https://arxiv.org/abs/2111.05826","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"63f31ed914734b621a6f202b72f95298","mtime":1638974051000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/XUUE4RSN","dc:replaces":"http://zotero.org/users/7902311/items/VX29AC7E"},"dateAdded":"2021-12-08T14:34:10Z","dateModified":"2022-06-13T16:19:37Z"}},{"key":"9GJG59R4","version":782,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9GJG59R4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9GJG59R4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y2SAYIXQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9GJG59R4","version":782,"parentItem":"Y2SAYIXQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T13:52:09Z","url":"https://arxiv.org/abs/2110.02711","note":"","contentType":"text/html","charset":"utf-8","filename":"2110.html","md5":"13b339198e37b33b47d59099d7e36f8b","mtime":1638971529000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:52:09Z","dateModified":"2021-12-08T13:52:09Z"}},{"key":"6HWPJSD7","version":782,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6HWPJSD7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6HWPJSD7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y2SAYIXQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6HWPJSD7","version":782,"parentItem":"Y2SAYIXQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T13:52:04Z","url":"https://arxiv.org/pdf/2110.02711v2.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kim and Ye - 2021 - DiffusionCLIP Text-Guided Diffusion Models for Ro.pdf","md5":"a1ca18444dd65e4b5d85faf6c2b9085a","mtime":1638971524000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:52:04Z","dateModified":"2021-12-08T13:52:04Z"}},{"key":"Y2SAYIXQ","version":778,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y2SAYIXQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y2SAYIXQ","type":"text/html"}},"meta":{"creatorSummary":"Kim and Ye","parsedDate":"2021-12-06","numChildren":2},"data":{"key":"Y2SAYIXQ","version":778,"itemType":"journalArticle","title":"DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation","creators":[{"creatorType":"author","firstName":"Gwanghyun","lastName":"Kim"},{"creatorType":"author","firstName":"Jong Chul","lastName":"Ye"}],"abstractNote":"Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or producing unwanted image artifacts. To mitigate these problems and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs text-driven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zero-shot image manipulation successfully even between unseen domains. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipulation. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our methods compared to the existing baselines.","publicationTitle":"arXiv:2110.02711 [cs]","volume":"","issue":"","pages":"","date":"2021-12-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"DiffusionCLIP","url":"http://arxiv.org/abs/2110.02711","accessDate":"2021-12-08T13:48:13Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2110.02711\nversion: 2","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-08T13:48:13Z","dateModified":"2021-12-08T13:48:13Z"}},{"key":"DFMFCRT2","version":778,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DFMFCRT2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DFMFCRT2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/46DDLBL4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DFMFCRT2","version":778,"parentItem":"46DDLBL4","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T13:47:46Z","url":"https://arxiv.org/abs/2111.15640","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"469baab3aa2d14200b68aaf7cb4fc1d2","mtime":1638971266000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:47:46Z","dateModified":"2021-12-08T13:47:46Z"}},{"key":"XZ3FNAM8","version":778,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XZ3FNAM8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XZ3FNAM8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/46DDLBL4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XZ3FNAM8","version":778,"parentItem":"46DDLBL4","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T13:47:41Z","url":"https://arxiv.org/pdf/2111.15640v2.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Preechakul et al. - 2021 - Diffusion Autoencoders Toward a Meaningful and De.pdf","md5":"f72dbd1017bfc3944a5ab768adeef93a","mtime":1638971261000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:47:41Z","dateModified":"2021-12-08T13:47:41Z"}},{"key":"C65HQT3S","version":778,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/C65HQT3S","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/C65HQT3S","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Z8JBRHBB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"C65HQT3S","version":778,"parentItem":"Z8JBRHBB","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T13:47:38Z","url":"https://arxiv.org/abs/2111.14818","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"8ac29a21d2f18eb1a058386cce9601f9","mtime":1638971258000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:47:38Z","dateModified":"2021-12-08T13:47:38Z"}},{"key":"CS3YLBAF","version":778,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CS3YLBAF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CS3YLBAF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Z8JBRHBB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CS3YLBAF","version":778,"parentItem":"Z8JBRHBB","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T13:47:32Z","url":"https://arxiv.org/pdf/2111.14818v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Avrahami et al. - 2021 - Blended Diffusion for Text-driven Editing of Natur.pdf","md5":"138c255d1440787b44a18e3c3d0deeb1","mtime":1638971252000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:47:32Z","dateModified":"2021-12-08T13:47:32Z"}},{"key":"87VY7XPJ","version":774,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/87VY7XPJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/87VY7XPJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RYIVANMF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"87VY7XPJ","version":774,"parentItem":"RYIVANMF","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T13:46:16Z","url":"https://arxiv.org/abs/2111.13606","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"0d9f470c5d9a1fd444764e102fc1785f","mtime":1638971175000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:46:16Z","dateModified":"2021-12-08T13:46:16Z"}},{"key":"NGVXSKP9","version":774,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NGVXSKP9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NGVXSKP9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5QDC8QPA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NGVXSKP9","version":774,"parentItem":"5QDC8QPA","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T13:46:15Z","url":"https://arxiv.org/abs/2112.00390","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"4b389607e7238c9de1ae8817534ee8b3","mtime":1638971175000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:46:15Z","dateModified":"2021-12-08T13:46:15Z"}},{"key":"29BCTMP8","version":774,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/29BCTMP8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/29BCTMP8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E95EE656","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"29BCTMP8","version":774,"parentItem":"E95EE656","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T13:46:14Z","url":"https://arxiv.org/abs/2112.03145","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"63a22e52987cc379c9fddd796f176972","mtime":1638971174000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:46:14Z","dateModified":"2021-12-08T13:46:14Z"}},{"key":"S37EL6AX","version":774,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/S37EL6AX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/S37EL6AX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RYIVANMF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"S37EL6AX","version":774,"parentItem":"RYIVANMF","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T13:46:08Z","url":"https://arxiv.org/pdf/2111.13606v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Batzolis et al. - 2021 - Conditional Image Generation with Score-Based Diff.pdf","md5":"d08f681fa2177ad7a5ae1f18301ab3b7","mtime":1638971168000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:46:08Z","dateModified":"2021-12-08T13:46:08Z"}},{"key":"T78WITGC","version":774,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/T78WITGC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/T78WITGC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E95EE656","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"T78WITGC","version":774,"parentItem":"E95EE656","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T13:46:06Z","url":"https://arxiv.org/pdf/2112.03145v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wolleb et al. - 2021 - Diffusion Models for Implicit Image Segmentation E.pdf","md5":"f8defdce74549d9a66df6e227e83716a","mtime":1638971165000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:46:06Z","dateModified":"2021-12-08T13:46:06Z"}},{"key":"3UHQ89FK","version":774,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3UHQ89FK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3UHQ89FK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5QDC8QPA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3UHQ89FK","version":774,"parentItem":"5QDC8QPA","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T13:46:05Z","url":"https://arxiv.org/pdf/2112.00390v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Amit et al. - 2021 - SegDiff Image Segmentation with Diffusion Probabi.pdf","md5":"295a39e8674baa15bb369072d7ca6be4","mtime":1638971165000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:46:05Z","dateModified":"2021-12-08T13:46:05Z"}},{"key":"I9UCPRA5","version":773,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I9UCPRA5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I9UCPRA5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/FUVVAF8X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"I9UCPRA5","version":773,"parentItem":"FUVVAF8X","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T13:45:53Z","url":"https://arxiv.org/pdf/2112.03126v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Baranchuk et al. - 2021 - Label-Efficient Semantic Segmentation with Diffusi.pdf","md5":"53ba42410362ed3e185378e4011c936f","mtime":1638971153000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:45:53Z","dateModified":"2021-12-08T13:46:02Z"}},{"key":"U9AEICXI","version":773,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U9AEICXI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U9AEICXI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/FUVVAF8X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"U9AEICXI","version":773,"parentItem":"FUVVAF8X","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T13:45:58Z","url":"https://arxiv.org/abs/2112.03126","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"a95d0b435b54a2395d0cd87fbc446755","mtime":1638971158000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:45:58Z","dateModified":"2021-12-08T13:46:02Z"}},{"key":"YFJDRD3S","version":772,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YFJDRD3S","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YFJDRD3S","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/46DDLBL4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YFJDRD3S","version":772,"parentItem":"46DDLBL4","itemType":"note","note":"Comment: Please visit our project page: https://Diff-AE.github.io/","tags":[],"relations":{},"dateAdded":"2021-12-08T13:45:32Z","dateModified":"2021-12-08T13:45:59Z"}},{"key":"46DDLBL4","version":1192,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/46DDLBL4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/46DDLBL4","type":"text/html"}},"meta":{"creatorSummary":"Preechakul et al.","parsedDate":"2021-12-01","numChildren":6},"data":{"key":"46DDLBL4","version":1192,"itemType":"journalArticle","title":"Diffusion Autoencoders: Toward a Meaningful and Decodable Representation","creators":[{"creatorType":"author","firstName":"Konpat","lastName":"Preechakul"},{"creatorType":"author","firstName":"Nattanat","lastName":"Chatthee"},{"creatorType":"author","firstName":"Suttisak","lastName":"Wizadwongsa"},{"creatorType":"author","firstName":"Supasorn","lastName":"Suwajanakorn"}],"abstractNote":"Diffusion probabilistic models (DPMs) have achieved remarkable quality in image generation that rivals GANs'. But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful representation for other tasks. This paper explores the possibility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level semantics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code, where the first part is semantically meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction. This capability enables challenging applications that currently foil GAN-based methods, such as attribute manipulation on real images. We also show that this two-level encoding improves denoising efficiency and naturally facilitates various downstream tasks including few-shot conditional sampling. Please visit our project page: https://Diff-AE.github.io/","publicationTitle":"arXiv:2111.15640 [cs]","volume":"","issue":"","pages":"","date":"2021-12-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Diffusion Autoencoders","url":"http://arxiv.org/abs/2111.15640","accessDate":"2021-12-08T13:45:28Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.15640\nversion: 2","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"dc:replaces":["http://zotero.org/users/7902311/items/ZC6L8WJJ","http://zotero.org/users/7902311/items/FIP3EB3Z"]},"dateAdded":"2021-12-08T13:45:31Z","dateModified":"2022-06-13T16:20:37Z"}},{"key":"SCSR9FTN","version":1184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SCSR9FTN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SCSR9FTN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HZ9IATD5","type":"application/json"}},"meta":{},"data":{"key":"SCSR9FTN","version":1184,"parentItem":"HZ9IATD5","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T13:45:45Z","url":"https://arxiv.org/abs/2111.14822","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"70a73cad26eaab3363b5952234c76386","mtime":1638971145000,"tags":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/CNZHIFCA"},"dateAdded":"2021-12-08T13:45:45Z","dateModified":"2022-06-13T16:19:51Z"}},{"key":"76CSXF93","version":1184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/76CSXF93","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/76CSXF93","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HZ9IATD5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"76CSXF93","version":1184,"parentItem":"HZ9IATD5","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T13:45:31Z","url":"https://arxiv.org/pdf/2111.14822v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Gu et al. - 2021 - Vector Quantized Diffusion Model for Text-to-Image.pdf","md5":"e2c2679ef4f64e0f49426cc70d002bcd","mtime":1638971131000,"tags":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/H4KYLAXH"},"dateAdded":"2021-12-08T13:45:31Z","dateModified":"2022-06-13T16:19:51Z"}},{"key":"YYFBU2CG","version":769,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YYFBU2CG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YYFBU2CG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/46DDLBL4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YYFBU2CG","version":769,"parentItem":"46DDLBL4","itemType":"note","note":"Comment: Please visit our project page: https://Diff-AE.github.io/","tags":[],"relations":{},"dateAdded":"2021-12-08T13:45:31Z","dateModified":"2021-12-08T13:45:31Z"}},{"key":"5QDC8QPA","version":769,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5QDC8QPA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5QDC8QPA","type":"text/html"}},"meta":{"creatorSummary":"Amit et al.","parsedDate":"2021-12-01","numChildren":2},"data":{"key":"5QDC8QPA","version":769,"itemType":"journalArticle","title":"SegDiff: Image Segmentation with Diffusion Probabilistic Models","creators":[{"creatorType":"author","firstName":"Tomer","lastName":"Amit"},{"creatorType":"author","firstName":"Eliya","lastName":"Nachmani"},{"creatorType":"author","firstName":"Tal","lastName":"Shaharbany"},{"creatorType":"author","firstName":"Lior","lastName":"Wolf"}],"abstractNote":"Diffusion Probabilistic Methods are employed for state-of-the-art image generation. In this work, we present a method for extending such models for performing image segmentation. The method learns end-to-end, without relying on a pre-trained backbone. The information in the input image and in the current estimation of the segmentation map is merged by summing the output of two encoders. Additional encoding layers and a decoder are then used to iteratively refine the segmentation map using a diffusion model. Since the diffusion model is probabilistic, it is applied multiple times and the results are merged into a final segmentation map. The new method obtains state-of-the-art results on the Cityscapes validation set, the Vaihingen building segmentation benchmark, and the MoNuSeg dataset.","publicationTitle":"arXiv:2112.00390 [cs]","volume":"","issue":"","pages":"","date":"2021-12-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"SegDiff","url":"http://arxiv.org/abs/2112.00390","accessDate":"2021-12-08T13:45:26Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.00390\nversion: 1","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-08T13:45:30Z","dateModified":"2021-12-08T13:45:30Z"}},{"key":"J2MDNYJG","version":769,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/J2MDNYJG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/J2MDNYJG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Z8JBRHBB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"J2MDNYJG","version":769,"parentItem":"Z8JBRHBB","itemType":"note","note":"Comment: Code will be available at: https://github.com/omriav/blended-diffusion","tags":[],"relations":{},"dateAdded":"2021-12-08T13:45:22Z","dateModified":"2021-12-08T13:45:22Z"}},{"key":"Z8JBRHBB","version":769,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z8JBRHBB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z8JBRHBB","type":"text/html"}},"meta":{"creatorSummary":"Avrahami et al.","parsedDate":"2021-11-29","numChildren":3},"data":{"key":"Z8JBRHBB","version":769,"itemType":"journalArticle","title":"Blended Diffusion for Text-driven Editing of Natural Images","creators":[{"creatorType":"author","firstName":"Omri","lastName":"Avrahami"},{"creatorType":"author","firstName":"Dani","lastName":"Lischinski"},{"creatorType":"author","firstName":"Ohad","lastName":"Fried"}],"abstractNote":"Natural language offers a highly intuitive interface for image editing. In this paper, we introduce the first solution for performing local (region-based) edits in generic natural images, based on a natural language description along with an ROI mask. We achieve our goal by leveraging and combining a pretrained language-image model (CLIP), to steer the edit towards a user-provided text prompt, with a denoising diffusion probabilistic model (DDPM) to generate natural-looking results. To seamlessly fuse the edited region with the unchanged parts of the image, we spatially blend noised versions of the input image with the local text-guided diffusion latent at a progression of noise levels. In addition, we show that adding augmentations to the diffusion process mitigates adversarial results. We compare against several baselines and related methods, both qualitatively and quantitatively, and show that our method outperforms these solutions in terms of overall realism, ability to preserve the background and matching the text. Finally, we show several text-driven editing applications, including adding a new object to an image, removing/replacing/altering existing objects, background replacement, and image extrapolation.","publicationTitle":"arXiv:2111.14818 [cs]","volume":"","issue":"","pages":"","date":"2021-11-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2111.14818","accessDate":"2021-12-08T13:45:20Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.14818\nversion: 1","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-08T13:45:22Z","dateModified":"2021-12-08T13:45:22Z"}},{"key":"RYIVANMF","version":769,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RYIVANMF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RYIVANMF","type":"text/html"}},"meta":{"creatorSummary":"Batzolis et al.","parsedDate":"2021-11-26","numChildren":2},"data":{"key":"RYIVANMF","version":769,"itemType":"journalArticle","title":"Conditional Image Generation with Score-Based Diffusion Models","creators":[{"creatorType":"author","firstName":"Georgios","lastName":"Batzolis"},{"creatorType":"author","firstName":"Jan","lastName":"Stanczuk"},{"creatorType":"author","firstName":"Carola-Bibiane","lastName":"Schönlieb"},{"creatorType":"author","firstName":"Christian","lastName":"Etmann"}],"abstractNote":"Score-based diffusion models have emerged as one of the most promising frameworks for deep generative modelling. In this work we conduct a systematic comparison and theoretical analysis of different approaches to learning conditional probability distributions with score-based diffusion models. In particular, we prove results which provide a theoretical justification for one of the most successful estimators of the conditional score. Moreover, we introduce a multi-speed diffusion framework, which leads to a new estimator for the conditional score, performing on par with previous state-of-the-art approaches. Our theoretical and experimental findings are accompanied by an open source library MSDiff which allows for application and further research of multi-speed diffusion models.","publicationTitle":"arXiv:2111.13606 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-11-26","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2111.13606","accessDate":"2021-12-08T13:45:18Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.13606\nversion: 1","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-08T13:45:18Z","dateModified":"2021-12-08T13:45:18Z"}},{"key":"HZ9IATD5","version":1184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HZ9IATD5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HZ9IATD5","type":"text/html"}},"meta":{"creatorSummary":"Gu et al.","parsedDate":"2021-11-29","numChildren":2},"data":{"key":"HZ9IATD5","version":1184,"itemType":"journalArticle","title":"Vector Quantized Diffusion Model for Text-to-Image Synthesis","creators":[{"creatorType":"author","firstName":"Shuyang","lastName":"Gu"},{"creatorType":"author","firstName":"Dong","lastName":"Chen"},{"creatorType":"author","firstName":"Jianmin","lastName":"Bao"},{"creatorType":"author","firstName":"Fang","lastName":"Wen"},{"creatorType":"author","firstName":"Bo","lastName":"Zhang"},{"creatorType":"author","firstName":"Dongdong","lastName":"Chen"},{"creatorType":"author","firstName":"Lu","lastName":"Yuan"},{"creatorType":"author","firstName":"Baining","lastName":"Guo"}],"abstractNote":"We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.","publicationTitle":"arXiv:2111.14822 [cs]","volume":"","issue":"","pages":"","date":"2021-11-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2111.14822","accessDate":"2021-12-08T13:45:10Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.14822\nversion: 1","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/YT4EWW5Q"},"dateAdded":"2021-12-08T13:45:10Z","dateModified":"2022-06-13T16:19:51Z"}},{"key":"RCNSHIJE","version":766,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RCNSHIJE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RCNSHIJE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/AT2DVW4D","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RCNSHIJE","version":766,"parentItem":"AT2DVW4D","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-12-08T13:40:53Z","url":"https://arxiv.org/pdf/1906.00446.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Razavi et al. - 2019 - Generating Diverse High-Fidelity Images with VQ-VA.pdf","md5":"178558cf95342ec023fb86f6d232b4f9","mtime":1638970853000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:40:53Z","dateModified":"2021-12-08T13:40:53Z"}},{"key":"R3H6W495","version":762,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/R3H6W495","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/R3H6W495","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/V59X4KJH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"R3H6W495","version":762,"parentItem":"V59X4KJH","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-12-08T13:36:48Z","url":"https://arxiv.org/pdf/2007.03898.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Vahdat and Kautz - 2021 - NVAE A Deep Hierarchical Variational Autoencoder.pdf","md5":"cb38a46c19e1e42d1d2c5b6240cdc6ad","mtime":1638970608000,"tags":[],"relations":{},"dateAdded":"2021-12-08T13:36:48Z","dateModified":"2021-12-08T13:36:48Z"}},{"key":"A8GESQ8Q","version":758,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A8GESQ8Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A8GESQ8Q","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XTFR82EL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"A8GESQ8Q","version":758,"parentItem":"XTFR82EL","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-12-08T12:15:02Z","url":"https://arxiv.org/pdf/1905.09883.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Neural Stochastic Differential Equations Deep Lat.pdf","md5":"2dbc7ba63c274ed4835ce8a83097e1f0","mtime":1638965702000,"tags":[],"relations":{},"dateAdded":"2021-12-08T12:15:02Z","dateModified":"2021-12-08T12:15:02Z"}},{"key":"ZN2S4CT5","version":756,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZN2S4CT5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZN2S4CT5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/D525LI7L","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZN2S4CT5","version":756,"parentItem":"D525LI7L","itemType":"note","note":"Comment: Project Page: https://people.csail.mit.edu/xiuming/projects/nerfactor/","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/2ZDIM7TD"},"dateAdded":"2021-10-20T12:51:19Z","dateModified":"2021-12-08T12:08:49Z"}},{"key":"D525LI7L","version":756,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/D525LI7L","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/D525LI7L","type":"text/html"}},"meta":{"creatorSummary":"Zhang et al.","parsedDate":"2021-06-03","numChildren":4},"data":{"key":"D525LI7L","version":756,"itemType":"journalArticle","title":"NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination","creators":[{"creatorType":"author","firstName":"Xiuming","lastName":"Zhang"},{"creatorType":"author","firstName":"Pratul P.","lastName":"Srinivasan"},{"creatorType":"author","firstName":"Boyang","lastName":"Deng"},{"creatorType":"author","firstName":"Paul","lastName":"Debevec"},{"creatorType":"author","firstName":"William T.","lastName":"Freeman"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"}],"abstractNote":"We address the problem of recovering the shape and spatially-varying reflectance of an object from posed multi-view images of the object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and the environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our code and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.","publicationTitle":"arXiv:2106.01970 [cs]","volume":"","issue":"","pages":"","date":"2021-06-03","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"NeRFactor","url":"http://arxiv.org/abs/2106.01970","accessDate":"2021-07-05T13:17:37Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.01970","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":["CPYKW3PF"],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/MRZQCDDS","http://zotero.org/groups/4458581/items/KWNT7KEQ"],"dc:replaces":"http://zotero.org/users/7902311/items/HHDCCJQN"},"dateAdded":"2021-07-05T13:17:37Z","dateModified":"2021-12-08T12:08:49Z"}},{"key":"RELEI973","version":754,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RELEI973","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RELEI973","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/FUVVAF8X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RELEI973","version":754,"parentItem":"FUVVAF8X","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-08T04:28:30Z","url":"https://arxiv.org/abs/2112.03126","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"4c30378e0ea55d17bec6b7e4b667aeaf","mtime":1638937710000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/W9KQS5LZ"},"dateAdded":"2021-12-08T04:28:30Z","dateModified":"2021-12-08T04:28:30Z"}},{"key":"GH3EXKAK","version":754,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GH3EXKAK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GH3EXKAK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/FUVVAF8X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GH3EXKAK","version":754,"parentItem":"FUVVAF8X","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-08T04:28:22Z","url":"https://arxiv.org/pdf/2112.03126v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Baranchuk et al. - 2021 - Label-Efficient Semantic Segmentation with Diffusi.pdf","md5":"53ba42410362ed3e185378e4011c936f","mtime":1638937702000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/BHP6MRRX"},"dateAdded":"2021-12-08T04:28:22Z","dateModified":"2021-12-08T04:28:22Z"}},{"key":"KB7JJ79C","version":745,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KB7JJ79C","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KB7JJ79C","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5DM9G73S","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KB7JJ79C","version":745,"parentItem":"5DM9G73S","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-12-02T06:26:55Z","url":"https://arxiv.org/abs/2110.08985","note":"","contentType":"text/html","charset":"utf-8","filename":"2110.html","md5":"61076ced8dc719fbbc0e5925b6620d30","mtime":1638426415000,"tags":[],"relations":{},"dateAdded":"2021-12-02T06:26:55Z","dateModified":"2021-12-02T06:26:55Z"}},{"key":"I8F9T7K4","version":745,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I8F9T7K4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I8F9T7K4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5DM9G73S","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"I8F9T7K4","version":745,"parentItem":"5DM9G73S","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-12-02T06:26:48Z","url":"https://arxiv.org/pdf/2110.08985.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Gu et al. - 2021 - StyleNeRF A Style-based 3D-Aware Generator for Hi.pdf","md5":"952b0e3182824da4b8be81c3f5733914","mtime":1638426408000,"tags":[],"relations":{},"dateAdded":"2021-12-02T06:26:48Z","dateModified":"2021-12-02T06:26:48Z"}},{"key":"CLKTM5H9","version":742,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CLKTM5H9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CLKTM5H9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5DM9G73S","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CLKTM5H9","version":742,"parentItem":"5DM9G73S","itemType":"note","note":"Comment: 24 pages, 19 figures. Project page: http://jiataogu.me/style_nerf/","tags":[],"relations":{},"dateAdded":"2021-12-02T06:24:10Z","dateModified":"2021-12-02T06:24:10Z"}},{"key":"5DM9G73S","version":742,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5DM9G73S","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5DM9G73S","type":"text/html"}},"meta":{"creatorSummary":"Gu et al.","parsedDate":"2021-10-17","numChildren":3},"data":{"key":"5DM9G73S","version":742,"itemType":"journalArticle","title":"StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis","creators":[{"creatorType":"author","firstName":"Jiatao","lastName":"Gu"},{"creatorType":"author","firstName":"Lingjie","lastName":"Liu"},{"creatorType":"author","firstName":"Peng","lastName":"Wang"},{"creatorType":"author","firstName":"Christian","lastName":"Theobalt"}],"abstractNote":"We propose StyleNeRF, a 3D-aware generative model for photo-realistic high-resolution image synthesis with high multi-view consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize high-resolution images with fine details or yield noticeable 3D-inconsistent artifacts. In addition, many of them lack control over style attributes and explicit 3D camera poses. StyleNeRF integrates the neural radiance field (NeRF) into a style-based generator to tackle the aforementioned challenges, i.e., improving rendering efficiency and 3D consistency for high-resolution image generation. We perform volume rendering only to produce a low-resolution feature map and progressively apply upsampling in 2D to address the first issue. To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs, including a better upsampler and a new regularization loss. With these designs, StyleNeRF can synthesize high-resolution images at interactive rates while preserving 3D consistency at high quality. StyleNeRF also enables control of camera poses and different levels of styles, which can generalize to unseen views. It also supports challenging tasks, including zoom-in and-out, style mixing, inversion, and semantic editing.","publicationTitle":"arXiv:2110.08985 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-10-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"StyleNeRF","url":"http://arxiv.org/abs/2110.08985","accessDate":"2021-12-02T06:24:10Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2110.08985","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-12-02T06:24:10Z","dateModified":"2021-12-02T06:24:10Z"}},{"key":"DI23D5CV","version":721,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DI23D5CV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DI23D5CV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/H3X7GIJ2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DI23D5CV","version":721,"parentItem":"H3X7GIJ2","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-11-22T05:05:57Z","url":"https://arxiv.org/abs/2111.05464","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"03978b576bb89d73451b858e3f64d1d9","mtime":1637557557000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/N449QX75"},"dateAdded":"2021-11-22T05:05:57Z","dateModified":"2021-11-22T05:05:57Z"}},{"key":"77PPGCBU","version":721,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/77PPGCBU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/77PPGCBU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/H3X7GIJ2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"77PPGCBU","version":721,"parentItem":"H3X7GIJ2","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-11-22T05:05:49Z","url":"https://arxiv.org/pdf/2111.05464v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Bai et al. - 2021 - Are Transformers More Robust Than CNNs.pdf","md5":"e91b31175cc3bb113f8db9079dea791e","mtime":1637557549000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/5VYGGVT9"},"dateAdded":"2021-11-22T05:05:49Z","dateModified":"2021-11-22T05:05:49Z"}},{"key":"H3X7GIJ2","version":721,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/H3X7GIJ2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/H3X7GIJ2","type":"text/html"}},"meta":{"creatorSummary":"Bai et al.","parsedDate":"2021-11-09","numChildren":2},"data":{"key":"H3X7GIJ2","version":721,"itemType":"journalArticle","title":"Are Transformers More Robust Than CNNs?","creators":[{"creatorType":"author","firstName":"Yutong","lastName":"Bai"},{"creatorType":"author","firstName":"Jieru","lastName":"Mei"},{"creatorType":"author","firstName":"Alan","lastName":"Yuille"},{"creatorType":"author","firstName":"Cihang","lastName":"Xie"}],"abstractNote":"Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks, recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair & in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations. With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at https://github.com/ytongbai/ViTs-vs-CNNs.","publicationTitle":"arXiv:2111.05464 [cs]","volume":"","issue":"","pages":"","date":"2021-11-09","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2111.05464","accessDate":"2021-11-22T05:05:42Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2111.05464\nversion: 1","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/EX3E9WFY"},"dateAdded":"2021-11-22T05:05:42Z","dateModified":"2021-11-22T05:05:42Z"}},{"key":"XTFR82EL","version":706,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XTFR82EL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XTFR82EL","type":"text/html"}},"meta":{"numChildren":1},"data":{"key":"XTFR82EL","version":706,"itemType":"webpage","title":"Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit | Abstract","creators":[],"abstractNote":"","websiteTitle":"","websiteType":"","date":"","shortTitle":"","url":"https://arxiv.org/abs/1905.09883","accessDate":"2021-10-29T17:52:26Z","language":"","rights":"","extra":"","tags":[],"collections":["TNWL7M5C"],"relations":{},"dateAdded":"2021-10-29T17:52:26Z","dateModified":"2021-10-29T17:52:26Z"}},{"key":"GTFZK4HL","version":697,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GTFZK4HL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GTFZK4HL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NFKN6ZJX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GTFZK4HL","version":697,"parentItem":"NFKN6ZJX","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-21T09:38:29Z","url":"https://arxiv.org/abs/2106.01505","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"30538abe894de5fe4cd2be40fd6ae851","mtime":1634809109000,"tags":[],"relations":{},"dateAdded":"2021-10-21T09:38:29Z","dateModified":"2021-10-21T09:38:29Z"}},{"key":"KBXRKQA7","version":697,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KBXRKQA7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KBXRKQA7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NFKN6ZJX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KBXRKQA7","version":697,"parentItem":"NFKN6ZJX","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-21T09:38:23Z","url":"https://arxiv.org/pdf/2106.01505.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhu et al. - 2021 - Barbershop GAN-based Image Compositing using Segm.pdf","md5":"f0e602a9d0d3b39ab5287fd201041ecb","mtime":1634809103000,"tags":[],"relations":{},"dateAdded":"2021-10-21T09:38:23Z","dateModified":"2021-10-21T09:38:23Z"}},{"key":"SMEA3DG5","version":695,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SMEA3DG5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SMEA3DG5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NFKN6ZJX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SMEA3DG5","version":695,"parentItem":"NFKN6ZJX","itemType":"note","note":"Comment: Project page: https://zpdesu.github.io/Barbershop/ Video: https://youtu.be/ZU-yrAvoJfQ","tags":[],"relations":{},"dateAdded":"2021-10-21T09:36:17Z","dateModified":"2021-10-21T09:36:17Z"}},{"key":"NFKN6ZJX","version":695,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NFKN6ZJX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NFKN6ZJX","type":"text/html"}},"meta":{"creatorSummary":"Zhu et al.","parsedDate":"2021-10-16","numChildren":3},"data":{"key":"NFKN6ZJX","version":695,"itemType":"journalArticle","title":"Barbershop: GAN-based Image Compositing using Segmentation Masks","creators":[{"creatorType":"author","firstName":"Peihao","lastName":"Zhu"},{"creatorType":"author","firstName":"Rameen","lastName":"Abdal"},{"creatorType":"author","firstName":"John","lastName":"Femiani"},{"creatorType":"author","firstName":"Peter","lastName":"Wonka"}],"abstractNote":"Seamlessly blending features from multiple images is extremely challenging because of complex relationships in lighting, geometry, and partial occlusion which cause coupling between different parts of the image. Even though recent work on GANs enables synthesis of realistic hair or faces, it remains difficult to combine them into a single, coherent, and plausible image rather than a disjointed set of image patches. We present a novel solution to image blending, particularly for the problem of hairstyle transfer, based on GAN-inversion. We propose a novel latent space for image blending which is better at preserving detail and encoding spatial information, and propose a new GAN-embedding algorithm which is able to slightly modify images to conform to a common segmentation mask. Our novel representation enables the transfer of the visual properties from multiple reference images including specific details such as moles and wrinkles, and because we do image blending in a latent-space we are able to synthesize images that are coherent. Our approach avoids blending artifacts present in other approaches and finds a globally consistent image. Our results demonstrate a significant improvement over the current state of the art in a user study, with users preferring our blending solution over 95 percent of the time.","publicationTitle":"arXiv:2106.01505 [cs]","volume":"","issue":"","pages":"","date":"2021-10-16","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Barbershop","url":"http://arxiv.org/abs/2106.01505","accessDate":"2021-10-21T09:36:17Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.01505","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":[],"relations":{},"dateAdded":"2021-10-21T09:36:17Z","dateModified":"2021-10-21T09:36:17Z"}},{"key":"HM7T4XF7","version":689,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HM7T4XF7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HM7T4XF7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/46243R87","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HM7T4XF7","version":689,"parentItem":"46243R87","itemType":"attachment","linkMode":"imported_url","title":"Fefferman et al. - 2016 - Testing the manifold hypothesis.pdf","accessDate":"2021-10-18T07:16:12Z","url":"http://www.mit.edu/~mitter/publications/121_Testing_Manifold.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Fefferman et al. - 2016 - Testing the manifold hypothesis.pdf","md5":"8ab8b104fcc3dcd2b7718b6810d01190","mtime":1634541376000,"tags":[],"relations":{},"dateAdded":"2021-10-18T07:16:12Z","dateModified":"2021-10-18T07:16:16Z"}},{"key":"46243R87","version":688,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/46243R87","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/46243R87","type":"text/html"}},"meta":{"creatorSummary":"Fefferman et al.","parsedDate":"2016-02-09","numChildren":1},"data":{"key":"46243R87","version":688,"itemType":"journalArticle","title":"Testing the manifold hypothesis","creators":[{"creatorType":"author","firstName":"Charles","lastName":"Fefferman"},{"creatorType":"author","firstName":"Sanjoy","lastName":"Mitter"},{"creatorType":"author","firstName":"Hariharan","lastName":"Narayanan"}],"abstractNote":"","publicationTitle":"Journal of the American Mathematical Society","volume":"29","issue":"4","pages":"983-1049","date":"2016-2-9","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"J. Amer. Math. Soc.","language":"en","DOI":"10.1090/jams/852","ISSN":"0894-0347, 1088-6834","shortTitle":"","url":"https://www.ams.org/jams/2016-29-04/S0894-0347-2016-00852-4/","accessDate":"2021-10-18T07:16:15Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-10-18T07:16:15Z","dateModified":"2021-10-18T07:16:15Z"}},{"key":"ZTIX794W","version":1183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZTIX794W","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZTIX794W","type":"text/html"}},"meta":{"creatorSummary":"Wu et al.","numChildren":4},"data":{"key":"ZTIX794W","version":1183,"itemType":"journalArticle","title":"StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation","creators":[{"creatorType":"author","firstName":"Zongze","lastName":"Wu"},{"creatorType":"author","firstName":"Dani","lastName":"Lischinski"},{"creatorType":"author","firstName":"Eli","lastName":"Shechtman"}],"abstractNote":"We explore and analyze the latent style space of StyleGAN2, a state-of-the-art architecture for image generation, using models pretrained on several different datasets. We ﬁrst show that StyleSpace, the space of channel-wise style parameters, is signiﬁcantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to control a distinct visual attribute in a highly localized and disentangled manner. Third, we propose a simple method for identifying style channels that control a speciﬁc attribute, using a pretrained classiﬁer or a small number of example images. Manipulation of visual attributes via these StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric. Finally, we demonstrate the applicability of StyleSpace controls to the manipulation of real images. Our ﬁndings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces.","publicationTitle":"","volume":"","issue":"","pages":"10","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"dc:replaces":["http://zotero.org/users/7902311/items/XA7E8JXW","http://zotero.org/users/7902311/items/I6AHWQRP"],"owl:sameAs":"http://zotero.org/groups/4458581/items/YPVRA4EW"},"dateAdded":"2021-10-14T09:40:38Z","dateModified":"2022-06-13T16:19:55Z"}},{"key":"5TWWD7E9","version":682,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5TWWD7E9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5TWWD7E9","type":"text/html"}},"meta":{"creatorSummary":"Kingma et al.","parsedDate":"2021-07-12","numChildren":2},"data":{"key":"5TWWD7E9","version":682,"itemType":"journalArticle","title":"Variational Diffusion Models","creators":[{"creatorType":"author","firstName":"Diederik P.","lastName":"Kingma"},{"creatorType":"author","firstName":"Tim","lastName":"Salimans"},{"creatorType":"author","firstName":"Ben","lastName":"Poole"},{"creatorType":"author","firstName":"Jonathan","lastName":"Ho"}],"abstractNote":"Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to turn the model into a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum.","publicationTitle":"arXiv:2107.00630 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-07-12","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2107.00630","accessDate":"2021-08-30T03:51:56Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2107.00630","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/46BKPUPN","owl:sameAs":"http://zotero.org/groups/4458581/items/TFGEEPJ7"},"dateAdded":"2021-08-30T03:51:56Z","dateModified":"2021-10-15T04:22:23Z"}},{"key":"XJ2W5X4B","version":682,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XJ2W5X4B","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XJ2W5X4B","type":"text/html"}},"meta":{"creatorSummary":"Higgins et al.","parsedDate":"2017","numChildren":1},"data":{"key":"XJ2W5X4B","version":682,"itemType":"journalArticle","title":"β-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK","creators":[{"creatorType":"author","firstName":"Irina","lastName":"Higgins"},{"creatorType":"author","firstName":"Loic","lastName":"Matthey"},{"creatorType":"author","firstName":"Arka","lastName":"Pal"},{"creatorType":"author","firstName":"Christopher","lastName":"Burgess"},{"creatorType":"author","firstName":"Xavier","lastName":"Glorot"},{"creatorType":"author","firstName":"Matthew","lastName":"Botvinick"},{"creatorType":"author","firstName":"Shakir","lastName":"Mohamed"},{"creatorType":"author","firstName":"Alexander","lastName":"Lerchner"}],"abstractNote":"Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artiﬁcial intelligence that is able to learn and reason in the same way that humans do. We introduce β-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modiﬁcation of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter β that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that β-VAE with appropriately tuned β > 1 qualitatively outperforms VAE (β = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also signiﬁcantly outperforms all baselines quantitatively. Unlike InfoGAN, β-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter β, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.","publicationTitle":"","volume":"","issue":"","pages":"22","date":"2017","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/RN8DLV2V"},"dateAdded":"2021-10-14T05:37:02Z","dateModified":"2021-10-15T04:22:21Z"}},{"key":"LHK76U25","version":677,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LHK76U25","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LHK76U25","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8G5V4ZSC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LHK76U25","version":677,"parentItem":"8G5V4ZSC","itemType":"attachment","linkMode":"imported_url","title":"Cozzolino et al. - ID-Reveal Identity-Aware DeepFake Video Detection.pdf","accessDate":"2021-10-14T12:26:23Z","url":"https://openaccess.thecvf.com/content/ICCV2021/papers/Cozzolino_ID-Reveal_Identity-Aware_DeepFake_Video_Detection_ICCV_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Cozzolino et al. - ID-Reveal Identity-Aware DeepFake Video Detection.pdf","md5":"a2783977f1d65d7ce269f2371dcfd314","mtime":1634214383000,"tags":[],"relations":{},"dateAdded":"2021-10-14T12:26:23Z","dateModified":"2021-10-14T12:26:25Z"}},{"key":"8G5V4ZSC","version":677,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8G5V4ZSC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8G5V4ZSC","type":"text/html"}},"meta":{"creatorSummary":"Cozzolino et al.","numChildren":1},"data":{"key":"8G5V4ZSC","version":677,"itemType":"journalArticle","title":"ID-Reveal: Identity-Aware DeepFake Video Detection","creators":[{"creatorType":"author","firstName":"Davide","lastName":"Cozzolino"},{"creatorType":"author","firstName":"Andreas","lastName":"Rossler"},{"creatorType":"author","firstName":"Justus","lastName":"Thies"},{"creatorType":"author","firstName":"Matthias","lastName":"Niessner"},{"creatorType":"author","firstName":"Luisa","lastName":"Verdoliva"}],"abstractNote":"A major challenge in DeepFake forgery detection is that state-of-the-art algorithms are mostly trained to detect a speciﬁc fake method. As a result, these approaches show poor generalization across different types of facial manipulations, e.g., from face swapping to facial reenactment. To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, speciﬁc of how a person moves while talking, by means of metric learning coupled with an adversarial training strategy. The advantage is that we do not need any training data of fakes, but only train on real videos. Moreover, we utilize high-level semantic features, which enables robustness to widespread and disruptive forms of post-processing. We perform a thorough experimental analysis on several publicly available benchmarks. Compared to state of the art, our method improves generalization and is more robust to low-quality videos, that are usually spread over social networks. In particular, we obtain an average improvement of more than 15% in terms of accuracy for facial reenactment on high compressed videos.","publicationTitle":"","volume":"","issue":"","pages":"10","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-10-14T12:26:25Z","dateModified":"2021-10-14T12:26:25Z"}},{"key":"RWVGQXL5","version":677,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RWVGQXL5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RWVGQXL5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/G2DPTBKK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RWVGQXL5","version":677,"parentItem":"G2DPTBKK","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-10-13T08:26:50Z","url":"https://arxiv.org/pdf/2012.09841.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Esser et al. - 2021 - Taming Transformers for High-Resolution Image Synt.pdf","md5":"361f4e4173be15ff63983f24c175c5ea","mtime":1634209121000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/3INJ3VV6"},"dateAdded":"2021-10-14T10:58:40Z","dateModified":"2021-10-14T10:58:40Z"}},{"key":"MSWHXLCZ","version":675,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MSWHXLCZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MSWHXLCZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/V59X4KJH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MSWHXLCZ","version":675,"parentItem":"V59X4KJH","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-13T05:21:51Z","url":"https://arxiv.org/abs/2007.03898","note":"","contentType":"text/html","charset":"utf-8","filename":"2007.html","md5":null,"mtime":null,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/EA9C9R9D"},"dateAdded":"2021-10-14T10:58:40Z","dateModified":"2021-10-14T10:58:40Z"}},{"key":"Y28V65IY","version":675,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y28V65IY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y28V65IY","type":"text/html"}},"meta":{"creatorSummary":"Yu et al.","parsedDate":"2016-06-04","numChildren":0},"data":{"key":"Y28V65IY","version":675,"itemType":"journalArticle","title":"LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop","creators":[{"creatorType":"author","firstName":"Fisher","lastName":"Yu"},{"creatorType":"author","firstName":"Ari","lastName":"Seff"},{"creatorType":"author","firstName":"Yinda","lastName":"Zhang"},{"creatorType":"author","firstName":"Shuran","lastName":"Song"},{"creatorType":"author","firstName":"Thomas","lastName":"Funkhouser"},{"creatorType":"author","firstName":"Jianxiong","lastName":"Xiao"}],"abstractNote":"While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.","publicationTitle":"arXiv:1506.03365 [cs]","volume":"","issue":"","pages":"","date":"2016-06-04","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"LSUN","url":"http://arxiv.org/abs/1506.03365","accessDate":"2021-10-13T05:13:42Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1506.03365","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/C95BPNC8"},"dateAdded":"2021-10-14T10:58:40Z","dateModified":"2021-10-14T10:58:40Z"}},{"key":"AT2DVW4D","version":675,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AT2DVW4D","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AT2DVW4D","type":"text/html"}},"meta":{"creatorSummary":"Razavi et al.","parsedDate":"2019-06-02","numChildren":2},"data":{"key":"AT2DVW4D","version":675,"itemType":"journalArticle","title":"Generating Diverse High-Fidelity Images with VQ-VAE-2","creators":[{"creatorType":"author","firstName":"Ali","lastName":"Razavi"},{"creatorType":"author","firstName":"Aaron van den","lastName":"Oord"},{"creatorType":"author","firstName":"Oriol","lastName":"Vinyals"}],"abstractNote":"We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.","publicationTitle":"arXiv:1906.00446 [cs, stat]","volume":"","issue":"","pages":"","date":"2019-06-02","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1906.00446","accessDate":"2021-10-13T05:18:59Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1906.00446","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/HZRCYFGG"},"dateAdded":"2021-10-14T10:58:40Z","dateModified":"2021-10-14T10:58:40Z"}},{"key":"2IWSAWF6","version":675,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2IWSAWF6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2IWSAWF6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/AT2DVW4D","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2IWSAWF6","version":675,"parentItem":"AT2DVW4D","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-13T05:19:50Z","url":"https://arxiv.org/abs/1906.00446","note":"","contentType":"text/html","charset":"utf-8","filename":"1906.html","md5":null,"mtime":null,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/GBLLTXXY"},"dateAdded":"2021-10-14T10:58:40Z","dateModified":"2021-10-14T10:58:40Z"}},{"key":"FGFRQBLE","version":675,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FGFRQBLE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FGFRQBLE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/V59X4KJH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FGFRQBLE","version":675,"parentItem":"V59X4KJH","itemType":"note","note":"Comment: Neural Information Processing Systems (NeurIPS) 2020 (spotlight)","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/7GZRPW6M"},"dateAdded":"2021-10-14T10:58:40Z","dateModified":"2021-10-14T10:58:40Z"}},{"key":"V59X4KJH","version":675,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/V59X4KJH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/V59X4KJH","type":"text/html"}},"meta":{"creatorSummary":"Vahdat and Kautz","parsedDate":"2021-01-07","numChildren":3},"data":{"key":"V59X4KJH","version":675,"itemType":"journalArticle","title":"NVAE: A Deep Hierarchical Variational Autoencoder","creators":[{"creatorType":"author","firstName":"Arash","lastName":"Vahdat"},{"creatorType":"author","firstName":"Jan","lastName":"Kautz"}],"abstractNote":"Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256$\\times$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .","publicationTitle":"arXiv:2007.03898 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-01-07","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"NVAE","url":"http://arxiv.org/abs/2007.03898","accessDate":"2021-10-13T05:19:14Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2007.03898","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/CJ7MIFDW"},"dateAdded":"2021-10-14T10:58:40Z","dateModified":"2021-10-14T10:58:40Z"}},{"key":"G2DPTBKK","version":675,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/G2DPTBKK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/G2DPTBKK","type":"text/html"}},"meta":{"creatorSummary":"Esser et al.","parsedDate":"2021-06-23","numChildren":1},"data":{"key":"G2DPTBKK","version":675,"itemType":"journalArticle","title":"Taming Transformers for High-Resolution Image Synthesis","creators":[{"creatorType":"author","firstName":"Patrick","lastName":"Esser"},{"creatorType":"author","firstName":"Robin","lastName":"Rombach"},{"creatorType":"author","firstName":"Björn","lastName":"Ommer"}],"abstractNote":"Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .","publicationTitle":"arXiv:2012.09841 [cs]","volume":"","issue":"","pages":"","date":"2021-06-23","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2012.09841","accessDate":"2021-10-13T08:20:50Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.09841","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/SF6TABFW"},"dateAdded":"2021-10-14T10:58:40Z","dateModified":"2021-10-14T10:58:40Z"}},{"key":"BMXSP7V2","version":672,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BMXSP7V2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BMXSP7V2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZTIX794W","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BMXSP7V2","version":672,"parentItem":"ZTIX794W","itemType":"attachment","linkMode":"imported_url","title":"Wu et al. - StyleSpace Analysis Disentangled Controls for Sty.pdf","accessDate":"2021-10-14T09:40:36Z","url":"https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_StyleSpace_Analysis_Disentangled_Controls_for_StyleGAN_Image_Generation_CVPR_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wu et al. - StyleSpace Analysis Disentangled Controls for Sty.pdf","md5":"1e4b0d76a7f2210a58ca46f51ffe40d0","mtime":1634204438000,"tags":[],"relations":{},"dateAdded":"2021-10-14T09:40:36Z","dateModified":"2021-10-14T09:40:38Z"}},{"key":"35E4HTDU","version":669,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/35E4HTDU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/35E4HTDU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XJ2W5X4B","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"35E4HTDU","version":669,"parentItem":"XJ2W5X4B","itemType":"attachment","linkMode":"imported_url","title":"Higgins et al. - 2017 - β-VAE LEARNING BASIC VISUAL CONCEPTS WITH A CONST.pdf","accessDate":"2021-10-14T05:37:00Z","url":"https://openreview.net/pdf?id=Sy2fzU9gl","note":"","contentType":"application/pdf","charset":"","filename":"Higgins et al. - 2017 - β-VAE LEARNING BASIC VISUAL CONCEPTS WITH A CONST.pdf","md5":"240a94da35af5103cfabb0084149e2b8","mtime":1634189822000,"tags":[],"relations":{},"dateAdded":"2021-10-14T05:37:00Z","dateModified":"2021-10-14T05:37:02Z"}},{"key":"UGJUMA42","version":665,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UGJUMA42","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UGJUMA42","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/L4GHVFP3","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UGJUMA42","version":665,"parentItem":"L4GHVFP3","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-10-12T12:35:30Z","url":"https://arxiv.org/pdf/1907.05600.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Song and Ermon - 2020 - Generative Modeling by Estimating Gradients of the.pdf","md5":"1857ba7f8a083fde0d1d76632271cb2a","mtime":1634101282222,"tags":[],"relations":{},"dateAdded":"2021-10-12T12:35:30Z","dateModified":"2021-10-13T05:03:04Z"}},{"key":"3Z6MH3QR","version":665,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3Z6MH3QR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3Z6MH3QR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/L4GHVFP3","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3Z6MH3QR","version":665,"parentItem":"L4GHVFP3","itemType":"note","note":"<p>Comment: NeurIPS 2019 (Oral)</p>","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/98A4SVJF"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-13T05:01:58Z"}},{"key":"JM8HQPJF","version":659,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JM8HQPJF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JM8HQPJF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CULAMFWA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JM8HQPJF","version":659,"parentItem":"CULAMFWA","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-12T02:22:31Z","url":"https://arxiv.org/pdf/2106.06819.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Sinha et al. - 2021 - D2C Diffusion-Denoising Models for Few-shot Condi.pdf","md5":"257f08a74d0e3f6f1b6232bdc7511bec","mtime":1634005351000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/TQ9ME9UT"},"dateAdded":"2021-10-12T02:22:31Z","dateModified":"2021-10-12T12:34:12Z"}},{"key":"25TXVHFZ","version":659,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/25TXVHFZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/25TXVHFZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CULAMFWA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"25TXVHFZ","version":659,"parentItem":"CULAMFWA","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-12T02:22:36Z","url":"https://arxiv.org/abs/2106.06819","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"3159420ca1a95fab6848e64f1adde103","mtime":1634005356000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/YKMQH8Z7"},"dateAdded":"2021-10-12T02:22:36Z","dateModified":"2021-10-12T12:34:12Z"}},{"key":"CULAMFWA","version":659,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CULAMFWA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CULAMFWA","type":"text/html"}},"meta":{"creatorSummary":"Sinha et al.","parsedDate":"2021-06-12","numChildren":5},"data":{"key":"CULAMFWA","version":659,"itemType":"journalArticle","title":"D2C: Diffusion-Denoising Models for Few-shot Conditional Generation","creators":[{"creatorType":"author","firstName":"Abhishek","lastName":"Sinha"},{"creatorType":"author","firstName":"Jiaming","lastName":"Song"},{"creatorType":"author","firstName":"Chenlin","lastName":"Meng"},{"creatorType":"author","firstName":"Stefano","lastName":"Ermon"}],"abstractNote":"Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study.","publicationTitle":"arXiv:2106.06819 [cs]","volume":"","issue":"","pages":"","date":"2021-06-12","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"D2C","url":"http://arxiv.org/abs/2106.06819","accessDate":"2021-07-22T16:31:54Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.06819","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"aek"}],"collections":["TNWL7M5C"],"relations":{"dc:replaces":["http://zotero.org/users/7902311/items/8LW53DI8","http://zotero.org/users/7902311/items/G5B4VQNR","http://zotero.org/users/7902311/items/NX2FQI32"],"owl:sameAs":["http://zotero.org/groups/4320173/items/766JR7U7","http://zotero.org/groups/4458581/items/7P6KTZ57"]},"dateAdded":"2021-07-22T16:31:55Z","dateModified":"2021-10-12T12:34:12Z"}},{"key":"PXJ74TA5","version":711,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PXJ74TA5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PXJ74TA5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UMU2ALXS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PXJ74TA5","version":711,"parentItem":"UMU2ALXS","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-12T12:25:51Z","url":"https://arxiv.org/pdf/2101.03288.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Song and Kingma - 2021 - How to Train Your Energy-Based Models.pdf","md5":"9b9b03dc57a78d82f25f5de16b6446f7","mtime":1634104643925,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4458581/items/WP3GQLL9","http://zotero.org/groups/4320173/items/8BUSTMWL"]},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"UMU2ALXS","version":664,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UMU2ALXS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UMU2ALXS","type":"text/html"}},"meta":{"creatorSummary":"Song and Kingma","parsedDate":"2021-02-17","numChildren":2},"data":{"key":"UMU2ALXS","version":664,"itemType":"journalArticle","title":"How to Train Your Energy-Based Models","creators":[{"creatorType":"author","firstName":"Yang","lastName":"Song"},{"creatorType":"author","firstName":"Diederik P.","lastName":"Kingma"}],"abstractNote":"Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Constrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.","publicationTitle":"arXiv:2101.03288 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-02-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2101.03288","accessDate":"2021-10-12T12:25:45Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2101.03288","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4458581/items/PIFJ5EB9","http://zotero.org/groups/4320173/items/4RV8AS26"]},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"U2AZFLHB","version":664,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U2AZFLHB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U2AZFLHB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UMU2ALXS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"U2AZFLHB","version":664,"parentItem":"UMU2ALXS","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-12T12:25:59Z","url":"https://arxiv.org/abs/2101.03288","note":"","contentType":"text/html","charset":"utf-8","filename":"2101.html","md5":"f06cae41e2c9a8f3a0c1d57706801fb1","mtime":1634041987000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4458581/items/VFZY9IZB","http://zotero.org/groups/4320173/items/J4V9P29N"]},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"TPKH98R8","version":660,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TPKH98R8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TPKH98R8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/J4E7E899","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TPKH98R8","version":660,"parentItem":"J4E7E899","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-12T12:27:08Z","url":"https://arxiv.org/abs/2104.02600","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"38a95c982b799092a98e22ca223eb658","mtime":1634041987000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/M2XU3CIY"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"NLJ2DZAL","version":660,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NLJ2DZAL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NLJ2DZAL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/J4E7E899","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NLJ2DZAL","version":660,"parentItem":"J4E7E899","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-12T12:27:04Z","url":"https://arxiv.org/pdf/2104.02600.pdf","note":"","contentType":"application/pdf","charset":"","filename":"San-Roman et al. - 2021 - Noise Estimation for Generative Diffusion Models.pdf","md5":"0ab39c02cf3c7dd6637249bb15e8b3ce","mtime":1634041987000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/MRVIUGGQ"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"TFKYUVEF","version":659,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TFKYUVEF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TFKYUVEF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8X6KEIIA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TFKYUVEF","version":659,"parentItem":"8X6KEIIA","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-12T12:26:14Z","url":"https://arxiv.org/abs/2103.16091","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"1f32f602967d60d80c3faa1eb8eabf9c","mtime":1634041987000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/JB5CFQRY"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"YABLPHJN","version":659,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YABLPHJN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YABLPHJN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8X6KEIIA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YABLPHJN","version":659,"parentItem":"8X6KEIIA","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-12T12:26:10Z","url":"https://arxiv.org/pdf/2103.16091.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Mittal et al. - 2021 - Symbolic Music Generation with Diffusion Models.pdf","md5":"67caefd552bfa7b9b4bbadf765edee31","mtime":1634041987000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/RE3MMDYN"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"2WUVNLCE","version":659,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2WUVNLCE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2WUVNLCE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/I5JPY5DU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2WUVNLCE","version":659,"parentItem":"I5JPY5DU","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-12T12:26:19Z","url":"https://arxiv.org/abs/2105.06337","note":"","contentType":"text/html","charset":"utf-8","filename":"2105.html","md5":"1d76ea34fecb6442d1ff27d07814b150","mtime":1634041987000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/3SVDU47N"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"EU6IR2BA","version":659,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EU6IR2BA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EU6IR2BA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/I5JPY5DU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EU6IR2BA","version":659,"parentItem":"I5JPY5DU","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-12T12:26:15Z","url":"https://arxiv.org/pdf/2105.06337.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Popov et al. - 2021 - Grad-TTS A Diffusion Probabilistic Model for Text.pdf","md5":"0cc6da290adf434b30e097c0ab7158d8","mtime":1634041987000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/F8T278CR"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"J7L99T5F","version":659,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/J7L99T5F","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/J7L99T5F","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4PP5MBZ7","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"J7L99T5F","version":659,"parentItem":"4PP5MBZ7","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-12T12:26:32Z","url":"https://arxiv.org/abs/2105.02446","note":"","contentType":"text/html","charset":"utf-8","filename":"2105.html","md5":"ed1a50202e881fbd8f149fe370e0168b","mtime":1634041987000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/SYT7JTQD"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"MD3KFHPS","version":659,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MD3KFHPS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MD3KFHPS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4PP5MBZ7","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MD3KFHPS","version":659,"parentItem":"4PP5MBZ7","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-12T12:26:28Z","url":"https://arxiv.org/pdf/2105.02446.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Liu et al. - 2021 - DiffSinger Singing Voice Synthesis via Shallow Di.pdf","md5":"49be3ee88dedd0b2720553c36eacc083","mtime":1634041987000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/YUXSPAE5"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"GI9GUQS6","version":658,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GI9GUQS6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GI9GUQS6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/L4GHVFP3","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GI9GUQS6","version":658,"parentItem":"L4GHVFP3","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-04T08:19:08Z","url":"https://arxiv.org/abs/1907.05600","note":"","contentType":"text/html","charset":"utf-8","filename":"1907.html","md5":null,"mtime":null,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/59B9HMAM"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"L4GHVFP3","version":657,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/L4GHVFP3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/L4GHVFP3","type":"text/html"}},"meta":{"creatorSummary":"Song and Ermon","parsedDate":"2020-10-10","numChildren":3},"data":{"key":"L4GHVFP3","version":657,"itemType":"journalArticle","title":"Generative Modeling by Estimating Gradients of the Data Distribution","creators":[{"creatorType":"author","firstName":"Yang","lastName":"Song"},{"creatorType":"author","firstName":"Stefano","lastName":"Ermon"}],"abstractNote":"We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.","publicationTitle":"arXiv:1907.05600 [cs, stat]","volume":"","issue":"","pages":"","date":"2020-10-10","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1907.05600","accessDate":"2021-08-04T08:18:15Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1907.05600","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1},{"tag":"aek"},{"tag":"aek_star"}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/3RM2GU45"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"J4E7E899","version":657,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/J4E7E899","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/J4E7E899","type":"text/html"}},"meta":{"creatorSummary":"San-Roman et al.","parsedDate":"2021-09-12","numChildren":2},"data":{"key":"J4E7E899","version":657,"itemType":"journalArticle","title":"Noise Estimation for Generative Diffusion Models","creators":[{"creatorType":"author","firstName":"Robin","lastName":"San-Roman"},{"creatorType":"author","firstName":"Eliya","lastName":"Nachmani"},{"creatorType":"author","firstName":"Lior","lastName":"Wolf"}],"abstractNote":"Generative diffusion models have emerged as leading models in speech and image generation. However, in order to perform well with a small number of denoising steps, a costly tuning of the set of noise parameters is needed. In this work, we present a simple and versatile learning scheme that can step-by-step adjust those noise parameters, for any given number of steps, while the previous work needs to retune for each number separately. Furthermore, without modifying the weights of the diffusion model, we are able to significantly improve the synthesis results, for a small number of steps. Our approach comes at a negligible computation cost.","publicationTitle":"arXiv:2104.02600 [cs]","volume":"","issue":"","pages":"","date":"2021-09-12","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2104.02600","accessDate":"2021-10-12T12:25:47Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.02600","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/NKJGIVKH"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"8X6KEIIA","version":657,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8X6KEIIA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8X6KEIIA","type":"text/html"}},"meta":{"creatorSummary":"Mittal et al.","parsedDate":"2021-03-30","numChildren":2},"data":{"key":"8X6KEIIA","version":657,"itemType":"journalArticle","title":"Symbolic Music Generation with Diffusion Models","creators":[{"creatorType":"author","firstName":"Gautam","lastName":"Mittal"},{"creatorType":"author","firstName":"Jesse","lastName":"Engel"},{"creatorType":"author","firstName":"Curtis","lastName":"Hawthorne"},{"creatorType":"author","firstName":"Ian","lastName":"Simon"}],"abstractNote":"Score-based generative models and diffusion probabilistic models have been successful at generating high-quality samples in continuous domains such as images and audio. However, due to their Langevin-inspired sampling mechanisms, their application to discrete and sequential data has been limited. In this work, we present a technique for training diffusion models on sequential data by parameterizing the discrete domain in the continuous latent space of a pre-trained variational autoencoder. Our method is non-autoregressive and learns to generate sequences of latent embeddings through the reverse process and offers parallel generation with a constant number of iterative refinement steps. We apply this technique to modeling symbolic music and show strong unconditional generation and post-hoc conditional infilling results compared to autoregressive language models operating over the same continuous embeddings.","publicationTitle":"arXiv:2103.16091 [cs, eess, stat]","volume":"","issue":"","pages":"","date":"2021-03-30","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2103.16091","accessDate":"2021-10-12T12:25:53Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.16091","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/84BME3C4"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"4PP5MBZ7","version":657,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4PP5MBZ7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4PP5MBZ7","type":"text/html"}},"meta":{"creatorSummary":"Liu et al.","parsedDate":"2021-05-30","numChildren":3},"data":{"key":"4PP5MBZ7","version":657,"itemType":"journalArticle","title":"DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism","creators":[{"creatorType":"author","firstName":"Jinglin","lastName":"Liu"},{"creatorType":"author","firstName":"Chengxi","lastName":"Li"},{"creatorType":"author","firstName":"Yi","lastName":"Ren"},{"creatorType":"author","firstName":"Feiyang","lastName":"Chen"},{"creatorType":"author","firstName":"Peng","lastName":"Liu"},{"creatorType":"author","firstName":"Zhou","lastName":"Zhao"}],"abstractNote":"Singing voice synthesis (SVS) system is built to synthesize high-quality and expressive singing voice, in which the acoustic model generates the acoustic features (e.g., mel-spectrogram) given a music score. Previous singing acoustic models adopt simple loss (e.g., L1 and L2) or generative adversarial network (GAN) to reconstruct the acoustic features, while they suffer from over-smoothing and unstable training issues respectively, which hinder the naturalness of synthesized singing. In this work, we propose DiffSinger, an acoustic model for SVS based on the diffusion probabilistic model. DiffSinger is a parameterized Markov chain which iteratively converts the noise into mel-spectrogram conditioned on the music score. By implicitly optimizing variational bound, DiffSinger can be stably trained and generates realistic outputs. To further improve the voice quality and speed up inference, we introduce a shallow diffusion mechanism to make better use of the prior knowledge learned by the simple loss. Specifically, DiffSinger starts generation at a shallow step smaller than the total number of diffusion steps, according to the intersection of the diffusion trajectories of the ground-truth mel-spectrogram and the one predicted by a simple mel-spectrogram decoder. Besides, we train a boundary prediction network to locate the intersection and determine the shallow step adaptively. The evaluations conducted on the Chinese singing dataset demonstrate that DiffSinger outperforms state-of-the-art SVS work. Our extensional experiments also prove the generalization of DiffSinger on text-to-speech task.","publicationTitle":"arXiv:2105.02446 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-05-30","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"DiffSinger","url":"http://arxiv.org/abs/2105.02446","accessDate":"2021-10-12T12:26:02Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2105.02446","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/3JX8DCF3"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"3Q9GDGWI","version":657,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3Q9GDGWI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3Q9GDGWI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4PP5MBZ7","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3Q9GDGWI","version":657,"parentItem":"4PP5MBZ7","itemType":"note","note":"Comment: acoustic model, singing voice synthesis, text to speech, diffusion model, shallow diffusion","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/STCWXZJT"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"I5JPY5DU","version":657,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I5JPY5DU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I5JPY5DU","type":"text/html"}},"meta":{"creatorSummary":"Popov et al.","parsedDate":"2021-08-05","numChildren":2},"data":{"key":"I5JPY5DU","version":657,"itemType":"journalArticle","title":"Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech","creators":[{"creatorType":"author","firstName":"Vadim","lastName":"Popov"},{"creatorType":"author","firstName":"Ivan","lastName":"Vovk"},{"creatorType":"author","firstName":"Vladimir","lastName":"Gogoryan"},{"creatorType":"author","firstName":"Tasnima","lastName":"Sadekova"},{"creatorType":"author","firstName":"Mikhail","lastName":"Kudinov"}],"abstractNote":"Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We will make the code publicly available shortly.","publicationTitle":"arXiv:2105.06337 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-08-05","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Grad-TTS","url":"http://arxiv.org/abs/2105.06337","accessDate":"2021-10-12T12:25:58Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2105.06337","tags":[{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/2GQWDRZ6"},"dateAdded":"2021-10-12T12:33:07Z","dateModified":"2021-10-12T12:33:07Z"}},{"key":"34CTP56G","version":656,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/34CTP56G","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/34CTP56G","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/42288D5I","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"34CTP56G","version":656,"parentItem":"42288D5I","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-12T03:09:52Z","url":"https://arxiv.org/abs/1610.03483","note":"","contentType":"text/html","charset":"utf-8","filename":"1610.html","md5":"3a5978abd50f9b68f80c8aeb69ce383a","mtime":1634008204000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/MWGGPYQY"},"dateAdded":"2021-10-12T03:10:04Z","dateModified":"2021-10-12T03:10:04Z"}},{"key":"USAVQKF4","version":656,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/USAVQKF4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/USAVQKF4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/42288D5I","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"USAVQKF4","version":656,"parentItem":"42288D5I","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-12T03:09:47Z","url":"https://arxiv.org/pdf/1610.03483.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Mohamed and Lakshminarayanan - 2017 - Learning in Implicit Generative Models.pdf","md5":"600d757c81e0fdfad886384f274f4e8e","mtime":1634008204000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/8LET2HIW"},"dateAdded":"2021-10-12T03:10:04Z","dateModified":"2021-10-12T03:10:04Z"}},{"key":"42288D5I","version":655,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/42288D5I","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/42288D5I","type":"text/html"}},"meta":{"creatorSummary":"Mohamed and Lakshminarayanan","parsedDate":"2017-02-27","numChildren":2},"data":{"key":"42288D5I","version":655,"itemType":"journalArticle","title":"Learning in Implicit Generative Models","creators":[{"creatorType":"author","firstName":"Shakir","lastName":"Mohamed"},{"creatorType":"author","firstName":"Balaji","lastName":"Lakshminarayanan"}],"abstractNote":"Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.","publicationTitle":"arXiv:1610.03483 [cs, stat]","volume":"","issue":"","pages":"","date":"2017-02-27","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1610.03483","accessDate":"2021-10-12T03:09:37Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1610.03483","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Computation","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/LR543BMY"},"dateAdded":"2021-10-12T03:10:04Z","dateModified":"2021-10-12T03:10:04Z"}},{"key":"NU2GWHM9","version":636,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NU2GWHM9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NU2GWHM9","type":"text/html"}},"meta":{"creatorSummary":"Radford et al.","parsedDate":"2021-02-26","numChildren":2},"data":{"key":"NU2GWHM9","version":636,"itemType":"journalArticle","title":"Learning Transferable Visual Models From Natural Language Supervision","creators":[{"creatorType":"author","firstName":"Alec","lastName":"Radford"},{"creatorType":"author","firstName":"Jong Wook","lastName":"Kim"},{"creatorType":"author","firstName":"Chris","lastName":"Hallacy"},{"creatorType":"author","firstName":"Aditya","lastName":"Ramesh"},{"creatorType":"author","firstName":"Gabriel","lastName":"Goh"},{"creatorType":"author","firstName":"Sandhini","lastName":"Agarwal"},{"creatorType":"author","firstName":"Girish","lastName":"Sastry"},{"creatorType":"author","firstName":"Amanda","lastName":"Askell"},{"creatorType":"author","firstName":"Pamela","lastName":"Mishkin"},{"creatorType":"author","firstName":"Jack","lastName":"Clark"},{"creatorType":"author","firstName":"Gretchen","lastName":"Krueger"},{"creatorType":"author","firstName":"Ilya","lastName":"Sutskever"}],"abstractNote":"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.","publicationTitle":"arXiv:2103.00020 [cs]","volume":"","issue":"1","pages":"","date":"2021-02-26","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2103.00020","accessDate":"2021-10-07T06:26:42Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.00020","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-10-07T06:26:42Z","dateModified":"2021-10-11T13:28:44Z"}},{"key":"VFS8S4HE","version":641,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VFS8S4HE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VFS8S4HE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ABEX8L6Y","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VFS8S4HE","version":641,"parentItem":"ABEX8L6Y","itemType":"attachment","linkMode":"imported_url","title":"Power et al. - GROKKING GENERALIZATION BEYOND OVERFIT- TING ON S.pdf","accessDate":"2021-10-11T05:10:55Z","url":"https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Power et al. - GROKKING GENERALIZATION BEYOND OVERFIT- TING ON S.pdf","md5":"ab8a7ad88de44f756041f7502446c9ae","mtime":1633929057000,"tags":[],"relations":{},"dateAdded":"2021-10-11T05:10:55Z","dateModified":"2021-10-11T05:10:57Z"}},{"key":"ABEX8L6Y","version":641,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ABEX8L6Y","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ABEX8L6Y","type":"text/html"}},"meta":{"creatorSummary":"Power et al.","numChildren":1},"data":{"key":"ABEX8L6Y","version":641,"itemType":"journalArticle","title":"GROKKING: GENERALIZATION BEYOND OVERFIT- TING ON SMALL ALGORITHMIC DATASETS","creators":[{"creatorType":"author","firstName":"Alethea","lastName":"Power"},{"creatorType":"author","firstName":"Yuri","lastName":"Burda"},{"creatorType":"author","firstName":"Harri","lastName":"Edwards"},{"creatorType":"author","firstName":"Igor","lastName":"Babuschkin"},{"creatorType":"author","firstName":"Vedant","lastName":"Misra"}],"abstractNote":"In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efﬁciency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of “grokking” a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overﬁtting. We also study generalization as a function of dataset size and ﬁnd that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the ﬁnite training dataset.","publicationTitle":"","volume":"","issue":"","pages":"9","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-10-11T05:10:57Z","dateModified":"2021-10-11T05:10:57Z"}},{"key":"WFF2BHRP","version":631,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WFF2BHRP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WFF2BHRP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/INM63AV3","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WFF2BHRP","version":631,"parentItem":"INM63AV3","itemType":"attachment","linkMode":"imported_url","title":"Snapshot","accessDate":"2021-10-07T14:53:36Z","url":"https://www.econometrics-with-r.org/6-1-omitted-variable-bias.html","note":"","contentType":"text/html","charset":"utf-8","filename":"6-1-omitted-variable-bias.html","md5":"42176226472e78eff53c96156fb766fb","mtime":1633618416000,"tags":[],"relations":{},"dateAdded":"2021-10-07T14:53:36Z","dateModified":"2021-10-07T14:53:36Z"}},{"key":"INM63AV3","version":630,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/INM63AV3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/INM63AV3","type":"text/html"}},"meta":{"creatorSummary":"Schmelzer","numChildren":1},"data":{"key":"INM63AV3","version":630,"itemType":"book","title":"6.1 Omitted Variable Bias | Introduction to Econometrics with R","creators":[{"creatorType":"author","firstName":"Christoph Hanck, Martin Arnold, Alexander Gerber, and Martin","lastName":"Schmelzer"}],"abstractNote":"Beginners with little background in statistics and econometrics often have a hard time understanding the benefits of having programming skills for learning and applying Econometrics. ‘Introduction to Econometrics with R’ is an interactive companion to the well-received textbook ‘Introduction to Econometrics’ by James H. Stock and Mark W. Watson (2015). It gives a gentle introduction to the essentials of R programming and guides students in implementing the empirical applications presented throughout the textbook using the newly aquired skills. This is supported by interactive programming exercises generated with DataCamp Light and integration of interactive visualizations of central concepts which are based on the flexible JavaScript library D3.js.","series":"","seriesNumber":"","volume":"","numberOfVolumes":"","edition":"","place":"","publisher":"","date":"","numPages":"","language":"","ISBN":"","shortTitle":"","url":"https://www.econometrics-with-r.org/","accessDate":"2021-10-07T14:53:30Z","archive":"","archiveLocation":"","libraryCatalog":"www.econometrics-with-r.org","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-10-07T14:53:30Z","dateModified":"2021-10-07T14:53:30Z"}},{"key":"UVGD5JVB","version":628,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UVGD5JVB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UVGD5JVB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NU2GWHM9","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UVGD5JVB","version":628,"parentItem":"NU2GWHM9","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-07T06:27:40Z","url":"https://arxiv.org/abs/2103.00020","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"f7f24bd45049e319bc35241a999995ea","mtime":1633588060000,"tags":[],"relations":{},"dateAdded":"2021-10-07T06:27:40Z","dateModified":"2021-10-07T06:27:40Z"}},{"key":"ATXWNCPC","version":628,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ATXWNCPC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ATXWNCPC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NU2GWHM9","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ATXWNCPC","version":628,"parentItem":"NU2GWHM9","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-07T06:27:27Z","url":"https://arxiv.org/pdf/2103.00020.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf","md5":"1fc956711b1d99f17c09102e6f73ff2e","mtime":1633588047000,"tags":[],"relations":{},"dateAdded":"2021-10-07T06:27:27Z","dateModified":"2021-10-07T06:27:27Z"}},{"key":"WN3SWYF6","version":625,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WN3SWYF6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WN3SWYF6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8WLMRYYS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WN3SWYF6","version":625,"parentItem":"8WLMRYYS","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-10-06T13:38:27Z","url":"https://arxiv.org/pdf/1905.00824.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Sun et al. - 2019 - Single Image Portrait Relighting.pdf","md5":"448ab3d27d56c0b3f4e021cd303fa1e6","mtime":1633527507000,"tags":[],"relations":{},"dateAdded":"2021-10-06T13:38:27Z","dateModified":"2021-10-06T13:38:27Z"}},{"key":"3LWW7AYN","version":624,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3LWW7AYN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3LWW7AYN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8WLMRYYS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3LWW7AYN","version":624,"parentItem":"8WLMRYYS","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-06T13:36:27Z","url":"https://arxiv.org/abs/1905.00824","note":"","contentType":"text/html","charset":"utf-8","filename":"1905.html","md5":"242718d46b8b30041c967c97e565432b","mtime":1633527387000,"tags":[],"relations":{},"dateAdded":"2021-10-06T13:36:27Z","dateModified":"2021-10-06T13:36:27Z"}},{"key":"WZ3GW9QT","version":624,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WZ3GW9QT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WZ3GW9QT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8WLMRYYS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WZ3GW9QT","version":624,"parentItem":"8WLMRYYS","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-06T13:36:21Z","url":"https://arxiv.org/pdf/1905.00824.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Sun et al. - 2019 - Single Image Portrait Relighting.pdf","md5":"448ab3d27d56c0b3f4e021cd303fa1e6","mtime":1633527381000,"tags":[],"relations":{},"dateAdded":"2021-10-06T13:36:21Z","dateModified":"2021-10-06T13:36:21Z"}},{"key":"UFJNRMHT","version":621,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UFJNRMHT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UFJNRMHT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5L2SAIKV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UFJNRMHT","version":621,"parentItem":"5L2SAIKV","itemType":"attachment","linkMode":"imported_url","title":"Deng et al. - 2020 - Disentangled and Controllable Face Image Generatio.pdf","accessDate":"2021-10-06T13:32:06Z","url":"https://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_Disentangled_and_Controllable_Face_Image_Generation_via_3D_Imitative-Contrastive_Learning_CVPR_2020_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Deng et al. - 2020 - Disentangled and Controllable Face Image Generatio.pdf","md5":"4cc8f175a61b313e855949188a013647","mtime":1633527129000,"tags":[],"relations":{},"dateAdded":"2021-10-06T13:32:06Z","dateModified":"2021-10-06T13:32:09Z"}},{"key":"8WLMRYYS","version":617,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8WLMRYYS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8WLMRYYS","type":"text/html"}},"meta":{"creatorSummary":"Sun et al.","parsedDate":"2019-07-12","numChildren":4},"data":{"key":"8WLMRYYS","version":617,"itemType":"journalArticle","title":"Single Image Portrait Relighting","creators":[{"creatorType":"author","firstName":"Tiancheng","lastName":"Sun"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Yun-Ta","lastName":"Tsai"},{"creatorType":"author","firstName":"Zexiang","lastName":"Xu"},{"creatorType":"author","firstName":"Xueming","lastName":"Yu"},{"creatorType":"author","firstName":"Graham","lastName":"Fyffe"},{"creatorType":"author","firstName":"Christoph","lastName":"Rhemann"},{"creatorType":"author","firstName":"Jay","lastName":"Busch"},{"creatorType":"author","firstName":"Paul","lastName":"Debevec"},{"creatorType":"author","firstName":"Ravi","lastName":"Ramamoorthi"}],"abstractNote":"Lighting plays a central role in conveying the essence and depth of the subject in a portrait photograph. Professional photographers will carefully control the lighting in their studio to manipulate the appearance of their subject, while consumer photographers are usually constrained to the illumination of their environment. Though prior works have explored techniques for relighting an image, their utility is usually limited due to requirements of specialized hardware, multiple images of the subject under controlled or known illuminations, or accurate models of geometry and reflectance. To this end, we present a system for portrait relighting: a neural network that takes as input a single RGB image of a portrait taken with a standard cellphone camera in an unconstrained environment, and from that image produces a relit image of that subject as though it were illuminated according to any provided environment map. Our method is trained on a small database of 18 individuals captured under different directional light sources in a controlled light stage setup consisting of a densely sampled sphere of lights. Our proposed technique produces quantitatively superior results on our dataset's validation set compared to prior works, and produces convincing qualitative relighting results on a dataset of hundreds of real-world cellphone portraits. Because our technique can produce a 640 $\\times$ 640 image in only 160 milliseconds, it may enable interactive user-facing photographic applications in the future.","publicationTitle":"ACM Transactions on Graphics","volume":"38","issue":"4","pages":"1-12","date":"2019-07-12","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"ACM Trans. Graph.","language":"","DOI":"10.1145/3306346.3323008","ISSN":"0730-0301, 1557-7368","shortTitle":"","url":"http://arxiv.org/abs/1905.00824","accessDate":"2021-10-06T13:31:14Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1905.00824","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1}],"collections":[],"relations":{},"dateAdded":"2021-10-06T13:31:15Z","dateModified":"2021-10-06T13:31:15Z"}},{"key":"PZH5GNRP","version":617,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PZH5GNRP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PZH5GNRP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8WLMRYYS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PZH5GNRP","version":617,"parentItem":"8WLMRYYS","itemType":"note","note":"Comment: SIGGRAPH 2019 Technical Paper accepted","tags":[],"relations":{},"dateAdded":"2021-10-06T13:31:15Z","dateModified":"2021-10-06T13:31:15Z"}},{"key":"JKHGKWJK","version":615,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JKHGKWJK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JKHGKWJK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BU3G7YG3","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JKHGKWJK","version":615,"parentItem":"BU3G7YG3","itemType":"attachment","linkMode":"imported_url","title":"Zhou et al. - 2019 - Deep Single-Image Portrait Relighting.pdf","accessDate":"2021-10-06T13:29:00Z","url":"https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Deep_Single-Image_Portrait_Relighting_ICCV_2019_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhou et al. - 2019 - Deep Single-Image Portrait Relighting.pdf","md5":"5a04bc4c254235e0fae9570e4bc1b62f","mtime":1633526946000,"tags":[],"relations":{},"dateAdded":"2021-10-06T13:29:00Z","dateModified":"2021-10-06T13:29:06Z"}},{"key":"BU3G7YG3","version":614,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BU3G7YG3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BU3G7YG3","type":"text/html"}},"meta":{"creatorSummary":"Zhou et al.","parsedDate":"2019","numChildren":1},"data":{"key":"BU3G7YG3","version":614,"itemType":"conferencePaper","title":"Deep Single-Image Portrait Relighting","creators":[{"creatorType":"author","firstName":"Hao","lastName":"Zhou"},{"creatorType":"author","firstName":"Sunil","lastName":"Hadap"},{"creatorType":"author","firstName":"Kalyan","lastName":"Sunkavalli"},{"creatorType":"author","firstName":"David","lastName":"Jacobs"}],"abstractNote":"","date":"10/2019","proceedingsTitle":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","conferenceName":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","place":"Seoul, Korea (South)","publisher":"IEEE","volume":"","pages":"7193-7201","series":"","language":"en","DOI":"10.1109/ICCV.2019.00729","ISBN":"978-1-72814-803-8","shortTitle":"","url":"https://ieeexplore.ieee.org/document/9010718/","accessDate":"2021-10-06T13:29:05Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-10-06T13:29:05Z","dateModified":"2021-10-06T13:29:05Z"}},{"key":"FBEY39PK","version":654,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FBEY39PK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FBEY39PK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/U6W2CG9B","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FBEY39PK","version":654,"parentItem":"U6W2CG9B","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-06T09:43:15Z","url":"https://arxiv.org/abs/1907.10786","note":"","contentType":"text/html","charset":"utf-8","filename":"1907.html","md5":"8e5e982d190a9501374ae88c14d2e1cc","mtime":1633513395000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/ZHUEHQZJ"},"dateAdded":"2021-10-06T09:43:15Z","dateModified":"2021-10-06T09:43:15Z"}},{"key":"XAHJ3QRT","version":654,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XAHJ3QRT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XAHJ3QRT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/U6W2CG9B","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XAHJ3QRT","version":654,"parentItem":"U6W2CG9B","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-06T09:43:07Z","url":"https://arxiv.org/pdf/1907.10786.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Shen et al. - 2020 - Interpreting the Latent Space of GANs for Semantic.pdf","md5":"3cc3df440cced84eeaeed4f0e9ca7ab1","mtime":1633513387000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/8MAKW4RR"},"dateAdded":"2021-10-06T09:43:07Z","dateModified":"2021-10-06T09:43:07Z"}},{"key":"WQ8SBJ5R","version":612,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WQ8SBJ5R","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WQ8SBJ5R","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DDRRXKTW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WQ8SBJ5R","version":612,"parentItem":"DDRRXKTW","itemType":"attachment","linkMode":"imported_url","title":"Ghosh et al. - 2020 - GIF Generative Interpretable Faces.pdf","accessDate":"2021-10-06T09:43:02Z","url":"https://files.is.tue.mpg.de/black/papers/3DV_GIF.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Ghosh et al. - 2020 - GIF Generative Interpretable Faces.pdf","md5":"e4919100381da3e75457ebeb1aa8569b","mtime":1633513386000,"tags":[],"relations":{},"dateAdded":"2021-10-06T09:43:02Z","dateModified":"2021-10-06T09:43:06Z"}},{"key":"DDRRXKTW","version":611,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DDRRXKTW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DDRRXKTW","type":"text/html"}},"meta":{"creatorSummary":"Ghosh et al.","parsedDate":"2020","numChildren":1},"data":{"key":"DDRRXKTW","version":611,"itemType":"conferencePaper","title":"GIF: Generative Interpretable Faces","creators":[{"creatorType":"author","firstName":"Partha","lastName":"Ghosh"},{"creatorType":"author","firstName":"Pravir Singh","lastName":"Gupta"},{"creatorType":"author","firstName":"Roy","lastName":"Uziel"},{"creatorType":"author","firstName":"Anurag","lastName":"Ranjan"},{"creatorType":"author","firstName":"Michael J.","lastName":"Black"},{"creatorType":"author","firstName":"Timo","lastName":"Bolkart"}],"abstractNote":"","date":"11/2020","proceedingsTitle":"2020 International Conference on 3D Vision (3DV)","conferenceName":"2020 International Conference on 3D Vision (3DV)","place":"Fukuoka, Japan","publisher":"IEEE","volume":"","pages":"868-878","series":"","language":"en","DOI":"10.1109/3DV50981.2020.00097","ISBN":"978-1-72818-128-8","shortTitle":"GIF","url":"https://ieeexplore.ieee.org/document/9320336/","accessDate":"2021-10-06T09:43:06Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-10-06T09:43:06Z","dateModified":"2021-10-06T09:43:06Z"}},{"key":"2UNQHRR8","version":654,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2UNQHRR8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2UNQHRR8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/U6W2CG9B","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2UNQHRR8","version":654,"parentItem":"U6W2CG9B","itemType":"note","note":"Comment: CVPR2020 camera-ready","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/MHCSCWVX"},"dateAdded":"2021-10-06T09:42:37Z","dateModified":"2021-10-06T09:42:37Z"}},{"key":"U6W2CG9B","version":653,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U6W2CG9B","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U6W2CG9B","type":"text/html"}},"meta":{"creatorSummary":"Shen et al.","parsedDate":"2020-03-31","numChildren":3},"data":{"key":"U6W2CG9B","version":653,"itemType":"journalArticle","title":"Interpreting the Latent Space of GANs for Semantic Face Editing","creators":[{"creatorType":"author","firstName":"Yujun","lastName":"Shen"},{"creatorType":"author","firstName":"Jinjin","lastName":"Gu"},{"creatorType":"author","firstName":"Xiaoou","lastName":"Tang"},{"creatorType":"author","firstName":"Bolei","lastName":"Zhou"}],"abstractNote":"Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation.","publicationTitle":"arXiv:1907.10786 [cs]","volume":"","issue":"","pages":"","date":"2020-03-31","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1907.10786","accessDate":"2021-10-06T09:42:37Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1907.10786","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/72ABGQIZ"},"dateAdded":"2021-10-06T09:42:37Z","dateModified":"2021-10-06T09:42:37Z"}},{"key":"JUFSS4SN","version":652,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JUFSS4SN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JUFSS4SN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S29DZ3PB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JUFSS4SN","version":652,"parentItem":"S29DZ3PB","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-10-04T15:56:04Z","url":"https://arxiv.org/abs/2106.05527","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"8b40ea922db109de768e13d18d74e7b7","mtime":1633362964000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/EFAFXN4I"},"dateAdded":"2021-10-04T15:56:04Z","dateModified":"2021-10-04T15:56:04Z"}},{"key":"AGVIIDF7","version":652,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AGVIIDF7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AGVIIDF7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S29DZ3PB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AGVIIDF7","version":652,"parentItem":"S29DZ3PB","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-10-04T15:56:00Z","url":"https://arxiv.org/pdf/2106.05527.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kim et al. - 2021 - Score Matching Model for Unbounded Data Score.pdf","md5":"550f19399ff914a9707ea916d3bc6cff","mtime":1633362960000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/37UJZWBC"},"dateAdded":"2021-10-04T15:56:00Z","dateModified":"2021-10-04T15:56:00Z"}},{"key":"TD5FS29P","version":642,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TD5FS29P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TD5FS29P","type":"text/html"}},"meta":{"creatorSummary":"Blum et al.","parsedDate":"2020-01-31","numChildren":1},"data":{"key":"TD5FS29P","version":642,"itemType":"book","title":"Foundations of Data Science","creators":[{"creatorType":"author","firstName":"Avrim","lastName":"Blum"},{"creatorType":"author","firstName":"John","lastName":"Hopcroft"},{"creatorType":"author","firstName":"Ravi","lastName":"Kannan"}],"abstractNote":"","series":"","seriesNumber":"","volume":"","numberOfVolumes":"","edition":"1","place":"","publisher":"Cambridge University Press","date":"2020-01-31","numPages":"","language":"en","ISBN":"978-1-108-75552-8 978-1-108-48506-7","shortTitle":"","url":"https://www.cambridge.org/core/product/identifier/9781108755528/type/book","accessDate":"2021-10-04T15:55:05Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"DOI: 10.1017/9781108755528","tags":[],"collections":[],"relations":{},"dateAdded":"2021-10-04T15:55:05Z","dateModified":"2021-10-04T15:55:05Z"}},{"key":"Z3SDDJQ4","version":605,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z3SDDJQ4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z3SDDJQ4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/TD5FS29P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Z3SDDJQ4","version":605,"parentItem":"TD5FS29P","itemType":"attachment","linkMode":"imported_url","title":"Blum et al. - 2020 - Foundations of Data Science.pdf","accessDate":"2021-10-04T15:54:59Z","url":"https://www.cs.cornell.edu/jeh/book.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Blum et al. - 2020 - Foundations of Data Science.pdf","md5":"3395ee0200031e7928845cf3f48d16a4","mtime":1633362905000,"tags":[],"relations":{},"dateAdded":"2021-10-04T15:54:59Z","dateModified":"2021-10-04T15:55:05Z"}},{"key":"ZUFJW526","version":652,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZUFJW526","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZUFJW526","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S29DZ3PB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZUFJW526","version":652,"parentItem":"S29DZ3PB","itemType":"note","note":"Comment: 20 pages, 14 figures","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/G67MXIG5"},"dateAdded":"2021-10-04T15:53:41Z","dateModified":"2021-10-04T15:53:41Z"}},{"key":"S29DZ3PB","version":649,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/S29DZ3PB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/S29DZ3PB","type":"text/html"}},"meta":{"creatorSummary":"Kim et al.","parsedDate":"2021-09-15","numChildren":3},"data":{"key":"S29DZ3PB","version":649,"itemType":"journalArticle","title":"Score Matching Model for Unbounded Data Score","creators":[{"creatorType":"author","firstName":"Dongjun","lastName":"Kim"},{"creatorType":"author","firstName":"Seungjae","lastName":"Shin"},{"creatorType":"author","firstName":"Kyungwoo","lastName":"Song"},{"creatorType":"author","firstName":"Wanmo","lastName":"Kang"},{"creatorType":"author","firstName":"Il-Chul","lastName":"Moon"}],"abstractNote":"Recent advance in diffusion models incorporates the Stochastic Differential Equation (SDE), which brings the state-of-the art performance on image generation tasks. This paper improves such diffusion models by analyzing the model at the zero diffusion time. In real datasets, the score function diverges as the diffusion time ($t$) decreases to zero, and this observation leads an argument that the score estimation fails at $t=0$ with any neural network structure. Subsequently, we introduce Unbounded Diffusion Model (UDM) that resolves the score diverging problem with an easily applicable modification to any diffusion models. Additionally, we introduce a new SDE that overcomes the theoretic and practical limitations of Variance Exploding SDE. On top of that, the introduced Soft Truncation method improves the sample quality by mitigating the loss scale issue that happens at $t=0$. We further provide a theoretic result of the proposed method to uncover the behind mechanism of the diffusion models.","publicationTitle":"arXiv:2106.05527 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-09-15","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2106.05527","accessDate":"2021-10-04T15:53:41Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.05527","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/UFDVYSF6"},"dateAdded":"2021-10-04T15:53:41Z","dateModified":"2021-10-04T15:53:41Z"}},{"key":"M4SVJTHJ","version":592,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/M4SVJTHJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/M4SVJTHJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8PEEG5WI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"M4SVJTHJ","version":592,"parentItem":"8PEEG5WI","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-10-02T08:14:14Z","url":"https://arxiv.org/pdf/2103.03891.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Saha et al. - 2021 - LOHO Latent Optimization of Hairstyles via Orthog.pdf","md5":"39590c33a26bb3fce04b1e1fdcdb5c2e","mtime":1633162454000,"tags":[],"relations":{},"dateAdded":"2021-10-02T08:14:14Z","dateModified":"2021-10-02T08:14:14Z"}},{"key":"B9VTQNS7","version":651,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/B9VTQNS7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/B9VTQNS7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ISYURXMF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"B9VTQNS7","version":651,"parentItem":"ISYURXMF","itemType":"attachment","linkMode":"imported_url","title":"Song et al. - 2021 - DENOISING DIFFUSION IMPLICIT MODELS.pdf","accessDate":"2021-10-01T14:25:37Z","url":"https://openreview.net/pdf?id=St1giarCHLP","note":"","contentType":"application/pdf","charset":"","filename":"Song et al. - 2021 - DENOISING DIFFUSION IMPLICIT MODELS.pdf","md5":"4a8dafafdd28ddeab007594ffc8e6cf1","mtime":1633098340000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/FWCE4PXD"},"dateAdded":"2021-10-01T14:25:37Z","dateModified":"2021-10-02T06:12:37Z"}},{"key":"ISYURXMF","version":648,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ISYURXMF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ISYURXMF","type":"text/html"}},"meta":{"creatorSummary":"Song et al.","parsedDate":"2020-10-06","numChildren":3},"data":{"key":"ISYURXMF","version":648,"itemType":"journalArticle","title":"Denoising Diffusion Implicit Models","creators":[{"creatorType":"author","firstName":"Jiaming","lastName":"Song"},{"creatorType":"author","firstName":"Chenlin","lastName":"Meng"},{"creatorType":"author","firstName":"Stefano","lastName":"Ermon"}],"abstractNote":"Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.","publicationTitle":"arXiv:2010.02502 [cs]","volume":"","issue":"","pages":"","date":"2020-10-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2010.02502","accessDate":"2021-09-13T16:55:46Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2010.02502","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/WBPEN649","http://zotero.org/groups/4458581/items/35CJU9Q9"],"dc:replaces":"http://zotero.org/users/7902311/items/KJB8TPB5"},"dateAdded":"2021-09-13T16:55:46Z","dateModified":"2021-10-02T06:12:37Z"}},{"key":"JZNWX9JG","version":650,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JZNWX9JG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JZNWX9JG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HT98Y9ZU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JZNWX9JG","version":650,"parentItem":"HT98Y9ZU","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-04T07:03:15Z","url":"https://arxiv.org/abs/2011.13456","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"04941ac66ccf3a8a8af32f0eac4c7b6c","mtime":1628060595000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/ZEICNPNT","http://zotero.org/groups/4458581/items/U2GVAI32"]},"dateAdded":"2021-08-04T07:03:15Z","dateModified":"2021-09-30T17:11:27Z"}},{"key":"BNE23JB9","version":650,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BNE23JB9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BNE23JB9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HT98Y9ZU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BNE23JB9","version":650,"parentItem":"HT98Y9ZU","itemType":"note","note":"Comment: ICLR 2021 (Oral)","tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/DAVPSVMA","http://zotero.org/groups/4458581/items/5CX88N5U"]},"dateAdded":"2021-08-04T07:00:31Z","dateModified":"2021-09-30T17:11:27Z"}},{"key":"HT98Y9ZU","version":648,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HT98Y9ZU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HT98Y9ZU","type":"text/html"}},"meta":{"creatorSummary":"Song et al.","parsedDate":"2021-02-10","numChildren":5},"data":{"key":"HT98Y9ZU","version":648,"itemType":"journalArticle","title":"Score-Based Generative Modeling through Stochastic Differential Equations","creators":[{"creatorType":"author","firstName":"Yang","lastName":"Song"},{"creatorType":"author","firstName":"Jascha","lastName":"Sohl-Dickstein"},{"creatorType":"author","firstName":"Diederik P.","lastName":"Kingma"},{"creatorType":"author","firstName":"Abhishek","lastName":"Kumar"},{"creatorType":"author","firstName":"Stefano","lastName":"Ermon"},{"creatorType":"author","firstName":"Ben","lastName":"Poole"}],"abstractNote":"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.","publicationTitle":"arXiv:2011.13456 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-02-10","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2011.13456","accessDate":"2021-06-26T16:16:48Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.13456","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1},{"tag":"read"}],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/73KSBICK","http://zotero.org/groups/4458581/items/JNVXTXEY"],"dc:replaces":"http://zotero.org/users/7902311/items/YGD6PLK9"},"dateAdded":"2021-06-26T16:16:48Z","dateModified":"2021-09-30T17:11:27Z"}},{"key":"86LEGTUN","version":583,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/86LEGTUN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/86LEGTUN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RF7ID2P5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"86LEGTUN","version":583,"parentItem":"RF7ID2P5","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-23T13:29:28Z","url":"https://arxiv.org/pdf/2011.13775.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Anokhin et al. - 2020 - Image Generators with Conditionally-Independent Pi.pdf","md5":"924f16fa45221a69fec7439ee827a585","mtime":1629725368000,"tags":[],"relations":{},"dateAdded":"2021-08-23T13:29:28Z","dateModified":"2021-09-30T17:11:27Z"}},{"key":"R8BG2IUB","version":583,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/R8BG2IUB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/R8BG2IUB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RF7ID2P5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"R8BG2IUB","version":583,"parentItem":"RF7ID2P5","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-23T13:29:32Z","url":"https://arxiv.org/abs/2011.13775","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"5198382060d1f7c143b23f3c7a709833","mtime":1629725372000,"tags":[],"relations":{},"dateAdded":"2021-08-23T13:29:32Z","dateModified":"2021-09-30T17:11:27Z"}},{"key":"RF7ID2P5","version":582,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RF7ID2P5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RF7ID2P5","type":"text/html"}},"meta":{"creatorSummary":"Anokhin et al.","parsedDate":"2020-11-27","numChildren":6},"data":{"key":"RF7ID2P5","version":582,"itemType":"journalArticle","title":"Image Generators with Conditionally-Independent Pixel Synthesis","creators":[{"creatorType":"author","firstName":"Ivan","lastName":"Anokhin"},{"creatorType":"author","firstName":"Kirill","lastName":"Demochkin"},{"creatorType":"author","firstName":"Taras","lastName":"Khakhulin"},{"creatorType":"author","firstName":"Gleb","lastName":"Sterkin"},{"creatorType":"author","firstName":"Victor","lastName":"Lempitsky"},{"creatorType":"author","firstName":"Denis","lastName":"Korzhenkov"}],"abstractNote":"Existing image generator networks rely heavily on spatial convolutions and, optionally, self-attention blocks in order to gradually synthesize images in a coarse-to-fine manner. Here, we present a new architecture for image generators, where the color value at each pixel is computed independently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are involved during the synthesis. We analyze the modeling capabilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve similar generation quality to state-of-the-art convolutional generators. We also investigate several interesting properties unique to the new architecture.","publicationTitle":"arXiv:2011.13775 [cs]","volume":"","issue":"","pages":"","date":"2020-11-27","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2011.13775","accessDate":"2021-06-20T16:03:51Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.13775","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":["LT39298Q"],"relations":{"dc:replaces":["http://zotero.org/users/7902311/items/X5PMFWUL","http://zotero.org/users/7902311/items/TQH2EHNS"],"owl:sameAs":"http://zotero.org/groups/4320173/items/563ANXWP"},"dateAdded":"2021-06-20T16:03:52Z","dateModified":"2021-09-30T17:11:27Z"}},{"key":"DAITMCEH","version":584,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DAITMCEH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DAITMCEH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CULAMFWA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DAITMCEH","version":584,"parentItem":"CULAMFWA","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-06T16:01:58Z","url":"https://arxiv.org/abs/2106.06819","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"93e33edf85ce4d538aa35d7b3fa2b1c4","mtime":1630944335000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/GGX7HZ8I"},"dateAdded":"2021-09-06T16:05:35Z","dateModified":"2021-09-30T17:11:26Z"}},{"key":"HBEDQXKX","version":584,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HBEDQXKX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HBEDQXKX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CULAMFWA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HBEDQXKX","version":584,"parentItem":"CULAMFWA","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-06T16:01:52Z","url":"https://arxiv.org/pdf/2106.06819.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Sinha et al. - 2021 - D2C Diffusion-Denoising Models for Few-shot Condi.pdf","md5":"257f08a74d0e3f6f1b6232bdc7511bec","mtime":1630944335000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/I7AHMXHM"},"dateAdded":"2021-09-06T16:05:35Z","dateModified":"2021-09-30T17:11:26Z"}},{"key":"7H7KEAM9","version":583,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7H7KEAM9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7H7KEAM9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HSN56RWV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7H7KEAM9","version":583,"parentItem":"HSN56RWV","itemType":"note","note":"Comment: 19 pages, 17 figures","tags":[],"relations":{},"dateAdded":"2021-08-23T13:27:44Z","dateModified":"2021-09-30T17:11:26Z"}},{"key":"ETE94AIK","version":583,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ETE94AIK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ETE94AIK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HSN56RWV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ETE94AIK","version":583,"parentItem":"HSN56RWV","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-23T13:28:40Z","url":"https://arxiv.org/abs/2011.12026","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"a83f61fca98d102cc76c9588a74253f7","mtime":1629725320000,"tags":[],"relations":{},"dateAdded":"2021-08-23T13:28:40Z","dateModified":"2021-09-30T17:11:26Z"}},{"key":"69CASKK2","version":583,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/69CASKK2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/69CASKK2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HSN56RWV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"69CASKK2","version":583,"parentItem":"HSN56RWV","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-23T13:28:35Z","url":"https://arxiv.org/pdf/2011.12026.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Skorokhodov et al. - 2021 - Adversarial Generation of Continuous Images.pdf","md5":"996764a832ab309fa3f6b38902fec6fe","mtime":1629725315000,"tags":[],"relations":{},"dateAdded":"2021-08-23T13:28:35Z","dateModified":"2021-09-30T17:11:26Z"}},{"key":"HSN56RWV","version":582,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HSN56RWV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HSN56RWV","type":"text/html"}},"meta":{"creatorSummary":"Skorokhodov et al.","parsedDate":"2020-11-24","numChildren":6},"data":{"key":"HSN56RWV","version":582,"itemType":"journalArticle","title":"Adversarial Generation of Continuous Images","creators":[{"creatorType":"author","firstName":"Ivan","lastName":"Skorokhodov"},{"creatorType":"author","firstName":"Savva","lastName":"Ignatyev"},{"creatorType":"author","firstName":"Mohamed","lastName":"Elhoseiny"}],"abstractNote":"In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) -- an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed architectural design improves the performance of continuous image generators by x6-40 times and reaches FID scores of 6.27 on LSUN bedroom 256x256 and 16.32 on FFHQ 1024x1024, greatly reducing the gap between continuous image GANs and pixel-based ones. To the best of our knowledge, these are the highest reported scores for an image generator, that consists entirely of fully-connected layers. Apart from that, we explore several exciting properties of INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries and strong geometric prior. The source code is available at https://github.com/universome/inr-gan","publicationTitle":"arXiv:2011.12026 [cs]","volume":"","issue":"","pages":"","date":"2020-11-24","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2011.12026","accessDate":"2021-06-20T16:03:49Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.12026","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/D6W2XS3A"},"dateAdded":"2021-06-20T16:03:49Z","dateModified":"2021-09-30T17:11:26Z"}},{"key":"CWB3GX22","version":651,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CWB3GX22","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CWB3GX22","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6BIBVQ7P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CWB3GX22","version":651,"parentItem":"6BIBVQ7P","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-27T05:18:21Z","url":"https://arxiv.org/pdf/2106.00132.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kong and Ping - 2021 - On Fast Sampling of Diffusion Probabilistic Models.pdf","md5":"a42c6e5bcda9bc1f059ec08589a540f7","mtime":1632719901000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/B2ZEXI2V"},"dateAdded":"2021-09-27T05:18:21Z","dateModified":"2021-09-30T17:11:25Z"}},{"key":"TGMUY3AH","version":651,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TGMUY3AH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TGMUY3AH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6BIBVQ7P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TGMUY3AH","version":651,"parentItem":"6BIBVQ7P","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-27T05:18:25Z","url":"https://arxiv.org/abs/2106.00132","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"451df2c5e8a6de5a6779f7dd1849d6e4","mtime":1632719905000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/W3RU7FJX"},"dateAdded":"2021-09-27T05:18:25Z","dateModified":"2021-09-30T17:11:25Z"}},{"key":"Q8LSDZDM","version":651,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Q8LSDZDM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Q8LSDZDM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6BIBVQ7P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Q8LSDZDM","version":651,"parentItem":"6BIBVQ7P","itemType":"note","note":"Comment: Code is released","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/8XZQXXDX"},"dateAdded":"2021-09-27T05:17:50Z","dateModified":"2021-09-30T17:11:25Z"}},{"key":"6BIBVQ7P","version":648,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6BIBVQ7P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6BIBVQ7P","type":"text/html"}},"meta":{"creatorSummary":"Kong and Ping","parsedDate":"2021-06-23","numChildren":6},"data":{"key":"6BIBVQ7P","version":648,"itemType":"journalArticle","title":"On Fast Sampling of Diffusion Probabilistic Models","creators":[{"creatorType":"author","firstName":"Zhifeng","lastName":"Kong"},{"creatorType":"author","firstName":"Wei","lastName":"Ping"}],"abstractNote":"In this work, we propose FastDPM, a unified framework for fast sampling in diffusion probabilistic models. FastDPM generalizes previous methods and gives rise to new algorithms with improved sample quality. We systematically investigate the fast sampling methods under this framework across different domains, on different datasets, and with different amount of conditional information provided for generation. We find the performance of a particular method depends on data domains (e.g., image or audio), the trade-off between sampling speed and sample quality, and the amount of conditional information. We further provide insights and recipes on the choice of methods for practitioners.","publicationTitle":"arXiv:2106.00132 [cs]","volume":"","issue":"","pages":"","date":"2021-06-23","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2106.00132","accessDate":"2021-06-28T04:26:24Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.00132","tags":[{"tag":"Computer Science - Machine Learning","type":1}],"collections":["TNWL7M5C"],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/AKAHQUX3","owl:sameAs":"http://zotero.org/groups/4458581/items/6984LD6N"},"dateAdded":"2021-06-28T04:26:24Z","dateModified":"2021-09-30T17:11:25Z"}},{"key":"LGLWJHAV","version":584,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LGLWJHAV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LGLWJHAV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9LNPDQAR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LGLWJHAV","version":584,"parentItem":"9LNPDQAR","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-27T07:37:26Z","url":"https://arxiv.org/abs/2104.02862","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"cce7812984bb0cfcfdd8d863da4223ce","mtime":1632728246000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/ND8EJ7KZ"},"dateAdded":"2021-09-27T07:37:26Z","dateModified":"2021-09-30T17:11:24Z"}},{"key":"REFJHZYQ","version":584,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/REFJHZYQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/REFJHZYQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9LNPDQAR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"REFJHZYQ","version":584,"parentItem":"9LNPDQAR","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-27T07:37:10Z","url":"https://arxiv.org/pdf/2104.02862.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tian et al. - 2021 - Farewell to Mutual Information Variational Distil.pdf","md5":"735109230c68c319b748ca95ae04c922","mtime":1632728230000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/TR52N97U"},"dateAdded":"2021-09-27T07:37:10Z","dateModified":"2021-09-30T17:11:24Z"}},{"key":"9LNPDQAR","version":582,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9LNPDQAR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9LNPDQAR","type":"text/html"}},"meta":{"creatorSummary":"Tian et al.","numChildren":3},"data":{"key":"9LNPDQAR","version":582,"itemType":"journalArticle","title":"Farewell to Mutual Information: Variational Distillation for Cross-Modal Person Re-Identification","creators":[{"creatorType":"author","firstName":"Xudong","lastName":"Tian"},{"creatorType":"author","firstName":"Zhizhong","lastName":"Zhang"},{"creatorType":"author","firstName":"Shaohui","lastName":"Lin"},{"creatorType":"author","firstName":"Yanyun","lastName":"Qu"},{"creatorType":"author","firstName":"Yuan","lastName":"Xie"},{"creatorType":"author","firstName":"Lizhuang","lastName":"Ma"}],"abstractNote":"The Information Bottleneck (IB) provides an information theoretic principle for representation learning, by retaining all information relevant for predicting label while minimizing the redundancy. Though IB principle has been applied to a wide range of applications, its optimization remains a challenging problem which heavily relies on the accurate estimation of mutual information. In this paper, we present a new strategy, Variational Self-Distillation (VSD), which provides a scalable, ﬂexible and analytic solution to essentially ﬁtting the mutual information but without explicitly estimating it. Under rigorously theoretical guarantee, VSD enables the IB to grasp the intrinsic correlation between representation and label for supervised training. Furthermore, by extending VSD to multi-view learning, we introduce two other strategies, Variational Cross-Distillation (VCD) and Variational Mutual-Learning (VML), which signiﬁcantly improve the robustness of representation to viewchanges by eliminating view-speciﬁc and task-irrelevant information. To verify our theoretically grounded strategies, we apply our approaches to cross-modal person Re-ID, and conduct extensive experiments, where the superior performance against state-of-the-art methods are demonstrated. Our intriguing ﬁndings highlight the need to rethink the way to estimate mutual information.","publicationTitle":"","volume":"","issue":"","pages":"10","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/RT7KAHYC","owl:sameAs":"http://zotero.org/groups/4320173/items/KVPWSBSJ"},"dateAdded":"2021-09-16T10:21:00Z","dateModified":"2021-09-30T17:11:24Z"}},{"key":"DFD78R2M","version":652,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DFD78R2M","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DFD78R2M","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LI4EMUQM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DFD78R2M","version":652,"parentItem":"LI4EMUQM","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-27T09:16:25Z","url":"https://arxiv.org/abs/2106.05931","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"ddbc37d9f72cb9274ee559709c1958bf","mtime":1632734185000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/KQRQ93ZL"},"dateAdded":"2021-09-27T09:16:25Z","dateModified":"2021-09-30T17:11:23Z"}},{"key":"L3VT6W76","version":652,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/L3VT6W76","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/L3VT6W76","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LI4EMUQM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"L3VT6W76","version":652,"parentItem":"LI4EMUQM","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-27T09:16:20Z","url":"https://arxiv.org/pdf/2106.05931.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Vahdat et al. - 2021 - Score-based Generative Modeling in Latent Space.pdf","md5":"65494f6b0ee848c2ec5d32ac74846412","mtime":1632734180000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/DSMILQWP"},"dateAdded":"2021-09-27T09:16:20Z","dateModified":"2021-09-30T17:11:23Z"}},{"key":"LI4EMUQM","version":648,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LI4EMUQM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LI4EMUQM","type":"text/html"}},"meta":{"creatorSummary":"Vahdat et al.","parsedDate":"2021-06-10","numChildren":4},"data":{"key":"LI4EMUQM","version":648,"itemType":"journalArticle","title":"Score-based Generative Modeling in Latent Space","creators":[{"creatorType":"author","firstName":"Arash","lastName":"Vahdat"},{"creatorType":"author","firstName":"Karsten","lastName":"Kreis"},{"creatorType":"author","firstName":"Jan","lastName":"Kautz"}],"abstractNote":"Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset.","publicationTitle":"arXiv:2106.05931 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-06-10","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2106.05931","accessDate":"2021-09-27T05:17:53Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.05931","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["TNWL7M5C"],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/56VXC5I9","owl:sameAs":"http://zotero.org/groups/4458581/items/E2NIKKPF"},"dateAdded":"2021-09-27T05:17:53Z","dateModified":"2021-09-30T17:11:23Z"}},{"key":"6TPTN29X","version":580,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6TPTN29X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6TPTN29X","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HE5GMGRK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6TPTN29X","version":580,"parentItem":"HE5GMGRK","itemType":"attachment","linkMode":"imported_url","title":"Evans - AN INTRODUCTION TO STOCHASTIC DIFFERENTIAL EQUATIO.pdf","accessDate":"2021-09-30T17:07:02Z","url":"http://ft-sipil.unila.ac.id/dbooks/AN%20INTRODUCTION%20TO%20STOCHASTIC%20DIFFERENTIAL%20EQUATIONS%20VERSION%201.2.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Evans - AN INTRODUCTION TO STOCHASTIC DIFFERENTIAL EQUATIO.pdf","md5":"9fe10c01960e1b8e8615df83382fb581","mtime":1633021624000,"tags":[],"relations":{},"dateAdded":"2021-09-30T17:07:02Z","dateModified":"2021-09-30T17:07:05Z"}},{"key":"HE5GMGRK","version":579,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HE5GMGRK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HE5GMGRK","type":"text/html"}},"meta":{"creatorSummary":"Evans","numChildren":1},"data":{"key":"HE5GMGRK","version":579,"itemType":"journalArticle","title":"AN INTRODUCTION TO STOCHASTIC DIFFERENTIAL EQUATIONS VERSION 1.2","creators":[{"creatorType":"author","firstName":"Lawrence C","lastName":"Evans"}],"abstractNote":"","publicationTitle":"","volume":"","issue":"","pages":"139","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":["TNWL7M5C"],"relations":{},"dateAdded":"2021-09-30T17:07:04Z","dateModified":"2021-09-30T17:07:04Z"}},{"key":"YDP5UBRT","version":652,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YDP5UBRT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YDP5UBRT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NNFN4KE2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YDP5UBRT","version":652,"parentItem":"NNFN4KE2","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-30T14:56:39Z","url":"https://arxiv.org/abs/2108.01073","note":"","contentType":"text/html","charset":"utf-8","filename":"2108.html","md5":"43af9cae3f73d4d1adeacfa1e2f2fa05","mtime":1633013799000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/ZGP9VURV"},"dateAdded":"2021-09-30T14:56:39Z","dateModified":"2021-09-30T14:56:39Z"}},{"key":"SHIMELXX","version":652,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SHIMELXX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SHIMELXX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NNFN4KE2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SHIMELXX","version":652,"parentItem":"NNFN4KE2","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-30T14:56:34Z","url":"https://arxiv.org/pdf/2108.01073.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Meng et al. - 2021 - SDEdit Image Synthesis and Editing with Stochasti.pdf","md5":"68e0fcdac7138b6bbe3e99614a989974","mtime":1633013794000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/JJYJLJ22"},"dateAdded":"2021-09-30T14:56:34Z","dateModified":"2021-09-30T14:56:34Z"}},{"key":"DC5945AD","version":652,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DC5945AD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DC5945AD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NNFN4KE2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DC5945AD","version":652,"parentItem":"NNFN4KE2","itemType":"note","note":"Comment: https://chenlin9.github.io/SDEdit/","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/V9DAMPN5"},"dateAdded":"2021-09-30T14:55:34Z","dateModified":"2021-09-30T14:55:34Z"}},{"key":"NNFN4KE2","version":649,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NNFN4KE2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NNFN4KE2","type":"text/html"}},"meta":{"creatorSummary":"Meng et al.","parsedDate":"2021-08-02","numChildren":3},"data":{"key":"NNFN4KE2","version":649,"itemType":"journalArticle","title":"SDEdit: Image Synthesis and Editing with Stochastic Differential Equations","creators":[{"creatorType":"author","firstName":"Chenlin","lastName":"Meng"},{"creatorType":"author","firstName":"Yang","lastName":"Song"},{"creatorType":"author","firstName":"Jiaming","lastName":"Song"},{"creatorType":"author","firstName":"Jiajun","lastName":"Wu"},{"creatorType":"author","firstName":"Jun-Yan","lastName":"Zhu"},{"creatorType":"author","firstName":"Stefano","lastName":"Ermon"}],"abstractNote":"We introduce a new image editing and synthesis framework, Stochastic Differential Editing (SDEdit), based on a recent generative model using stochastic differential equations (SDEs). Given an input image with user edits (e.g., hand-drawn color strokes), we first add noise to the input according to an SDE, and subsequently denoise it by simulating the reverse SDE to gradually increase its likelihood under the prior. Our method does not require task-specific loss function designs, which are critical components for recent image editing methods based on GAN inversion. Compared to conditional GANs, we do not need to collect new datasets of original and edited images for new applications. Therefore, our method can quickly adapt to various editing tasks at test time without re-training models. Our approach achieves strong performance on a wide range of applications, including image synthesis and editing guided by stroke paintings and image compositing.","publicationTitle":"arXiv:2108.01073 [cs]","volume":"","issue":"","pages":"","date":"2021-08-02","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"SDEdit","url":"http://arxiv.org/abs/2108.01073","accessDate":"2021-09-30T14:55:34Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2108.01073","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/IMMTN6JQ"},"dateAdded":"2021-09-30T14:55:34Z","dateModified":"2021-09-30T14:55:34Z"}},{"key":"RNCLSN9A","version":654,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RNCLSN9A","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RNCLSN9A","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/I49GNHEB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RNCLSN9A","version":654,"parentItem":"I49GNHEB","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-27T11:23:19Z","url":"https://arxiv.org/abs/1912.04958","note":"","contentType":"text/html","charset":"utf-8","filename":"1912.html","md5":"a6a130564afcc28125b3378882b99c46","mtime":1632741799000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/J5ECBL6J"},"dateAdded":"2021-09-27T11:23:19Z","dateModified":"2021-09-27T11:23:19Z"}},{"key":"I49GNHEB","version":653,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I49GNHEB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I49GNHEB","type":"text/html"}},"meta":{"creatorSummary":"Karras et al.","parsedDate":"2020-03-23","numChildren":2},"data":{"key":"I49GNHEB","version":653,"itemType":"journalArticle","title":"Analyzing and Improving the Image Quality of StyleGAN","creators":[{"creatorType":"author","firstName":"Tero","lastName":"Karras"},{"creatorType":"author","firstName":"Samuli","lastName":"Laine"},{"creatorType":"author","firstName":"Miika","lastName":"Aittala"},{"creatorType":"author","firstName":"Janne","lastName":"Hellsten"},{"creatorType":"author","firstName":"Jaakko","lastName":"Lehtinen"},{"creatorType":"author","firstName":"Timo","lastName":"Aila"}],"abstractNote":"The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.","publicationTitle":"arXiv:1912.04958 [cs, eess, stat]","volume":"","issue":"","pages":"","date":"2020-03-23","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1912.04958","accessDate":"2021-09-27T11:21:13Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1912.04958","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Neural and Evolutionary Computing","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/PDATNWCT"},"dateAdded":"2021-09-27T11:21:13Z","dateModified":"2021-09-27T11:23:19Z"}},{"key":"73T3VNA3","version":654,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/73T3VNA3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/73T3VNA3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/I49GNHEB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"73T3VNA3","version":654,"parentItem":"I49GNHEB","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-27T11:23:15Z","url":"https://arxiv.org/pdf/1912.04958.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Karras et al. - 2020 - Analyzing and Improving the Image Quality of Style.pdf","md5":"84c2b4272b4fac1143d6919975d35ed2","mtime":1632741795000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/YT7HM7EI"},"dateAdded":"2021-09-27T11:23:15Z","dateModified":"2021-09-27T11:23:15Z"}},{"key":"PKTPTIDK","version":572,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PKTPTIDK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PKTPTIDK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2V993BEV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PKTPTIDK","version":572,"parentItem":"2V993BEV","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-27T08:47:46Z","url":"https://arxiv.org/abs/2002.07017","note":"","contentType":"text/html","charset":"utf-8","filename":"2002.html","md5":"5df7c17251d4ba076491546f91e8b125","mtime":1632732466000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/27LQ4JT2"},"dateAdded":"2021-09-27T08:47:46Z","dateModified":"2021-09-27T08:47:46Z"}},{"key":"A8QHB8G6","version":572,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A8QHB8G6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A8QHB8G6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2V993BEV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"A8QHB8G6","version":572,"parentItem":"2V993BEV","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-27T08:47:41Z","url":"https://arxiv.org/pdf/2002.07017.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Federici et al. - 2020 - Learning Robust Representations via Multi-View Inf.pdf","md5":"35c82001d8bf52e433c3461821b20d0d","mtime":1632732461000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/2GTVU3Y6"},"dateAdded":"2021-09-27T08:47:41Z","dateModified":"2021-09-27T08:47:41Z"}},{"key":"2V993BEV","version":572,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2V993BEV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2V993BEV","type":"text/html"}},"meta":{"creatorSummary":"Federici et al.","parsedDate":"2020-02-18","numChildren":2},"data":{"key":"2V993BEV","version":572,"itemType":"journalArticle","title":"Learning Robust Representations via Multi-View Information Bottleneck","creators":[{"creatorType":"author","firstName":"Marco","lastName":"Federici"},{"creatorType":"author","firstName":"Anjan","lastName":"Dutta"},{"creatorType":"author","firstName":"Patrick","lastName":"Forré"},{"creatorType":"author","firstName":"Nate","lastName":"Kushman"},{"creatorType":"author","firstName":"Zeynep","lastName":"Akata"}],"abstractNote":"The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to common unsupervised approaches for representation learning.","publicationTitle":"arXiv:2002.07017 [cs, stat]","volume":"","issue":"","pages":"","date":"2020-02-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2002.07017","accessDate":"2021-09-27T08:47:30Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2002.07017","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/HT2HBU8C"},"dateAdded":"2021-09-27T08:47:31Z","dateModified":"2021-09-27T08:47:31Z"}},{"key":"LUS7LGRZ","version":563,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LUS7LGRZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LUS7LGRZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/V8LURJYZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LUS7LGRZ","version":563,"parentItem":"V8LURJYZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-27T07:54:41Z","url":"https://arxiv.org/abs/physics/0004057","note":"","contentType":"text/html","charset":"utf-8","filename":"0004057.html","md5":"96bd86310885d129a8da86ccbd79faa0","mtime":1632729281000,"tags":[],"relations":{},"dateAdded":"2021-09-27T07:54:41Z","dateModified":"2021-09-27T07:54:41Z"}},{"key":"NWT7DF4K","version":563,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NWT7DF4K","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NWT7DF4K","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/V8LURJYZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NWT7DF4K","version":563,"parentItem":"V8LURJYZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-27T07:54:36Z","url":"https://arxiv.org/pdf/physics/0004057.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tishby et al. - 2000 - The information bottleneck method.pdf","md5":"3fbadeb27543f1e17635e67fc0299f4c","mtime":1632729276000,"tags":[],"relations":{},"dateAdded":"2021-09-27T07:54:36Z","dateModified":"2021-09-27T07:54:36Z"}},{"key":"V8LURJYZ","version":562,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/V8LURJYZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/V8LURJYZ","type":"text/html"}},"meta":{"creatorSummary":"Tishby et al.","parsedDate":"2000-04-24","numChildren":2},"data":{"key":"V8LURJYZ","version":562,"itemType":"journalArticle","title":"The information bottleneck method","creators":[{"creatorType":"author","firstName":"Naftali","lastName":"Tishby"},{"creatorType":"author","firstName":"Fernando C.","lastName":"Pereira"},{"creatorType":"author","firstName":"William","lastName":"Bialek"}],"abstractNote":"We define the relevant information in a signal $x\\in X$ as being the information that this signal provides about another signal $y\\in \\Y$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal $x$ requires more than just predicting $y$, it also requires specifying which features of $\\X$ play a role in the prediction. We formalize this problem as that of finding a short code for $\\X$ that preserves the maximum information about $\\Y$. That is, we squeeze the information that $\\X$ provides about $\\Y$ through a `bottleneck' formed by a limited set of codewords $\\tX$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $d(x,\\x)$ emerges from the joint statistics of $\\X$ and $\\Y$. This approach yields an exact set of self consistent equations for the coding rules $X \\to \\tX$ and $\\tX \\to \\Y$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.","publicationTitle":"arXiv:physics/0004057","volume":"","issue":"","pages":"","date":"2000-04-24","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/physics/0004057","accessDate":"2021-09-27T07:54:33Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: physics/0004057","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Condensed Matter - Disordered Systems and Neural Networks","type":1},{"tag":"Nonlinear Sciences - Adaptation and Self-Organizing Systems","type":1},{"tag":"Physics - Data Analysis, Statistics and Probability","type":1}],"collections":[],"relations":{},"dateAdded":"2021-09-27T07:54:33Z","dateModified":"2021-09-27T07:54:33Z"}},{"key":"2ZXDF7SP","version":554,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2ZXDF7SP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2ZXDF7SP","type":"text/html"}},"meta":{"numChildren":0},"data":{"key":"2ZXDF7SP","version":554,"itemType":"attachment","linkMode":"imported_url","title":"information.pdf","accessDate":"2021-09-27T06:46:43Z","url":"https://www.princeton.edu/~cuff/ele201/kulkarni_text/information.pdf","note":"","contentType":"application/pdf","charset":"","filename":"information.pdf","md5":"c9c163d05a47dd760ede002eedd5e372","mtime":1632725203000,"tags":[],"collections":[],"relations":{},"dateAdded":"2021-09-27T06:46:43Z","dateModified":"2021-09-27T06:46:43Z"}},{"key":"P4MB88EK","version":652,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/P4MB88EK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/P4MB88EK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LI4EMUQM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"P4MB88EK","version":652,"parentItem":"LI4EMUQM","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-27T05:20:31Z","url":"https://arxiv.org/abs/2106.05931","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"973ddf25fc7a696a6d3c95dc2738a607","mtime":1632720031000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/QHPJBK8Q"},"dateAdded":"2021-09-27T05:20:31Z","dateModified":"2021-09-27T05:20:31Z"}},{"key":"56KSYYI7","version":652,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/56KSYYI7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/56KSYYI7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LI4EMUQM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"56KSYYI7","version":652,"parentItem":"LI4EMUQM","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-27T05:20:28Z","url":"https://arxiv.org/pdf/2106.05931.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Vahdat et al. - 2021 - Score-based Generative Modeling in Latent Space.pdf","md5":"65494f6b0ee848c2ec5d32ac74846412","mtime":1632720028000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/E6DFU76X"},"dateAdded":"2021-09-27T05:20:28Z","dateModified":"2021-09-27T05:20:28Z"}},{"key":"R7LDZVXH","version":544,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/R7LDZVXH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/R7LDZVXH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9LNPDQAR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"R7LDZVXH","version":544,"parentItem":"9LNPDQAR","itemType":"attachment","linkMode":"imported_url","title":"Tian et al. - Farewell to Mutual Information Variational Distil.pdf","accessDate":"2021-09-16T10:20:57Z","url":"https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_Farewell_to_Mutual_Information_Variational_Distillation_for_Cross-Modal_Person_Re-Identification_CVPR_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tian et al. - Farewell to Mutual Information Variational Distil.pdf","md5":"78fd7962e9f71d56bc53d90ff8e6da8e","mtime":1631787660000,"tags":[],"relations":{},"dateAdded":"2021-09-16T10:20:57Z","dateModified":"2021-09-16T10:21:00Z"}},{"key":"ISJBI4XC","version":651,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ISJBI4XC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ISJBI4XC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MXZBFTHW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ISJBI4XC","version":651,"parentItem":"MXZBFTHW","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-14T12:16:13Z","url":"https://arxiv.org/abs/2106.07582","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"4c513c12c5a997c461bb6fcd44b5a961","mtime":1631621773000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/RE52MEIR"},"dateAdded":"2021-09-14T12:16:13Z","dateModified":"2021-09-14T12:16:13Z"}},{"key":"L3AENL2V","version":651,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/L3AENL2V","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/L3AENL2V","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MXZBFTHW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"L3AENL2V","version":651,"parentItem":"MXZBFTHW","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-14T12:16:08Z","url":"https://arxiv.org/pdf/2106.07582.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Nachmani et al. - 2021 - Non Gaussian Denoising Diffusion Models.pdf","md5":"3d53057642cbf92bd5e14eedcbb1ae41","mtime":1631621768000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/APTZQSBE"},"dateAdded":"2021-09-14T12:16:08Z","dateModified":"2021-09-14T12:16:08Z"}},{"key":"TL3H58IN","version":540,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TL3H58IN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TL3H58IN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HXNUEG2D","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TL3H58IN","version":540,"parentItem":"HXNUEG2D","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-14T12:16:04Z","url":"https://arxiv.org/abs/2108.11514","note":"","contentType":"text/html","charset":"utf-8","filename":"2108.html","md5":"885ce122cfec09a2f5e6936f18872542","mtime":1631621764000,"tags":[],"relations":{},"dateAdded":"2021-09-14T12:16:04Z","dateModified":"2021-09-14T12:16:04Z"}},{"key":"MXZBFTHW","version":648,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MXZBFTHW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MXZBFTHW","type":"text/html"}},"meta":{"creatorSummary":"Nachmani et al.","parsedDate":"2021-06-14","numChildren":2},"data":{"key":"MXZBFTHW","version":648,"itemType":"journalArticle","title":"Non Gaussian Denoising Diffusion Models","creators":[{"creatorType":"author","firstName":"Eliya","lastName":"Nachmani"},{"creatorType":"author","firstName":"Robin San","lastName":"Roman"},{"creatorType":"author","firstName":"Lior","lastName":"Wolf"}],"abstractNote":"Generative diffusion processes are an emerging and effective tool for image and speech generation. In the existing methods, the underline noise distribution of the diffusion process is Gaussian noise. However, fitting distributions with more degrees of freedom, could help the performance of such generative models. In this work, we investigate other types of noise distribution for the diffusion process. Specifically, we show that noise from Gamma distribution provides improved results for image and speech generation. Moreover, we show that using a mixture of Gaussian noise variables in the diffusion process improves the performance over a diffusion process that is based on a single distribution. Our approach preserves the ability to efficiently sample state in the training diffusion process while using Gamma noise and a mixture of noise.","publicationTitle":"arXiv:2106.07582 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-06-14","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2106.07582","accessDate":"2021-09-14T12:15:47Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.07582","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1}],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/N7LI5ECH"},"dateAdded":"2021-09-14T12:15:47Z","dateModified":"2021-09-14T12:16:00Z"}},{"key":"HXNUEG2D","version":539,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HXNUEG2D","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HXNUEG2D","type":"text/html"}},"meta":{"creatorSummary":"Lam et al.","parsedDate":"2021-08-31","numChildren":2},"data":{"key":"HXNUEG2D","version":539,"itemType":"journalArticle","title":"Bilateral Denoising Diffusion Models","creators":[{"creatorType":"author","firstName":"Max W. Y.","lastName":"Lam"},{"creatorType":"author","firstName":"Jun","lastName":"Wang"},{"creatorType":"author","firstName":"Rongjie","lastName":"Huang"},{"creatorType":"author","firstName":"Dan","lastName":"Su"},{"creatorType":"author","firstName":"Dong","lastName":"Yu"}],"abstractNote":"Denoising diffusion probabilistic models (DDPMs) have emerged as competitive generative models yet brought challenges to efficient sampling. In this paper, we propose novel bilateral denoising diffusion models (BDDMs), which take significantly fewer steps to generate high-quality samples. From a bilateral modeling objective, BDDMs parameterize the forward and reverse processes with a score network and a scheduling network, respectively. We show that a new lower bound tighter than the standard evidence lower bound can be derived as a surrogate objective for training the two networks. In particular, BDDMs are efficient, simple-to-train, and capable of further improving any pre-trained DDPM by optimizing the inference noise schedules. Our experiments demonstrated that BDDMs can generate high-fidelity samples with as few as 3 sampling steps and produce comparable or even higher quality samples than DDPMs using 1000 steps with only 16 sampling steps (a 62x speedup).","publicationTitle":"arXiv:2108.11514 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-08-31","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2108.11514","accessDate":"2021-09-14T12:15:45Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2108.11514","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1},{"tag":"Electrical Engineering and Systems Science - Signal Processing","type":1}],"collections":["TNWL7M5C"],"relations":{},"dateAdded":"2021-09-14T12:15:45Z","dateModified":"2021-09-14T12:15:55Z"}},{"key":"NKHA46IS","version":540,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NKHA46IS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NKHA46IS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HXNUEG2D","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NKHA46IS","version":540,"parentItem":"HXNUEG2D","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-14T12:15:54Z","url":"https://arxiv.org/pdf/2108.11514.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Lam et al. - 2021 - Bilateral Denoising Diffusion Models.pdf","md5":"bb96447ec5420cb0f1ca9620fe936f2c","mtime":1631621754000,"tags":[],"relations":{},"dateAdded":"2021-09-14T12:15:54Z","dateModified":"2021-09-14T12:15:54Z"}},{"key":"TPK2IQLS","version":651,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TPK2IQLS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TPK2IQLS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ISYURXMF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TPK2IQLS","version":651,"parentItem":"ISYURXMF","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-13T16:56:32Z","url":"https://arxiv.org/abs/2010.02502","note":"","contentType":"text/html","charset":"utf-8","filename":"2010.html","md5":"bd05af8f996455625fa11a0790122281","mtime":1631552192000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/TFIUXXET","http://zotero.org/groups/4458581/items/NIKI5RRP"]},"dateAdded":"2021-09-13T16:56:32Z","dateModified":"2021-09-13T16:56:32Z"}},{"key":"52EHSNFV","version":651,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/52EHSNFV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/52EHSNFV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ISYURXMF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"52EHSNFV","version":651,"parentItem":"ISYURXMF","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-13T16:56:28Z","url":"https://arxiv.org/pdf/2010.02502.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Song et al. - 2020 - Denoising Diffusion Implicit Models.pdf","md5":"35452c4edb0286578681fc8078c6bf01","mtime":1631552188000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/HV9Y9X5J","http://zotero.org/groups/4458581/items/NKHK9SN5"]},"dateAdded":"2021-09-13T16:56:28Z","dateModified":"2021-09-13T16:56:28Z"}},{"key":"CCUCIBFJ","version":528,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CCUCIBFJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CCUCIBFJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/D4SAGRQA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CCUCIBFJ","version":528,"parentItem":"D4SAGRQA","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-07T05:49:15Z","url":"https://arxiv.org/abs/2109.01750","note":"","contentType":"text/html","charset":"utf-8","filename":"2109.html","md5":"ae559d6bb53a8e68895e2f23d6ead066","mtime":1630993755000,"tags":[],"relations":{},"dateAdded":"2021-09-07T05:49:15Z","dateModified":"2021-09-07T05:49:15Z"}},{"key":"78DKG6FF","version":528,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/78DKG6FF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/78DKG6FF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/D4SAGRQA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"78DKG6FF","version":528,"parentItem":"D4SAGRQA","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-07T05:49:11Z","url":"https://arxiv.org/pdf/2109.01750.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Jang and Agapito - 2021 - CodeNeRF Disentangled Neural Radiance Fields for .pdf","md5":"8dfb08dc6bda9e5d4bfcd5d7a0837f3d","mtime":1630993751000,"tags":[],"relations":{},"dateAdded":"2021-09-07T05:49:11Z","dateModified":"2021-09-07T05:49:11Z"}},{"key":"YYKBQ8RW","version":526,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YYKBQ8RW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YYKBQ8RW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/D4SAGRQA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YYKBQ8RW","version":526,"parentItem":"D4SAGRQA","itemType":"note","note":"Comment: 10 pages, 15 figures, ICCV 2021","tags":[],"relations":{},"dateAdded":"2021-09-07T05:41:28Z","dateModified":"2021-09-07T05:41:28Z"}},{"key":"D4SAGRQA","version":526,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/D4SAGRQA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/D4SAGRQA","type":"text/html"}},"meta":{"creatorSummary":"Jang and Agapito","parsedDate":"2021-09-03","numChildren":3},"data":{"key":"D4SAGRQA","version":526,"itemType":"journalArticle","title":"CodeNeRF: Disentangled Neural Radiance Fields for Object Categories","creators":[{"creatorType":"author","firstName":"Wonbong","lastName":"Jang"},{"creatorType":"author","firstName":"Lourdes","lastName":"Agapito"}],"abstractNote":"CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the original NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embeddings. At test time, given a single unposed image of an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF generalises well to unseen objects and achieves on-par performance with methods that require known camera pose at test time. Our results on real-world images demonstrate that CodeNeRF can bridge the sim-to-real gap. Project page: \\url{https://github.com/wayne1123/code-nerf}","publicationTitle":"arXiv:2109.01750 [cs]","volume":"","issue":"","pages":"","date":"2021-09-03","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"CodeNeRF","url":"http://arxiv.org/abs/2109.01750","accessDate":"2021-09-07T05:41:28Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2109.01750","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-09-07T05:41:28Z","dateModified":"2021-09-07T05:41:28Z"}},{"key":"L3QH3F6F","version":523,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/L3QH3F6F","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/L3QH3F6F","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2SKLEA7A","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"L3QH3F6F","version":523,"parentItem":"2SKLEA7A","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-09-02T11:56:58Z","url":"https://arxiv.org/abs/2006.10455","note":"","contentType":"text/html","charset":"utf-8","filename":"2006.html","md5":"2e05d13f8c5611863164bd40e4ab3d8b","mtime":1630583818000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/2EPHCARP"},"dateAdded":"2021-09-02T11:56:58Z","dateModified":"2021-09-02T11:56:58Z"}},{"key":"6ZUQ4QKW","version":523,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6ZUQ4QKW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6ZUQ4QKW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2SKLEA7A","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6ZUQ4QKW","version":523,"parentItem":"2SKLEA7A","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-09-02T11:56:53Z","url":"https://arxiv.org/pdf/2006.10455.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Maennel et al. - 2020 - What Do Neural Networks Learn When Trained With Ra.pdf","md5":"7fe5926495236db561809270816ec68e","mtime":1630583813000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/FST6PNH4"},"dateAdded":"2021-09-02T11:56:53Z","dateModified":"2021-09-02T11:56:53Z"}},{"key":"CQQ7BXSL","version":523,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CQQ7BXSL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CQQ7BXSL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2SKLEA7A","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CQQ7BXSL","version":523,"parentItem":"2SKLEA7A","itemType":"note","note":"Comment: Accepted, NeurIPS2020","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/JMA5PQJG"},"dateAdded":"2021-09-02T11:56:33Z","dateModified":"2021-09-02T11:56:33Z"}},{"key":"2SKLEA7A","version":523,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2SKLEA7A","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2SKLEA7A","type":"text/html"}},"meta":{"creatorSummary":"Maennel et al.","parsedDate":"2020-11-11","numChildren":3},"data":{"key":"2SKLEA7A","version":523,"itemType":"journalArticle","title":"What Do Neural Networks Learn When Trained With Random Labels?","creators":[{"creatorType":"author","firstName":"Hartmut","lastName":"Maennel"},{"creatorType":"author","firstName":"Ibrahim","lastName":"Alabdulmohsin"},{"creatorType":"author","firstName":"Ilya","lastName":"Tolstikhin"},{"creatorType":"author","firstName":"Robert J. N.","lastName":"Baldock"},{"creatorType":"author","firstName":"Olivier","lastName":"Bousquet"},{"creatorType":"author","firstName":"Sylvain","lastName":"Gelly"},{"creatorType":"author","firstName":"Daniel","lastName":"Keysers"}],"abstractNote":"We study deep neural networks (DNNs) trained on natural image data with entirely random labels. Despite its popularity in the literature, where it is often used to study memorization, generalization, and other phenomena, little is known about what DNNs learn in this setting. In this paper, we show analytically for convolutional and fully connected networks that an alignment between the principal components of network parameters and data takes place when training with random labels. We study this alignment effect by investigating neural networks pre-trained on randomly labelled image data and subsequently fine-tuned on disjoint datasets with random or real labels. We show how this alignment produces a positive transfer: networks pre-trained with random labels train faster downstream compared to training from scratch even after accounting for simple effects, such as weight scaling. We analyze how competing effects, such as specialization at later layers, may hide the positive transfer. These effects are studied in several network architectures, including VGG16 and ResNet18, on CIFAR10 and ImageNet.","publicationTitle":"arXiv:2006.10455 [cs, stat]","volume":"","issue":"","pages":"","date":"2020-11-11","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2006.10455","accessDate":"2021-09-02T11:56:33Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2006.10455","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/286VVKTS"},"dateAdded":"2021-09-02T11:56:33Z","dateModified":"2021-09-02T11:56:33Z"}},{"key":"SM33CSRG","version":519,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SM33CSRG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SM33CSRG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/A465FIQU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SM33CSRG","version":519,"parentItem":"A465FIQU","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-30T04:10:28Z","url":"https://arxiv.org/abs/2108.01285","note":"","contentType":"text/html","charset":"utf-8","filename":"2108.html","md5":"85ff4b4ce7935569356f767e94e60ce9","mtime":1630296628000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/324MUXGD"},"dateAdded":"2021-08-30T04:10:28Z","dateModified":"2021-08-30T04:10:28Z"}},{"key":"9E492DMI","version":519,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9E492DMI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9E492DMI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/A465FIQU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9E492DMI","version":519,"parentItem":"A465FIQU","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-30T04:10:23Z","url":"https://arxiv.org/pdf/2108.01285.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Choi et al. - 2021 - Toward Spatially Unbiased Generative Models.pdf","md5":"07ea4e907a01bd8b4c0ec0e97d6a3619","mtime":1630296623000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/AAF5RNFF"},"dateAdded":"2021-08-30T04:10:23Z","dateModified":"2021-08-30T04:10:23Z"}},{"key":"KD6KHEFZ","version":519,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KD6KHEFZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KD6KHEFZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/A465FIQU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KD6KHEFZ","version":519,"parentItem":"A465FIQU","itemType":"note","note":"Comment: ICCV 2021","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/HHSDHNMX"},"dateAdded":"2021-08-30T04:10:08Z","dateModified":"2021-08-30T04:10:08Z"}},{"key":"A465FIQU","version":519,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A465FIQU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A465FIQU","type":"text/html"}},"meta":{"creatorSummary":"Choi et al.","parsedDate":"2021-08-03","numChildren":3},"data":{"key":"A465FIQU","version":519,"itemType":"journalArticle","title":"Toward Spatially Unbiased Generative Models","creators":[{"creatorType":"author","firstName":"Jooyoung","lastName":"Choi"},{"creatorType":"author","firstName":"Jungbeom","lastName":"Lee"},{"creatorType":"author","firstName":"Yonghyun","lastName":"Jeong"},{"creatorType":"author","firstName":"Sungroh","lastName":"Yoon"}],"abstractNote":"Recent image generation models show remarkable generation performance. However, they mirror strong location preference in datasets, which we call spatial bias. Therefore, generators render poor samples at unseen locations and scales. We argue that the generators rely on their implicit positional encoding to render spatial content. From our observations, the generator's implicit positional encoding is translation-variant, making the generator spatially biased. To address this issue, we propose injecting explicit positional encoding at each scale of the generator. By learning the spatially unbiased generator, we facilitate the robust use of generators in multiple tasks, such as GAN inversion, multi-scale generation, generation of arbitrary sizes and aspect ratios. Furthermore, we show that our method can also be applied to denoising diffusion probabilistic models.","publicationTitle":"arXiv:2108.01285 [cs]","volume":"","issue":"","pages":"","date":"2021-08-03","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2108.01285","accessDate":"2021-08-30T04:10:08Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2108.01285","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/3JIF7LRC"},"dateAdded":"2021-08-30T04:10:08Z","dateModified":"2021-08-30T04:10:08Z"}},{"key":"QP85W34H","version":514,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QP85W34H","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QP85W34H","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5TWWD7E9","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QP85W34H","version":514,"parentItem":"5TWWD7E9","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-30T03:52:14Z","url":"https://arxiv.org/abs/2107.00630","note":"","contentType":"text/html","charset":"utf-8","filename":"2107.html","md5":"5968930dbb08b6fea11a31fa98e89672","mtime":1630295534000,"tags":[],"relations":{},"dateAdded":"2021-08-30T03:52:14Z","dateModified":"2021-08-30T03:52:14Z"}},{"key":"W5T7FHJD","version":514,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/W5T7FHJD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/W5T7FHJD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5TWWD7E9","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"W5T7FHJD","version":514,"parentItem":"5TWWD7E9","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-30T03:52:09Z","url":"https://arxiv.org/pdf/2107.00630.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kingma et al. - 2021 - Variational Diffusion Models.pdf","md5":"e1db5b128e28fd09679739ec3464f2bb","mtime":1630295529000,"tags":[],"relations":{},"dateAdded":"2021-08-30T03:52:09Z","dateModified":"2021-08-30T03:52:09Z"}},{"key":"U39DCWMH","version":511,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U39DCWMH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U39DCWMH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PHYY97CD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"U39DCWMH","version":511,"parentItem":"PHYY97CD","itemType":"attachment","linkMode":"imported_url","title":"Snapshot","accessDate":"2021-08-29T12:51:29Z","url":"https://bjlkeng.github.io/posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/","note":"","contentType":"text/html","charset":"utf-8","filename":"importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders.html","md5":"71c6551b9c588d33947e9548a57d792b","mtime":1630241489624,"tags":[],"relations":{},"dateAdded":"2021-08-29T12:51:29Z","dateModified":"2021-08-29T12:51:29Z"}},{"key":"PHYY97CD","version":510,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PHYY97CD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PHYY97CD","type":"text/html"}},"meta":{"creatorSummary":"Keng","parsedDate":"2019-02-06","numChildren":1},"data":{"key":"PHYY97CD","version":510,"itemType":"webpage","title":"Importance Sampling and Estimating Marginal Likelihood in Variational","creators":[{"creatorType":"author","firstName":"Brian","lastName":"Keng"}],"abstractNote":"A short post describing how to use importance sampling to estimate marginal likelihood in variational autoencoders.","websiteTitle":"Bounded Rationality","websiteType":"","date":"2019-02-06T08:20:11-04:00","shortTitle":"","url":"http://bjlkeng.github.io/posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/","accessDate":"2021-08-29T12:51:23Z","language":"en","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-08-29T12:51:23Z","dateModified":"2021-08-29T12:51:23Z"}},{"key":"V5YVRHX3","version":509,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/V5YVRHX3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/V5YVRHX3","type":"text/html"}},"meta":{"numChildren":0},"data":{"key":"V5YVRHX3","version":509,"itemType":"attachment","linkMode":"imported_url","title":"Ch-var-is.pdf","accessDate":"2021-08-29T10:43:16Z","url":"https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Ch-var-is.pdf","md5":"593890cbd9e8a1ca790004a92f140b97","mtime":1630233796253,"tags":[],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/NFRIXL7W"},"dateAdded":"2021-08-29T10:43:28Z","dateModified":"2021-08-29T10:43:28Z"}},{"key":"A2MZ2UGA","version":507,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A2MZ2UGA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A2MZ2UGA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DX3FDTCL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"A2MZ2UGA","version":507,"parentItem":"DX3FDTCL","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-27T05:58:18Z","url":"https://arxiv.org/abs/1912.02762","note":"","contentType":"text/html","charset":"utf-8","filename":"1912.html","md5":"c6210592ac0fda6873815bbab1e04caa","mtime":1630043906000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/AJPWWTI4"},"dateAdded":"2021-08-27T05:58:26Z","dateModified":"2021-08-27T05:58:26Z"}},{"key":"3KD5K7NX","version":507,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3KD5K7NX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3KD5K7NX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DX3FDTCL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3KD5K7NX","version":507,"parentItem":"DX3FDTCL","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-27T05:58:13Z","url":"https://arxiv.org/pdf/1912.02762.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Papamakarios et al. - 2021 - Normalizing Flows for Probabilistic Modeling and I.pdf","md5":"0bd28de8cbe0fb250f8c105e9cb15d93","mtime":1630043906000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/X8JMV9VW"},"dateAdded":"2021-08-27T05:58:26Z","dateModified":"2021-08-27T05:58:26Z"}},{"key":"DX3FDTCL","version":506,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DX3FDTCL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DX3FDTCL","type":"text/html"}},"meta":{"creatorSummary":"Papamakarios et al.","parsedDate":"2021-04-08","numChildren":3},"data":{"key":"DX3FDTCL","version":506,"itemType":"journalArticle","title":"Normalizing Flows for Probabilistic Modeling and Inference","creators":[{"creatorType":"author","firstName":"George","lastName":"Papamakarios"},{"creatorType":"author","firstName":"Eric","lastName":"Nalisnick"},{"creatorType":"author","firstName":"Danilo Jimenez","lastName":"Rezende"},{"creatorType":"author","firstName":"Shakir","lastName":"Mohamed"},{"creatorType":"author","firstName":"Balaji","lastName":"Lakshminarayanan"}],"abstractNote":"Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.","publicationTitle":"arXiv:1912.02762 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-04-08","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1912.02762","accessDate":"2021-08-27T05:58:02Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1912.02762","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/SY4F5EG6"},"dateAdded":"2021-08-27T05:58:26Z","dateModified":"2021-08-27T05:58:26Z"}},{"key":"UQ8RCWIE","version":506,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UQ8RCWIE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UQ8RCWIE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DX3FDTCL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UQ8RCWIE","version":506,"parentItem":"DX3FDTCL","itemType":"note","note":"Comment: Review article, 64 pages, 9 figures. Published in the Journal of Machine Learning Research (see https://jmlr.org/papers/v22/19-1028.html)","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/SBU7NDMN"},"dateAdded":"2021-08-27T05:58:26Z","dateModified":"2021-08-27T05:58:26Z"}},{"key":"UN5PNL39","version":504,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UN5PNL39","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UN5PNL39","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/TB6I7UG4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UN5PNL39","version":504,"parentItem":"TB6I7UG4","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-26T14:46:39Z","url":"https://arxiv.org/abs/1908.09257","note":"","contentType":"text/html","charset":"utf-8","filename":"1908.html","md5":"0825d856162225458a0c90df63b8fcb6","mtime":1629989199000,"tags":[],"relations":{},"dateAdded":"2021-08-26T14:46:39Z","dateModified":"2021-08-26T14:46:39Z"}},{"key":"6LE3KC3S","version":504,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6LE3KC3S","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6LE3KC3S","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/TB6I7UG4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6LE3KC3S","version":504,"parentItem":"TB6I7UG4","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-26T14:46:34Z","url":"https://arxiv.org/pdf/1908.09257.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf","md5":"94afe9312a88df0a35108cd1c5de6c40","mtime":1629989194000,"tags":[],"relations":{},"dateAdded":"2021-08-26T14:46:34Z","dateModified":"2021-08-26T14:46:34Z"}},{"key":"Q2UUJJGP","version":504,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Q2UUJJGP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Q2UUJJGP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BVCI8J9W","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Q2UUJJGP","version":504,"parentItem":"BVCI8J9W","itemType":"attachment","linkMode":"imported_url","title":"Lugmayr et al. - 2020 - SRFlow Learning the Super-Resolution Space with N.pdf","accessDate":"2021-08-26T14:46:16Z","url":"http://de.arxiv.org/pdf/2006.14200?hCon","note":"","contentType":"application/pdf","charset":"","filename":"Lugmayr et al. - 2020 - SRFlow Learning the Super-Resolution Space with N.pdf","md5":"78f528bab13755854bf48efb3350e0b7","mtime":1629989181000,"tags":[],"relations":{},"dateAdded":"2021-08-26T14:46:16Z","dateModified":"2021-08-26T14:46:21Z"}},{"key":"XQG3SWXW","version":503,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XQG3SWXW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XQG3SWXW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BVCI8J9W","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XQG3SWXW","version":503,"parentItem":"BVCI8J9W","itemType":"note","note":"Comment: ECCV 2020 Spotlight | git.io/SRFlow","tags":[],"relations":{},"dateAdded":"2021-08-26T14:46:21Z","dateModified":"2021-08-26T14:46:21Z"}},{"key":"BVCI8J9W","version":503,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BVCI8J9W","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BVCI8J9W","type":"text/html"}},"meta":{"creatorSummary":"Lugmayr et al.","parsedDate":"2020-07-31","numChildren":2},"data":{"key":"BVCI8J9W","version":503,"itemType":"journalArticle","title":"SRFlow: Learning the Super-Resolution Space with Normalizing Flow","creators":[{"creatorType":"author","firstName":"Andreas","lastName":"Lugmayr"},{"creatorType":"author","firstName":"Martin","lastName":"Danelljan"},{"creatorType":"author","firstName":"Luc","lastName":"Van Gool"},{"creatorType":"author","firstName":"Radu","lastName":"Timofte"}],"abstractNote":"Super-resolution is an ill-posed problem, since it allows for multiple predictions for a given low-resolution image. This fundamental fact is largely ignored by state-of-the-art deep learning based approaches. These methods instead train a deterministic mapping using combinations of reconstruction and adversarial losses. In this work, we therefore propose SRFlow: a normalizing flow based super-resolution method capable of learning the conditional distribution of the output given the low-resolution input. Our model is trained in a principled manner using a single loss, namely the negative log-likelihood. SRFlow therefore directly accounts for the ill-posed nature of the problem, and learns to predict diverse photo-realistic high-resolution images. Moreover, we utilize the strong image posterior learned by SRFlow to design flexible image manipulation techniques, capable of enhancing super-resolved images by, e.g., transferring content from other images. We perform extensive experiments on faces, as well as on super-resolution in general. SRFlow outperforms state-of-the-art GAN-based approaches in terms of both PSNR and perceptual quality metrics, while allowing for diversity through the exploration of the space of super-resolved solutions.","publicationTitle":"arXiv:2006.14200 [cs, eess]","volume":"","issue":"","pages":"","date":"2020-07-31","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"SRFlow","url":"http://arxiv.org/abs/2006.14200","accessDate":"2021-08-26T14:46:21Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2006.14200","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1}],"collections":[],"relations":{},"dateAdded":"2021-08-26T14:46:21Z","dateModified":"2021-08-26T14:46:21Z"}},{"key":"A67X7MAX","version":503,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A67X7MAX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A67X7MAX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/TB6I7UG4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"A67X7MAX","version":503,"parentItem":"TB6I7UG4","itemType":"note","note":"Comment: This paper appears in: IEEE Transactions on Pattern Analysis and Machine Intelligence On page(s): 1-16 Print ISSN: 0162-8828 Online ISSN: 0162-8828","tags":[],"relations":{},"dateAdded":"2021-08-26T14:46:06Z","dateModified":"2021-08-26T14:46:06Z"}},{"key":"TB6I7UG4","version":503,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TB6I7UG4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TB6I7UG4","type":"text/html"}},"meta":{"creatorSummary":"Kobyzev et al.","parsedDate":"2020","numChildren":3},"data":{"key":"TB6I7UG4","version":503,"itemType":"journalArticle","title":"Normalizing Flows: An Introduction and Review of Current Methods","creators":[{"creatorType":"author","firstName":"Ivan","lastName":"Kobyzev"},{"creatorType":"author","firstName":"Simon J. D.","lastName":"Prince"},{"creatorType":"author","firstName":"Marcus A.","lastName":"Brubaker"}],"abstractNote":"Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.","publicationTitle":"IEEE Transactions on Pattern Analysis and Machine Intelligence","volume":"","issue":"","pages":"1-1","date":"2020","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"IEEE Trans. Pattern Anal. Mach. Intell.","language":"","DOI":"10.1109/TPAMI.2020.2992934","ISSN":"0162-8828, 2160-9292, 1939-3539","shortTitle":"Normalizing Flows","url":"http://arxiv.org/abs/1908.09257","accessDate":"2021-08-26T14:46:06Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1908.09257","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-08-26T14:46:06Z","dateModified":"2021-08-26T14:46:06Z"}},{"key":"JYFX3L2W","version":500,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JYFX3L2W","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JYFX3L2W","type":"text/html"}},"meta":{"numChildren":0},"data":{"key":"JYFX3L2W","version":500,"itemType":"attachment","linkMode":"imported_url","title":"samplingPart1.pdf","accessDate":"2021-08-25T04:57:26Z","url":"http://www.cse.psu.edu/~rtc12/CSE586/lectures/samplingPart1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"samplingPart1.pdf","md5":"08af61f51da3a33d6e77a850df1eac2a","mtime":1629867446000,"tags":[],"collections":[],"relations":{},"dateAdded":"2021-08-25T04:57:26Z","dateModified":"2021-08-25T04:57:26Z"}},{"key":"DUSDVTE8","version":491,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DUSDVTE8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DUSDVTE8","type":"text/html"}},"meta":{"creatorSummary":"Perez et al.","parsedDate":"2017-12-18","numChildren":3},"data":{"key":"DUSDVTE8","version":491,"itemType":"journalArticle","title":"FiLM: Visual Reasoning with a General Conditioning Layer","creators":[{"creatorType":"author","firstName":"Ethan","lastName":"Perez"},{"creatorType":"author","firstName":"Florian","lastName":"Strub"},{"creatorType":"author","firstName":"Harm","lastName":"de Vries"},{"creatorType":"author","firstName":"Vincent","lastName":"Dumoulin"},{"creatorType":"author","firstName":"Aaron","lastName":"Courville"}],"abstractNote":"We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.","publicationTitle":"arXiv:1709.07871 [cs, stat]","volume":"","issue":"","pages":"","date":"2017-12-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"FiLM","url":"http://arxiv.org/abs/1709.07871","accessDate":"2021-08-23T08:10:32Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1709.07871","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/SK233QWE","http://zotero.org/groups/4320173/items/P6ZETIVM"]},"dateAdded":"2021-08-23T08:11:21Z","dateModified":"2021-08-23T08:11:21Z"}},{"key":"ITEVK9AW","version":491,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ITEVK9AW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ITEVK9AW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DUSDVTE8","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ITEVK9AW","version":491,"parentItem":"DUSDVTE8","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-23T08:11:14Z","url":"https://arxiv.org/abs/1709.07871","note":"","contentType":"text/html","charset":"utf-8","filename":"1709.html","md5":"029ed45d7cc266167a226741c9a97d15","mtime":1629706281000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/YJFPIYKX","http://zotero.org/groups/4320173/items/7GYFPL8R"]},"dateAdded":"2021-08-23T08:11:21Z","dateModified":"2021-08-23T08:11:21Z"}},{"key":"AIVM53D2","version":491,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AIVM53D2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AIVM53D2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DUSDVTE8","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AIVM53D2","version":491,"parentItem":"DUSDVTE8","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-23T08:11:09Z","url":"https://arxiv.org/pdf/1709.07871.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Perez et al. - 2017 - FiLM Visual Reasoning with a General Conditioning.pdf","md5":"37e878eb800c806b230d26486760f22c","mtime":1629706281000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/QSJNYRJ4","http://zotero.org/groups/4320173/items/BSINT2ZB"]},"dateAdded":"2021-08-23T08:11:21Z","dateModified":"2021-08-23T08:11:21Z"}},{"key":"6T65UBQM","version":491,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6T65UBQM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6T65UBQM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DUSDVTE8","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6T65UBQM","version":491,"parentItem":"DUSDVTE8","itemType":"note","note":"Comment: AAAI 2018. Code available at http://github.com/ethanjperez/film . Extends arXiv:1707.03017","tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/64V6DFQN","http://zotero.org/groups/4320173/items/AFWZ47JS"]},"dateAdded":"2021-08-23T08:11:21Z","dateModified":"2021-08-23T08:11:21Z"}},{"key":"LHCSNP3V","version":490,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LHCSNP3V","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LHCSNP3V","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/JRXZ9IDX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LHCSNP3V","version":490,"parentItem":"JRXZ9IDX","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-23T07:44:39Z","url":"https://arxiv.org/pdf/2106.09660.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chen et al. - 2021 - WaveGrad 2 Iterative Refinement for Text-to-Speec.pdf","md5":"de2aadc6897a716b2ebc48a4def185ef","mtime":1629704715000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/YJQTICSQ"},"dateAdded":"2021-08-23T07:45:15Z","dateModified":"2021-08-23T07:45:15Z"}},{"key":"ZTRQZ33H","version":490,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZTRQZ33H","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZTRQZ33H","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/JRXZ9IDX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZTRQZ33H","version":490,"parentItem":"JRXZ9IDX","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-23T07:44:58Z","url":"https://arxiv.org/abs/2106.09660","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"e9ccf554bd02c7d52579a8f85b5877b1","mtime":1629704715000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/L35LHN8I"},"dateAdded":"2021-08-23T07:45:15Z","dateModified":"2021-08-23T07:45:15Z"}},{"key":"6CR27DNI","version":488,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6CR27DNI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6CR27DNI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/JRXZ9IDX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6CR27DNI","version":488,"parentItem":"JRXZ9IDX","itemType":"note","note":"Comment: Proceedings of INTERSPEECH","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/CJUJ2IWT"},"dateAdded":"2021-08-23T07:45:15Z","dateModified":"2021-08-23T07:45:15Z"}},{"key":"JRXZ9IDX","version":488,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JRXZ9IDX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JRXZ9IDX","type":"text/html"}},"meta":{"creatorSummary":"Chen et al.","parsedDate":"2021-06-18","numChildren":3},"data":{"key":"JRXZ9IDX","version":488,"itemType":"journalArticle","title":"WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis","creators":[{"creatorType":"author","firstName":"Nanxin","lastName":"Chen"},{"creatorType":"author","firstName":"Yu","lastName":"Zhang"},{"creatorType":"author","firstName":"Heiga","lastName":"Zen"},{"creatorType":"author","firstName":"Ron J.","lastName":"Weiss"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"},{"creatorType":"author","firstName":"Najim","lastName":"Dehak"},{"creatorType":"author","firstName":"William","lastName":"Chan"}],"abstractNote":"This paper introduces WaveGrad 2, a non-autoregressive generative model for text-to-speech synthesis. WaveGrad 2 is trained to estimate the gradient of the log conditional density of the waveform given a phoneme sequence. The model takes an input phoneme sequence, and through an iterative refinement process, generates an audio waveform. This contrasts to the original WaveGrad vocoder which conditions on mel-spectrogram features, generated by a separate model. The iterative refinement process starts from Gaussian noise, and through a series of refinement steps (e.g., 50 steps), progressively recovers the audio sequence. WaveGrad 2 offers a natural way to trade-off between inference speed and sample quality, through adjusting the number of refinement steps. Experiments show that the model can generate high fidelity audio, approaching the performance of a state-of-the-art neural TTS system. We also report various ablation studies over different model configurations. Audio samples are available at https://wavegrad.github.io/v2.","publicationTitle":"arXiv:2106.09660 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-06-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"WaveGrad 2","url":"http://arxiv.org/abs/2106.09660","accessDate":"2021-08-23T07:44:35Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.09660","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1}],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/7UJJ2WYT"},"dateAdded":"2021-08-23T07:45:15Z","dateModified":"2021-08-23T07:45:15Z"}},{"key":"ZI522ZAX","version":484,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZI522ZAX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZI522ZAX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S8D7ZN3X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZI522ZAX","version":484,"parentItem":"S8D7ZN3X","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:40:32Z","url":"https://arxiv.org/pdf/2104.05358.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Sasaki et al. - 2021 - UNIT-DDPM UNpaired Image Translation with Denoisi.pdf","md5":"49910cb1e36cc88183228fcd9eac157d","mtime":1624207232000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:40:32Z","dateModified":"2021-08-18T13:24:46Z"}},{"key":"DDB2ETC9","version":481,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DDB2ETC9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DDB2ETC9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5CL67G2S","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DDB2ETC9","version":481,"parentItem":"5CL67G2S","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-17T13:26:57Z","url":"https://arxiv.org/abs/1904.01326","note":"","contentType":"text/html","charset":"utf-8","filename":"1904.html","md5":"c4a6e84a7770279f21b0377207400263","mtime":1629206817000,"tags":[],"relations":{},"dateAdded":"2021-08-17T13:26:57Z","dateModified":"2021-08-17T13:26:57Z"}},{"key":"SFNYJTYL","version":478,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SFNYJTYL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SFNYJTYL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5CL67G2S","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SFNYJTYL","version":478,"parentItem":"5CL67G2S","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-17T08:38:08Z","url":"https://arxiv.org/pdf/1904.01326.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Nguyen-Phuoc et al. - 2019 - HoloGAN Unsupervised learning of 3D representatio.pdf","md5":"d73d8ff026b50e46684ec1d43aba9d6c","mtime":1629189488000,"tags":[],"relations":{},"dateAdded":"2021-08-17T08:38:08Z","dateModified":"2021-08-17T08:38:08Z"}},{"key":"5CL67G2S","version":477,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5CL67G2S","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5CL67G2S","type":"text/html"}},"meta":{"creatorSummary":"Nguyen-Phuoc et al.","parsedDate":"2019-10-01","numChildren":3},"data":{"key":"5CL67G2S","version":477,"itemType":"journalArticle","title":"HoloGAN: Unsupervised learning of 3D representations from natural images","creators":[{"creatorType":"author","firstName":"Thu","lastName":"Nguyen-Phuoc"},{"creatorType":"author","firstName":"Chuan","lastName":"Li"},{"creatorType":"author","firstName":"Lucas","lastName":"Theis"},{"creatorType":"author","firstName":"Christian","lastName":"Richardt"},{"creatorType":"author","firstName":"Yong-Liang","lastName":"Yang"}],"abstractNote":"We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.","publicationTitle":"arXiv:1904.01326 [cs]","volume":"","issue":"","pages":"","date":"2019-10-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"HoloGAN","url":"http://arxiv.org/abs/1904.01326","accessDate":"2021-08-17T08:37:33Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1904.01326","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2021-08-17T08:37:33Z","dateModified":"2021-08-17T08:37:33Z"}},{"key":"43B6G49X","version":477,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/43B6G49X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/43B6G49X","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5CL67G2S","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"43B6G49X","version":477,"parentItem":"5CL67G2S","itemType":"note","note":"Comment: International Conference on Computer Vision ICCV 2019. For project page, see https://www.monkeyoverflow.com/#/hologan-unsupervised-learning-of-3d-representations-from-natural-images/","tags":[],"relations":{},"dateAdded":"2021-08-17T08:37:33Z","dateModified":"2021-08-17T08:37:33Z"}},{"key":"KUA5WGCH","version":483,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KUA5WGCH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KUA5WGCH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9TGH4IPE","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KUA5WGCH","version":483,"parentItem":"9TGH4IPE","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-08-17T07:59:15Z","url":"https://arxiv.org/abs/2104.08418","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"b607329f1e8898d41dc2e9e429fc0610","mtime":1629187155000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/TNGCUXXD"},"dateAdded":"2021-08-17T07:59:15Z","dateModified":"2021-08-17T07:59:15Z"}},{"key":"NIPFKPPY","version":483,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NIPFKPPY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NIPFKPPY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9TGH4IPE","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NIPFKPPY","version":483,"parentItem":"9TGH4IPE","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-17T07:59:01Z","url":"https://arxiv.org/pdf/2104.08418.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Xie et al. - 2021 - FiG-NeRF Figure-Ground Neural Radiance Fields for.pdf","md5":"e94917b69363670dfa9c3beeb3b74d2e","mtime":1629187141000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/9B38ZGQ7"},"dateAdded":"2021-08-17T07:59:01Z","dateModified":"2021-08-17T07:59:01Z"}},{"key":"9TGH4IPE","version":483,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9TGH4IPE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9TGH4IPE","type":"text/html"}},"meta":{"creatorSummary":"Xie et al.","parsedDate":"2021-04-16","numChildren":2},"data":{"key":"9TGH4IPE","version":483,"itemType":"journalArticle","title":"FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling","creators":[{"creatorType":"author","firstName":"Christopher","lastName":"Xie"},{"creatorType":"author","firstName":"Keunhong","lastName":"Park"},{"creatorType":"author","firstName":"Ricardo","lastName":"Martin-Brualla"},{"creatorType":"author","firstName":"Matthew","lastName":"Brown"}],"abstractNote":"We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image fidelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.","publicationTitle":"arXiv:2104.08418 [cs]","volume":"","issue":"","pages":"","date":"2021-04-16","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"FiG-NeRF","url":"http://arxiv.org/abs/2104.08418","accessDate":"2021-08-17T07:58:38Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.08418","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/8YGWBCHJ"},"dateAdded":"2021-08-17T07:58:38Z","dateModified":"2021-08-17T07:58:38Z"}},{"key":"MDCBU9RP","version":467,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MDCBU9RP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MDCBU9RP","type":"text/html"}},"meta":{"numChildren":0},"data":{"key":"MDCBU9RP","version":467,"itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-08-04T07:03:04Z","url":"https://arxiv.org/pdf/2011.13456.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf","md5":"3327d1e3196026d81e8ee11b39c49943","mtime":1628060584000,"tags":[],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/GPIBEZAU"},"dateAdded":"2021-08-04T07:03:04Z","dateModified":"2021-08-04T07:06:15Z"}},{"key":"6GK4EUYC","version":456,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6GK4EUYC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6GK4EUYC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/V4GQUIZW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6GK4EUYC","version":456,"parentItem":"V4GQUIZW","itemType":"attachment","linkMode":"imported_url","title":"Browne - An Introduction to MCMC methods and Bayesian Stati.pdf","accessDate":"2021-07-26T09:24:45Z","url":"https://www.ukdataservice.ac.uk/media/307220/presentation4.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Browne - An Introduction to MCMC methods and Bayesian Stati.pdf","md5":"4df47383f1edcdcc47cc8b7bbe76c583","mtime":1627291487000,"tags":[],"relations":{},"dateAdded":"2021-07-26T09:24:45Z","dateModified":"2021-07-26T09:24:47Z"}},{"key":"V4GQUIZW","version":455,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/V4GQUIZW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/V4GQUIZW","type":"text/html"}},"meta":{"creatorSummary":"Browne","numChildren":1},"data":{"key":"V4GQUIZW","version":455,"itemType":"journalArticle","title":"An Introduction to MCMC methods and Bayesian Statistics","creators":[{"creatorType":"author","firstName":"William","lastName":"Browne"}],"abstractNote":"","publicationTitle":"","volume":"","issue":"","pages":"69","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-07-26T09:24:47Z","dateModified":"2021-07-26T09:24:47Z"}},{"key":"KBUMFZTU","version":453,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KBUMFZTU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KBUMFZTU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/3GGK9957","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KBUMFZTU","version":453,"parentItem":"3GGK9957","itemType":"attachment","linkMode":"imported_url","title":"Welling and Teh - Bayesian Learning via Stochastic Gradient Langevin.pdf","accessDate":"2021-07-26T08:03:35Z","url":"http://www.gatsby.ucl.ac.uk/~ywteh/research/compstats/WelTeh2011a.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Welling and Teh - Bayesian Learning via Stochastic Gradient Langevin.pdf","md5":"08e1603f3d7cd5f8a476a084fd3f391a","mtime":1627286618000,"tags":[],"relations":{},"dateAdded":"2021-07-26T08:03:35Z","dateModified":"2021-07-26T08:03:38Z"}},{"key":"3GGK9957","version":452,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3GGK9957","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3GGK9957","type":"text/html"}},"meta":{"creatorSummary":"Welling and Teh","numChildren":1},"data":{"key":"3GGK9957","version":452,"itemType":"journalArticle","title":"Bayesian Learning via Stochastic Gradient Langevin Dynamics","creators":[{"creatorType":"author","firstName":"Max","lastName":"Welling"},{"creatorType":"author","firstName":"Yee Whye","lastName":"Teh"}],"abstractNote":"In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overﬁtting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a “sampling threshold” and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.","publicationTitle":"","volume":"","issue":"","pages":"8","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-07-26T08:03:37Z","dateModified":"2021-07-26T08:03:37Z"}},{"key":"MHIZINMQ","version":450,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MHIZINMQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MHIZINMQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CULAMFWA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MHIZINMQ","version":450,"parentItem":"CULAMFWA","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-07-22T16:32:38Z","url":"https://arxiv.org/pdf/2106.06819.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Sinha et al. - 2021 - D2C Diffusion-Denoising Models for Few-shot Condi.pdf","md5":"257f08a74d0e3f6f1b6232bdc7511bec","mtime":1626971622000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/JTHV3QMW"},"dateAdded":"2021-07-22T16:33:42Z","dateModified":"2021-07-26T06:53:48Z"}},{"key":"WI3LUR5M","version":451,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WI3LUR5M","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WI3LUR5M","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HFRNTGLQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WI3LUR5M","version":451,"parentItem":"HFRNTGLQ","itemType":"attachment","linkMode":"imported_url","title":"Cheng et al. - 2019 - A Bayesian Perspective on the Deep Image Prior.pdf","accessDate":"2021-07-26T06:50:56Z","url":"https://openaccess.thecvf.com/content_CVPR_2019/papers/Cheng_A_Bayesian_Perspective_on_the_Deep_Image_Prior_CVPR_2019_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Cheng et al. - 2019 - A Bayesian Perspective on the Deep Image Prior.pdf","md5":"2c25e185ebad9b7145db8dacdd0a2f19","mtime":1627282373000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/J9J8EY6P"},"dateAdded":"2021-07-26T06:52:53Z","dateModified":"2021-07-26T06:52:53Z"}},{"key":"HFRNTGLQ","version":450,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HFRNTGLQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HFRNTGLQ","type":"text/html"}},"meta":{"creatorSummary":"Cheng et al.","parsedDate":"2019","numChildren":1},"data":{"key":"HFRNTGLQ","version":450,"itemType":"conferencePaper","title":"A Bayesian Perspective on the Deep Image Prior","creators":[{"creatorType":"author","firstName":"Zezhou","lastName":"Cheng"},{"creatorType":"author","firstName":"Matheus","lastName":"Gadelha"},{"creatorType":"author","firstName":"Subhransu","lastName":"Maji"},{"creatorType":"author","firstName":"Daniel","lastName":"Sheldon"}],"abstractNote":"The deep image prior [26] was recently introduced as a prior for natural images. It represents images as the output of a convolutional network with random inputs. For “inference”, gradient descent is performed to adjust network parameters to make the output match observations. This approach yields good performance on a range of image reconstruction tasks. We show that the deep image prior is asymptotically equivalent to a stationary Gaussian process prior in the limit as the number of channels in each layer of the network goes to inﬁnity, and derive the corresponding kernel. This informs a Bayesian approach to inference. We show that by conducting posterior inference using stochastic gradient Langevin dynamics we avoid the need for early stopping, which is a drawback of the current approach, and improve results for denoising and impainting tasks. We illustrate these intuitions on a number of 1D and 2D signal reconstruction tasks.","date":"6/2019","proceedingsTitle":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","place":"Long Beach, CA, USA","publisher":"IEEE","volume":"","pages":"5438-5446","series":"","language":"en","DOI":"10.1109/CVPR.2019.00559","ISBN":"978-1-72813-293-8","shortTitle":"","url":"https://ieeexplore.ieee.org/document/8954206/","accessDate":"2021-07-26T06:51:00Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/EH6JRM9E"},"dateAdded":"2021-07-26T06:52:53Z","dateModified":"2021-07-26T06:52:53Z"}},{"key":"6A2V5AG4","version":448,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6A2V5AG4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6A2V5AG4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Z9FIEAFE","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6A2V5AG4","version":448,"parentItem":"Z9FIEAFE","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-07-26T06:51:41Z","url":"https://arxiv.org/abs/2006.07733","note":"","contentType":"text/html","charset":"utf-8","filename":"2006.html","md5":"9c45cae5fb6b6989c5aaa1bcf40d9589","mtime":1627282301000,"tags":[],"relations":{},"dateAdded":"2021-07-26T06:51:41Z","dateModified":"2021-07-26T06:51:41Z"}},{"key":"GEKQDA8H","version":448,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GEKQDA8H","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GEKQDA8H","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Z9FIEAFE","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GEKQDA8H","version":448,"parentItem":"Z9FIEAFE","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-07-26T06:51:36Z","url":"https://arxiv.org/pdf/2006.07733.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf","md5":"345ce318a237747d2531d3b2b728e2b7","mtime":1627282296000,"tags":[],"relations":{},"dateAdded":"2021-07-26T06:51:36Z","dateModified":"2021-07-26T06:51:36Z"}},{"key":"Z9FIEAFE","version":446,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z9FIEAFE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z9FIEAFE","type":"text/html"}},"meta":{"creatorSummary":"Grill et al.","parsedDate":"2020-09-10","numChildren":2},"data":{"key":"Z9FIEAFE","version":446,"itemType":"journalArticle","title":"Bootstrap your own latent: A new approach to self-supervised Learning","creators":[{"creatorType":"author","firstName":"Jean-Bastien","lastName":"Grill"},{"creatorType":"author","firstName":"Florian","lastName":"Strub"},{"creatorType":"author","firstName":"Florent","lastName":"Altché"},{"creatorType":"author","firstName":"Corentin","lastName":"Tallec"},{"creatorType":"author","firstName":"Pierre H.","lastName":"Richemond"},{"creatorType":"author","firstName":"Elena","lastName":"Buchatskaya"},{"creatorType":"author","firstName":"Carl","lastName":"Doersch"},{"creatorType":"author","firstName":"Bernardo Avila","lastName":"Pires"},{"creatorType":"author","firstName":"Zhaohan Daniel","lastName":"Guo"},{"creatorType":"author","firstName":"Mohammad Gheshlaghi","lastName":"Azar"},{"creatorType":"author","firstName":"Bilal","lastName":"Piot"},{"creatorType":"author","firstName":"Koray","lastName":"Kavukcuoglu"},{"creatorType":"author","firstName":"Rémi","lastName":"Munos"},{"creatorType":"author","firstName":"Michal","lastName":"Valko"}],"abstractNote":"We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.","publicationTitle":"arXiv:2006.07733 [cs, stat]","volume":"","issue":"","pages":"","date":"2020-09-10","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Bootstrap your own latent","url":"http://arxiv.org/abs/2006.07733","accessDate":"2021-07-26T06:51:08Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2006.07733","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-07-26T06:51:08Z","dateModified":"2021-07-26T06:51:08Z"}},{"key":"M7ZLIB6F","version":441,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/M7ZLIB6F","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/M7ZLIB6F","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/AR4NYQRM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"M7ZLIB6F","version":441,"parentItem":"AR4NYQRM","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-07-22T16:32:36Z","url":"https://arxiv.org/abs/2106.03802","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"32075aa217073e7581f9fb5829b52805","mtime":1626971556000,"tags":[],"relations":{},"dateAdded":"2021-07-22T16:32:36Z","dateModified":"2021-07-22T16:32:36Z"}},{"key":"KDITVDFZ","version":441,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KDITVDFZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KDITVDFZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/AR4NYQRM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KDITVDFZ","version":441,"parentItem":"AR4NYQRM","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-07-22T16:32:30Z","url":"https://arxiv.org/pdf/2106.03802.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Watson et al. - 2021 - Learning to Efficiently Sample from Diffusion Prob.pdf","md5":"d24d7d95aa6e4e1e1d7d21dd1f13211d","mtime":1626971550000,"tags":[],"relations":{},"dateAdded":"2021-07-22T16:32:30Z","dateModified":"2021-07-22T16:32:30Z"}},{"key":"AR4NYQRM","version":440,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AR4NYQRM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AR4NYQRM","type":"text/html"}},"meta":{"creatorSummary":"Watson et al.","parsedDate":"2021-06-07","numChildren":2},"data":{"key":"AR4NYQRM","version":440,"itemType":"journalArticle","title":"Learning to Efficiently Sample from Diffusion Probabilistic Models","creators":[{"creatorType":"author","firstName":"Daniel","lastName":"Watson"},{"creatorType":"author","firstName":"Jonathan","lastName":"Ho"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"},{"creatorType":"author","firstName":"William","lastName":"Chan"}],"abstractNote":"Denoising Diffusion Probabilistic Models (DDPMs) have emerged as a powerful family of generative models that can yield high-fidelity samples and competitive log-likelihoods across a range of domains, including image and speech synthesis. Key advantages of DDPMs include ease of training, in contrast to generative adversarial networks, and speed of generation, in contrast to autoregressive models. However, DDPMs typically require hundreds-to-thousands of steps to generate a high fidelity sample, making them prohibitively expensive for high dimensional problems. Fortunately, DDPMs allow trading generation speed for sample quality through adjusting the number of refinement steps as a post process. Prior work has been successful in improving generation speed through handcrafting the time schedule by trial and error. We instead view the selection of the inference time schedules as an optimization problem, and introduce an exact dynamic programming algorithm that finds the optimal discrete time schedules for any pre-trained DDPM. Our method exploits the fact that ELBO can be decomposed into separate KL terms, and given any computation budget, discovers the time schedule that maximizes the training ELBO exactly. Our method is efficient, has no hyper-parameters of its own, and can be applied to any pre-trained DDPM with no retraining. We discover inference time schedules requiring as few as 32 refinement steps, while sacrificing less than 0.1 bits per dimension compared to the default 4,000 steps used on ImageNet 64x64 [Ho et al., 2020; Nichol and Dhariwal, 2021].","publicationTitle":"arXiv:2106.03802 [cs]","volume":"","issue":"","pages":"","date":"2021-06-07","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2106.03802","accessDate":"2021-07-22T16:32:18Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.03802","tags":[{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-07-22T16:32:18Z","dateModified":"2021-07-22T16:32:18Z"}},{"key":"LTBUWUG5","version":653,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LTBUWUG5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LTBUWUG5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DSBTHNHU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LTBUWUG5","version":653,"parentItem":"DSBTHNHU","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-07-22T10:14:11Z","url":"https://arxiv.org/pdf/1812.04948.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf","md5":"b6e9e3584ef0bc24db7b2c7599192f4a","mtime":1626948851000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/HGC67PAF"},"dateAdded":"2021-07-22T10:14:11Z","dateModified":"2021-07-22T10:14:11Z"}},{"key":"5A4I3GI7","version":653,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5A4I3GI7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5A4I3GI7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DSBTHNHU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5A4I3GI7","version":653,"parentItem":"DSBTHNHU","itemType":"note","note":"Comment: CVPR 2019 final version","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/LBYGLCZR"},"dateAdded":"2021-07-22T05:42:42Z","dateModified":"2021-07-22T05:42:42Z"}},{"key":"DSBTHNHU","version":653,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DSBTHNHU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DSBTHNHU","type":"text/html"}},"meta":{"creatorSummary":"Karras et al.","parsedDate":"2019-03-29","numChildren":2},"data":{"key":"DSBTHNHU","version":653,"itemType":"journalArticle","title":"A Style-Based Generator Architecture for Generative Adversarial Networks","creators":[{"creatorType":"author","firstName":"Tero","lastName":"Karras"},{"creatorType":"author","firstName":"Samuli","lastName":"Laine"},{"creatorType":"author","firstName":"Timo","lastName":"Aila"}],"abstractNote":"We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.","publicationTitle":"arXiv:1812.04948 [cs, stat]","volume":"","issue":"","pages":"","date":"2019-03-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1812.04948","accessDate":"2021-07-22T05:42:42Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1812.04948","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Neural and Evolutionary Computing","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/YAZ97626"},"dateAdded":"2021-07-22T05:42:42Z","dateModified":"2021-07-22T05:42:42Z"}},{"key":"FKMYNW4B","version":653,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FKMYNW4B","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FKMYNW4B","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5AA4YFUK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FKMYNW4B","version":653,"parentItem":"5AA4YFUK","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-07-22T05:17:52Z","url":"https://arxiv.org/pdf/2007.06600.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Shen and Zhou - 2021 - Closed-Form Factorization of Latent Semantics in G.pdf","md5":"9241fa28c8f72f48c768b2309cb1f729","mtime":1626931072000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/JI5JY57K"},"dateAdded":"2021-07-22T05:17:52Z","dateModified":"2021-07-22T05:17:52Z"}},{"key":"5AA4YFUK","version":653,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5AA4YFUK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5AA4YFUK","type":"text/html"}},"meta":{"creatorSummary":"Shen and Zhou","parsedDate":"2021-04-03","numChildren":2},"data":{"key":"5AA4YFUK","version":653,"itemType":"journalArticle","title":"Closed-Form Factorization of Latent Semantics in GANs","creators":[{"creatorType":"author","firstName":"Yujun","lastName":"Shen"},{"creatorType":"author","firstName":"Bolei","lastName":"Zhou"}],"abstractNote":"A rich set of interpretable dimensions has been shown to emerge in the latent space of the Generative Adversarial Networks (GANs) trained for synthesizing images. In order to identify such latent dimensions for image editing, previous methods typically annotate a collection of synthesized samples and train linear classifiers in the latent space. However, they require a clear definition of the target attribute as well as the corresponding manual annotations, limiting their applications in practice. In this work, we examine the internal representation learned by GANs to reveal the underlying variation factors in an unsupervised manner. In particular, we take a closer look into the generation mechanism of GANs and further propose a closed-form factorization algorithm for latent semantic discovery by directly decomposing the pre-trained weights. With a lightning-fast implementation, our approach is capable of not only finding semantically meaningful dimensions comparably to the state-of-the-art supervised methods, but also resulting in far more versatile concepts across multiple GAN models trained on a wide range of datasets.","publicationTitle":"arXiv:2007.06600 [cs]","volume":"","issue":"","pages":"","date":"2021-04-03","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2007.06600","accessDate":"2021-07-22T05:15:38Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2007.06600","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/963KE6YH"},"dateAdded":"2021-07-22T05:15:39Z","dateModified":"2021-07-22T05:15:39Z"}},{"key":"BYCZTQZP","version":653,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BYCZTQZP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BYCZTQZP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5AA4YFUK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BYCZTQZP","version":653,"parentItem":"5AA4YFUK","itemType":"note","note":"Comment: CVPR 2021 camera-ready","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/Y6IJLS9K"},"dateAdded":"2021-07-22T05:15:39Z","dateModified":"2021-07-22T05:15:39Z"}},{"key":"BJVA7FRI","version":426,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BJVA7FRI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BJVA7FRI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/7FCRP639","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BJVA7FRI","version":426,"parentItem":"7FCRP639","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-07-22T04:50:07Z","url":"https://arxiv.org/pdf/2107.05775v1.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Guo et al. - 2021 - Fast and Explicit Neural View Synthesis.pdf","md5":"9183569d49d2ed3b594b184478410020","mtime":1626929407000,"tags":[],"relations":{},"dateAdded":"2021-07-22T04:50:07Z","dateModified":"2021-07-22T04:50:07Z"}},{"key":"BIHLPAUD","version":424,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BIHLPAUD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BIHLPAUD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/7FCRP639","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BIHLPAUD","version":424,"parentItem":"7FCRP639","itemType":"attachment","linkMode":"imported_url","title":"Snapshot","accessDate":"2021-07-20T05:01:55Z","url":"https://arxiv.org/abs/2107.05775","note":"","contentType":"text/html","charset":"utf-8","filename":"2107.html","md5":null,"mtime":null,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/M7TF644Q"},"dateAdded":"2021-07-22T04:47:34Z","dateModified":"2021-07-22T04:47:34Z"}},{"key":"69UGEBW6","version":424,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/69UGEBW6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/69UGEBW6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/7FCRP639","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"69UGEBW6","version":424,"parentItem":"7FCRP639","itemType":"note","note":"<p>Pure: use 3D unet to predict RGBA voxel, then volumn rendering voxel.<br />dataset require to train and  seem like doesn't support view-dependent<br />single V100 @ FPS 245</p>","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/5CZBN3CT"},"dateAdded":"2021-07-22T04:47:34Z","dateModified":"2021-07-22T04:47:34Z"}},{"key":"7FCRP639","version":424,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7FCRP639","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7FCRP639","type":"text/html"}},"meta":{"creatorSummary":"Guo et al.","parsedDate":"2021-07-12","numChildren":3},"data":{"key":"7FCRP639","version":424,"itemType":"journalArticle","title":"Fast and Explicit Neural View Synthesis","creators":[{"creatorType":"author","firstName":"Pengsheng","lastName":"Guo"},{"creatorType":"author","firstName":"Miguel Angel","lastName":"Bautista"},{"creatorType":"author","firstName":"Alex","lastName":"Colburn"},{"creatorType":"author","firstName":"Liang","lastName":"Yang"},{"creatorType":"author","firstName":"Daniel","lastName":"Ulbricht"},{"creatorType":"author","firstName":"Joshua M.","lastName":"Susskind"},{"creatorType":"author","firstName":"Qi","lastName":"Shan"}],"abstractNote":"We study the problem of novel view synthesis of a scene comprised of 3D objects. We propose a simple yet effective approach that is neither continuous nor implicit, challenging recent trends on view synthesis. We demonstrate that although continuous radiance field representations have gained a lot of attention due to their expressive power, our simple approach obtains comparable or even better novel view reconstruction quality comparing with state-of-the-art baselines while increasing rendering speed by over 400x. Our model is trained in a category-agnostic manner and does not require scene-specific optimization. Therefore, it is able to generalize novel view synthesis to object categories not seen during training. In addition, we show that with our simple formulation, we can use view synthesis as a self-supervision signal for efficient learning of 3D geometry without explicit 3D supervision.","publicationTitle":"","volume":"","issue":"","pages":"","date":"2021/07/12","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"https://arxiv.org/abs/2107.05775v1","accessDate":"2021-07-20T05:01:51Z","archive":"","archiveLocation":"","libraryCatalog":"arxiv.org","callNumber":"","rights":"","extra":"","tags":[{"tag":"pure"}],"collections":["CPYKW3PF"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/4LPNJ4WU"},"dateAdded":"2021-07-22T04:47:34Z","dateModified":"2021-07-22T04:47:34Z"}},{"key":"VJ63H28S","version":418,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VJ63H28S","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VJ63H28S","type":"text/html"}},"meta":{"creatorSummary":"Spezialetti et al.","parsedDate":"2019-09-15","numChildren":3},"data":{"key":"VJ63H28S","version":418,"itemType":"journalArticle","title":"Learning an Effective Equivariant 3D Descriptor Without Supervision","creators":[{"creatorType":"author","firstName":"Riccardo","lastName":"Spezialetti"},{"creatorType":"author","firstName":"Samuele","lastName":"Salti"},{"creatorType":"author","firstName":"Luigi","lastName":"Di Stefano"}],"abstractNote":"Establishing correspondences between 3D shapes is a fundamental task in 3D Computer Vision, typically addressed by matching local descriptors. Recently, a few attempts at applying the deep learning paradigm to the task have shown promising results. Yet, the only explored way to learn rotation invariant descriptors has been to feed neural networks with highly engineered and invariant representations provided by existing hand-crafted descriptors, a path that goes in the opposite direction of end-to-end learning from raw data so successfully deployed for 2D images. In this paper, we explore the benefits of taking a step back in the direction of end-to-end learning of 3D descriptors by disentangling the creation of a robust and distinctive rotation equivariant representation, which can be learned from unoriented input data, and the definition of a good canonical orientation, required only at test time to obtain an invariant descriptor. To this end, we leverage two recent innovations: spherical convolutional neural networks to learn an equivariant descriptor and plane folding decoders to learn without supervision. The effectiveness of the proposed approach is experimentally validated by outperforming hand-crafted and learned descriptors on a standard benchmark.","publicationTitle":"arXiv:1909.06887 [cs]","volume":"","issue":"","pages":"","date":"2019-09-15","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1909.06887","accessDate":"2021-07-19T10:33:05Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1909.06887","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"aek"}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/QD8NM686"},"dateAdded":"2021-07-19T10:33:06Z","dateModified":"2021-07-19T10:33:28Z"}},{"key":"CV3HFNPK","version":419,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CV3HFNPK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CV3HFNPK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VJ63H28S","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CV3HFNPK","version":419,"parentItem":"VJ63H28S","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-07-19T10:33:24Z","url":"https://arxiv.org/abs/1909.06887","note":"","contentType":"text/html","charset":"utf-8","filename":"1909.html","md5":"f55d38d0666737e4a801844d4d30c611","mtime":1626690804000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/U6K8SKLW"},"dateAdded":"2021-07-19T10:33:24Z","dateModified":"2021-07-19T10:33:24Z"}},{"key":"32NVJWQ5","version":419,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/32NVJWQ5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/32NVJWQ5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VJ63H28S","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"32NVJWQ5","version":419,"parentItem":"VJ63H28S","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-07-19T10:33:19Z","url":"https://arxiv.org/pdf/1909.06887.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Spezialetti et al. - 2019 - Learning an Effective Equivariant 3D Descriptor Wi.pdf","md5":"3a769f15720bae67877efc7d0074624f","mtime":1626690799000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/ECNG7ZGP"},"dateAdded":"2021-07-19T10:33:19Z","dateModified":"2021-07-19T10:33:19Z"}},{"key":"AEY84LJN","version":418,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AEY84LJN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AEY84LJN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VJ63H28S","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AEY84LJN","version":418,"parentItem":"VJ63H28S","itemType":"note","note":"Comment: Accepted to International Conference on Computer Vision 2019","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/QW77TRNV"},"dateAdded":"2021-07-19T10:33:06Z","dateModified":"2021-07-19T10:33:06Z"}},{"key":"GT64CMAU","version":416,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GT64CMAU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GT64CMAU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XPVFILBL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GT64CMAU","version":416,"parentItem":"XPVFILBL","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-07-12T06:00:37Z","url":"https://arxiv.org/pdf/2107.04589.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Lee et al. - 2021 - ViTGAN Training GANs with Vision Transformers.pdf","md5":"c51c067d09bebb02f26a6c7f1f0ec455","mtime":1626160968000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/R6GY2NFF"},"dateAdded":"2021-07-13T07:22:48Z","dateModified":"2021-07-17T05:02:15Z"}},{"key":"3WU7XIZH","version":417,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3WU7XIZH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3WU7XIZH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/74XD9YIJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3WU7XIZH","version":417,"parentItem":"74XD9YIJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-07-16T08:59:42Z","url":"https://arxiv.org/pdf/2009.09485.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tewari et al. - 2020 - PIE Portrait Image Embedding for Semantic Control.pdf","md5":"8e4b17f52fcadc1cdae152cb148f741c","mtime":1626498127000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/NV9YSZR8"},"dateAdded":"2021-07-17T05:02:07Z","dateModified":"2021-07-17T05:02:10Z"}},{"key":"74XD9YIJ","version":415,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/74XD9YIJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/74XD9YIJ","type":"text/html"}},"meta":{"creatorSummary":"Tewari et al.","parsedDate":"2020-09-20","numChildren":2},"data":{"key":"74XD9YIJ","version":415,"itemType":"journalArticle","title":"PIE: Portrait Image Embedding for Semantic Control","creators":[{"creatorType":"author","firstName":"Ayush","lastName":"Tewari"},{"creatorType":"author","firstName":"Mohamed","lastName":"Elgharib"},{"creatorType":"author","firstName":"Mallikarjun B.","lastName":"R."},{"creatorType":"author","firstName":"Florian","lastName":"Bernard"},{"creatorType":"author","firstName":"Hans-Peter","lastName":"Seidel"},{"creatorType":"author","firstName":"Patrick","lastName":"Pérez"},{"creatorType":"author","firstName":"Michael","lastName":"Zollhöfer"},{"creatorType":"author","firstName":"Christian","lastName":"Theobalt"}],"abstractNote":"Editing of portrait images is a very popular and important research topic with a large variety of applications. For ease of use, control should be provided via a semantically meaningful parameterization that is akin to computer animation controls. The vast majority of existing techniques do not provide such intuitive and fine-grained control, or only enable coarse editing of a single isolated control parameter. Very recently, high-quality semantically controlled editing has been demonstrated, however only on synthetically created StyleGAN images. We present the first approach for embedding real portrait images in the latent space of StyleGAN, which allows for intuitive editing of the head pose, facial expression, and scene illumination in the image. Semantic editing in parameter space is achieved based on StyleRig, a pretrained neural network that maps the control space of a 3D morphable face model to the latent space of the GAN. We design a novel hierarchical non-linear optimization problem to obtain the embedding. An identity preservation energy term allows spatially coherent edits while maintaining facial integrity. Our approach runs at interactive frame rates and thus allows the user to explore the space of possible edits. We evaluate our approach on a wide set of portrait photos, compare it to the current state of the art, and validate the effectiveness of its components in an ablation study.","publicationTitle":"arXiv:2009.09485 [cs]","volume":"","issue":"","pages":"","date":"2020-09-20","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"PIE","url":"http://arxiv.org/abs/2009.09485","accessDate":"2021-07-16T08:58:35Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2009.09485","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/I6MPCQS2"},"dateAdded":"2021-07-16T08:58:39Z","dateModified":"2021-07-17T05:01:49Z"}},{"key":"Q8TF2BKA","version":413,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Q8TF2BKA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Q8TF2BKA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/74XD9YIJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Q8TF2BKA","version":413,"parentItem":"74XD9YIJ","itemType":"note","note":"Comment: To appear in SIGGRAPH Asia 2020. Project webpage: https://gvv.mpi-inf.mpg.de/projects/PIE/","tags":[],"relations":{},"dateAdded":"2021-07-16T08:58:39Z","dateModified":"2021-07-16T08:58:39Z"}},{"key":"8PEEG5WI","version":412,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8PEEG5WI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8PEEG5WI","type":"text/html"}},"meta":{"creatorSummary":"Saha et al.","parsedDate":"2021-03-10","numChildren":2},"data":{"key":"8PEEG5WI","version":412,"itemType":"journalArticle","title":"LOHO: Latent Optimization of Hairstyles via Orthogonalization","creators":[{"creatorType":"author","firstName":"Rohit","lastName":"Saha"},{"creatorType":"author","firstName":"Brendan","lastName":"Duke"},{"creatorType":"author","firstName":"Florian","lastName":"Shkurti"},{"creatorType":"author","firstName":"Graham W.","lastName":"Taylor"},{"creatorType":"author","firstName":"Parham","lastName":"Aarabi"}],"abstractNote":"Hairstyle transfer is challenging due to hair structure differences in the source and target hair. Therefore, we propose Latent Optimization of Hairstyles via Orthogonalization (LOHO), an optimization-based approach using GAN inversion to infill missing hair structure details in latent space during hairstyle transfer. Our approach decomposes hair into three attributes: perceptual structure, appearance, and style, and includes tailored losses to model each of these attributes independently. Furthermore, we propose two-stage optimization and gradient orthogonalization to enable disentangled latent space optimization of our hair attributes. Using LOHO for latent space manipulation, users can synthesize novel photorealistic images by manipulating hair attributes either individually or jointly, transferring the desired attributes from reference hairstyles. LOHO achieves a superior FID compared with the current state-of-the-art (SOTA) for hairstyle transfer. Additionally, LOHO preserves the subject's identity comparably well according to PSNR and SSIM when compared to SOTA image embedding pipelines. Code is available at https://github.com/dukebw/LOHO.","publicationTitle":"arXiv:2103.03891 [cs]","volume":"","issue":"","pages":"","date":"2021-03-10","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"LOHO","url":"http://arxiv.org/abs/2103.03891","accessDate":"2021-07-16T04:08:51Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.03891","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":["F2HZKCVK"],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/KH3GZIYP"},"dateAdded":"2021-07-16T04:08:51Z","dateModified":"2021-07-16T08:17:14Z"}},{"key":"P5VAN9KJ","version":410,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/P5VAN9KJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/P5VAN9KJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/8PEEG5WI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"P5VAN9KJ","version":410,"parentItem":"8PEEG5WI","itemType":"note","note":"Comment: CVPR 2021","tags":[],"relations":{},"dateAdded":"2021-07-16T04:08:51Z","dateModified":"2021-07-16T04:08:51Z"}},{"key":"XPVFILBL","version":406,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XPVFILBL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XPVFILBL","type":"text/html"}},"meta":{"creatorSummary":"Lee et al.","parsedDate":"2021-07-09","numChildren":1},"data":{"key":"XPVFILBL","version":406,"itemType":"journalArticle","title":"ViTGAN: Training GANs with Vision Transformers","creators":[{"creatorType":"author","firstName":"Kwonjoon","lastName":"Lee"},{"creatorType":"author","firstName":"Huiwen","lastName":"Chang"},{"creatorType":"author","firstName":"Lu","lastName":"Jiang"},{"creatorType":"author","firstName":"Han","lastName":"Zhang"},{"creatorType":"author","firstName":"Zhuowen","lastName":"Tu"},{"creatorType":"author","firstName":"Ce","lastName":"Liu"}],"abstractNote":"Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such observation can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). We observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce novel regularization techniques for training GANs with ViTs. Empirically, our approach, named ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2 on CIFAR-10, CelebA, and LSUN bedroom datasets.","publicationTitle":"arXiv:2107.04589 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-07-09","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"ViTGAN","url":"http://arxiv.org/abs/2107.04589","accessDate":"2021-07-12T06:00:16Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2107.04589","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1}],"collections":["LT39298Q"],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/4WSSKLDR"},"dateAdded":"2021-07-12T06:00:20Z","dateModified":"2021-07-13T05:47:42Z"}},{"key":"FJACULSK","version":406,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FJACULSK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FJACULSK","type":"text/html"}},"meta":{"creatorSummary":"Siarohin et al.","parsedDate":"2021-04-22","numChildren":0},"data":{"key":"FJACULSK","version":406,"itemType":"journalArticle","title":"Motion Representations for Articulated Animation","creators":[{"creatorType":"author","firstName":"Aliaksandr","lastName":"Siarohin"},{"creatorType":"author","firstName":"Oliver J.","lastName":"Woodford"},{"creatorType":"author","firstName":"Jian","lastName":"Ren"},{"creatorType":"author","firstName":"Menglei","lastName":"Chai"},{"creatorType":"author","firstName":"Sergey","lastName":"Tulyakov"}],"abstractNote":"We propose novel motion representations for animating articulated objects consisting of distinct parts. In a completely unsupervised manner, our method identifies object parts, tracks them in a driving video, and infers their motions by considering their principal axes. In contrast to the previous keypoint-based works, our method extracts meaningful and consistent regions, describing locations, shape, and pose. The regions correspond to semantically relevant and distinct object parts, that are more easily detected in frames of the driving video. To force decoupling of foreground from background, we model non-object related global motion with an additional affine transformation. To facilitate animation and prevent the leakage of the shape of the driving object, we disentangle shape and pose of objects in the region space. Our model can animate a variety of objects, surpassing previous methods by a large margin on existing benchmarks. We present a challenging new benchmark with high-resolution videos and show that the improvement is particularly pronounced when articulated objects are considered, reaching 96.6% user preference vs. the state of the art.","publicationTitle":"arXiv:2104.11280 [cs]","volume":"","issue":"","pages":"","date":"2021-04-22","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2104.11280","accessDate":"2021-07-13T04:20:14Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.11280","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["F2HZKCVK"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/YX5KKJFZ","dc:replaces":"http://zotero.org/users/7902311/items/DJ6YW5RF"},"dateAdded":"2021-07-13T04:20:16Z","dateModified":"2021-07-13T05:47:40Z"}},{"key":"QXN8HANK","version":402,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QXN8HANK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QXN8HANK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ILRZA9ZA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QXN8HANK","version":402,"parentItem":"ILRZA9ZA","itemType":"attachment","linkMode":"imported_url","title":"Variational Inference - Deriving the ELBO · Infinite n♾rm","accessDate":"2021-06-24T10:34:49Z","url":"https://chrisorm.github.io/VI-ELBO.html","note":"","contentType":"text/html","charset":"utf-8","filename":"VI-ELBO.html","md5":"09fb20cd1d465881f37d86cc94339dff","mtime":1624530889000,"tags":[],"relations":{},"dateAdded":"2021-06-24T10:34:49Z","dateModified":"2021-07-11T08:19:12Z"}},{"key":"HA4JH92Q","version":399,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HA4JH92Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HA4JH92Q","type":"text/html"}},"meta":{"creatorSummary":"Bi et al.","parsedDate":"2020-08-16","numChildren":0},"data":{"key":"HA4JH92Q","version":399,"itemType":"journalArticle","title":"Neural Reflectance Fields for Appearance Acquisition","creators":[{"creatorType":"author","firstName":"Sai","lastName":"Bi"},{"creatorType":"author","firstName":"Zexiang","lastName":"Xu"},{"creatorType":"author","firstName":"Pratul","lastName":"Srinivasan"},{"creatorType":"author","firstName":"Ben","lastName":"Mildenhall"},{"creatorType":"author","firstName":"Kalyan","lastName":"Sunkavalli"},{"creatorType":"author","firstName":"Miloš","lastName":"Hašan"},{"creatorType":"author","firstName":"Yannick","lastName":"Hold-Geoffroy"},{"creatorType":"author","firstName":"David","lastName":"Kriegman"},{"creatorType":"author","firstName":"Ravi","lastName":"Ramamoorthi"}],"abstractNote":"We present Neural Reflectance Fields, a novel deep scene representation that encodes volume density, normal and reflectance properties at any 3D point in a scene using a fully-connected neural network. We combine this representation with a physically-based differentiable ray marching framework that can render images from a neural reflectance field under any viewpoint and light. We demonstrate that neural reflectance fields can be estimated from images captured with a simple collocated camera-light setup, and accurately model the appearance of real-world scenes with complex geometry and reflectance. Once estimated, they can be used to render photo-realistic images under novel viewpoint and (non-collocated) lighting conditions and accurately reproduce challenging effects like specularities, shadows and occlusions. This allows us to perform high-quality view synthesis and relighting that is significantly better than previous methods. We also demonstrate that we can compose the estimated neural reflectance field of a real scene with traditional scene models and render them using standard Monte Carlo rendering engines. Our work thus enables a complete pipeline from high-quality and practical appearance acquisition to 3D scene composition and rendering.","publicationTitle":"arXiv:2008.03824 [cs]","volume":"","issue":"","pages":"","date":"2020-08-16","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2008.03824","accessDate":"2021-07-09T14:43:26Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2008.03824","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":["CPYKW3PF"],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/FHW6GZKG"},"dateAdded":"2021-07-09T14:43:27Z","dateModified":"2021-07-10T05:02:09Z"}},{"key":"8PG6C7E7","version":395,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8PG6C7E7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8PG6C7E7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IPMUGLKE","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8PG6C7E7","version":395,"parentItem":"IPMUGLKE","itemType":"attachment","linkMode":"imported_url","title":"Wang et al. - One-Shot Free-View Neural Talking-Head Synthesis f.pdf","accessDate":"2021-07-06T03:19:13Z","url":"https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_One-Shot_Free-View_Neural_Talking-Head_Synthesis_for_Video_Conferencing_CVPR_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wang et al. - One-Shot Free-View Neural Talking-Head Synthesis f.pdf","md5":"23482531849e5df50a270a33aca6dc97","mtime":1625541555000,"tags":[],"relations":{},"dateAdded":"2021-07-06T03:19:13Z","dateModified":"2021-07-08T16:00:54Z"}},{"key":"IPMUGLKE","version":395,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IPMUGLKE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IPMUGLKE","type":"text/html"}},"meta":{"creatorSummary":"Wang et al.","parsedDate":"2021-04-02","numChildren":4},"data":{"key":"IPMUGLKE","version":395,"itemType":"journalArticle","title":"One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing","creators":[{"creatorType":"author","firstName":"Ting-Chun","lastName":"Wang"},{"creatorType":"author","firstName":"Arun","lastName":"Mallya"},{"creatorType":"author","firstName":"Ming-Yu","lastName":"Liu"}],"abstractNote":"We propose a neural talking-head video synthesis model and demonstrate its application to video conferencing. Our model learns to synthesize a talking-head video using a source image containing the target person's appearance and a driving video that dictates the motion in the output. Our motion is encoded based on a novel keypoint representation, where the identity-specific and motion-related information is decomposed unsupervisedly. Extensive experimental validation shows that our model outperforms competing methods on benchmark datasets. Moreover, our compact keypoint representation enables a video conferencing system that achieves the same visual quality as the commercial H.264 standard while only using one-tenth of the bandwidth. Besides, we show our keypoint representation allows the user to rotate the head during synthesis, which is useful for simulating face-to-face video conferencing experiences.","publicationTitle":"arXiv:2011.15126 [cs]","volume":"","issue":"","pages":"","date":"2021-04-02","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2011.15126","accessDate":"2021-06-29T08:34:13Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.15126","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["F2HZKCVK"],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/CU6M3ILJ"},"dateAdded":"2021-06-29T08:34:13Z","dateModified":"2021-07-08T16:00:54Z"}},{"key":"TV9BRWHD","version":395,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TV9BRWHD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TV9BRWHD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RF7ID2P5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TV9BRWHD","version":395,"parentItem":"RF7ID2P5","itemType":"note","note":"<p>pixel independent using MLP</p>","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/GQBYIFA4"},"dateAdded":"2021-07-06T03:37:16Z","dateModified":"2021-07-08T16:00:53Z"}},{"key":"BAIDR9BN","version":395,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BAIDR9BN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BAIDR9BN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RF7ID2P5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BAIDR9BN","version":395,"parentItem":"RF7ID2P5","itemType":"attachment","linkMode":"imported_url","title":"Anokhin et al. - Image Generators With Conditionally-Independent Pi.pdf","accessDate":"2021-07-06T03:17:32Z","url":"https://openaccess.thecvf.com/content/CVPR2021/papers/Anokhin_Image_Generators_With_Conditionally-Independent_Pixel_Synthesis_CVPR_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Anokhin et al. - Image Generators With Conditionally-Independent Pi.pdf","md5":"23e94dc779e06f860785d24baff2896a","mtime":1625541455000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/N659SULU"},"dateAdded":"2021-07-06T03:17:32Z","dateModified":"2021-07-08T16:00:53Z"}},{"key":"FDKKTGXY","version":392,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FDKKTGXY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FDKKTGXY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PEW9ADIM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FDKKTGXY","version":392,"parentItem":"PEW9ADIM","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-07-08T09:24:32Z","url":"https://arxiv.org/abs/1810.12894","note":"","contentType":"text/html","charset":"utf-8","filename":"1810.html","md5":"f0cf153bb57cacf74659a1319285ce24","mtime":1625736272000,"tags":[],"relations":{},"dateAdded":"2021-07-08T09:24:32Z","dateModified":"2021-07-08T09:24:32Z"}},{"key":"6G342CTV","version":391,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6G342CTV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6G342CTV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PEW9ADIM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6G342CTV","version":391,"parentItem":"PEW9ADIM","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-07-08T09:24:24Z","url":"https://arxiv.org/pdf/1810.12894.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Burda et al. - 2018 - Exploration by Random Network Distillation.pdf","md5":"2ef130b3bb6d46def41642cb8a3219fa","mtime":1625736264000,"tags":[],"relations":{},"dateAdded":"2021-07-08T09:24:24Z","dateModified":"2021-07-08T09:24:24Z"}},{"key":"PEW9ADIM","version":390,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PEW9ADIM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PEW9ADIM","type":"text/html"}},"meta":{"creatorSummary":"Burda et al.","parsedDate":"2018-10-30","numChildren":2},"data":{"key":"PEW9ADIM","version":390,"itemType":"journalArticle","title":"Exploration by Random Network Distillation","creators":[{"creatorType":"author","firstName":"Yuri","lastName":"Burda"},{"creatorType":"author","firstName":"Harrison","lastName":"Edwards"},{"creatorType":"author","firstName":"Amos","lastName":"Storkey"},{"creatorType":"author","firstName":"Oleg","lastName":"Klimov"}],"abstractNote":"We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.","publicationTitle":"arXiv:1810.12894 [cs, stat]","volume":"","issue":"","pages":"","date":"2018-10-30","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1810.12894","accessDate":"2021-07-08T09:24:05Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1810.12894","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-07-08T09:24:05Z","dateModified":"2021-07-08T09:24:05Z"}},{"key":"CI2S3ESR","version":387,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CI2S3ESR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CI2S3ESR","type":"text/html"}},"meta":{"creatorSummary":"Božicˇ et al.","numChildren":1},"data":{"key":"CI2S3ESR","version":387,"itemType":"journalArticle","title":"TransformerFusion: Monocular RGB Scene Reconstruction using Transformers","creators":[{"creatorType":"author","firstName":"Aljaž","lastName":"Božicˇ"},{"creatorType":"author","firstName":"Pablo","lastName":"Palafox"},{"creatorType":"author","firstName":"Justus","lastName":"Thies"},{"creatorType":"author","firstName":"Angela","lastName":"Dai"},{"creatorType":"author","firstName":"Matthias","lastName":"Nießner"}],"abstractNote":"We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-ﬁne fashion, storing ﬁne-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-ﬁne 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.","publicationTitle":"","volume":"","issue":"","pages":"17","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-07-07T15:38:20Z","dateModified":"2021-07-07T15:38:22Z"}},{"key":"Y9ZHRR4U","version":388,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y9ZHRR4U","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y9ZHRR4U","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CI2S3ESR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Y9ZHRR4U","version":388,"parentItem":"CI2S3ESR","itemType":"attachment","linkMode":"imported_url","title":"Božicˇ et al. - TransformerFusion Monocular RGB Scene Reconstruct.pdf","accessDate":"2021-07-07T15:38:18Z","url":"https://aljazbozic.github.io/transformerfusion/bozic_2021_transformerfusion.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Božicˇ et al. - TransformerFusion Monocular RGB Scene Reconstruct.pdf","md5":"44767dc343f5f3ec66d84c1d60c35047","mtime":1625672300000,"tags":[],"relations":{},"dateAdded":"2021-07-07T15:38:18Z","dateModified":"2021-07-07T15:38:20Z"}},{"key":"LFSDSB8N","version":384,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LFSDSB8N","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LFSDSB8N","type":"text/html"}},"meta":{"creatorSummary":"Rochow et al.","parsedDate":"2021-06-24","numChildren":0},"data":{"key":"LFSDSB8N","version":384,"itemType":"journalArticle","title":"FaDIV-Syn: Fast Depth-Independent View Synthesis","creators":[{"creatorType":"author","firstName":"Andre","lastName":"Rochow"},{"creatorType":"author","firstName":"Max","lastName":"Schwarz"},{"creatorType":"author","firstName":"Michael","lastName":"Weinmann"},{"creatorType":"author","firstName":"Sven","lastName":"Behnke"}],"abstractNote":"We introduce FaDIV-Syn, a fast depth-independent view synthesis method. Our multi-view approach addresses the problem that view synthesis methods are often limited by their depth estimation stage, where incorrect depth predictions can lead to large projection errors. To avoid this issue, we efficiently warp multiple input images into the target frame for a range of assumed depth planes. The resulting tensor representation is fed into a U-Net-like CNN with gated convolutions, which directly produces the novel output view. We therefore side-step explicit depth estimation. This improves efficiency and performance on transparent, reflective, and feature-less scene parts. FaDIV-Syn can handle both interpolation and extrapolation tasks and outperforms state-of-the-art extrapolation methods on the large-scale RealEstate10k dataset. In contrast to comparable methods, it is capable of real-time operation due to its lightweight architecture. We further demonstrate data efficiency of FaDIV-Syn by training from fewer examples as well as its generalization to higher resolutions and arbitrary depth ranges under severe depth discretization.","publicationTitle":"arXiv:2106.13139 [cs]","volume":"","issue":"","pages":"","date":"2021-06-24","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"FaDIV-Syn","url":"http://arxiv.org/abs/2106.13139","accessDate":"2021-07-06T08:24:41Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.13139","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-07-06T08:24:44Z","dateModified":"2021-07-06T08:24:46Z"}},{"key":"IHITAGKA","version":653,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IHITAGKA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IHITAGKA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VGQBFZXW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"IHITAGKA","version":653,"parentItem":"VGQBFZXW","itemType":"attachment","linkMode":"imported_url","title":"Richardson et al. - Encoding in Style A StyleGAN Encoder for Image-to.pdf","accessDate":"2021-07-06T03:38:34Z","url":"https://openaccess.thecvf.com/content/CVPR2021/papers/Richardson_Encoding_in_Style_A_StyleGAN_Encoder_for_Image-to-Image_Translation_CVPR_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Richardson et al. - Encoding in Style A StyleGAN Encoder for Image-to.pdf","md5":"5cf223f60809646f512b8ec60578440d","mtime":1625542719000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/NRY6JRTQ"},"dateAdded":"2021-07-06T03:38:34Z","dateModified":"2021-07-06T03:38:39Z"}},{"key":"VGQBFZXW","version":1183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VGQBFZXW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VGQBFZXW","type":"text/html"}},"meta":{"creatorSummary":"Richardson et al.","numChildren":4},"data":{"key":"VGQBFZXW","version":1183,"itemType":"journalArticle","title":"Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation","creators":[{"creatorType":"author","firstName":"Elad","lastName":"Richardson"},{"creatorType":"author","firstName":"Yuval","lastName":"Alaluf"},{"creatorType":"author","firstName":"Or","lastName":"Patashnik"},{"creatorType":"author","firstName":"Yotam","lastName":"Nitzan"},{"creatorType":"author","firstName":"Yaniv","lastName":"Azar"},{"creatorType":"author","firstName":"Stav","lastName":"Shapiro"},{"creatorType":"author","firstName":"Daniel","lastName":"Cohen-Or"}],"abstractNote":"","publicationTitle":"","volume":"","issue":"","pages":"10","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["F2HZKCVK"],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/6Z68JVAL","dc:replaces":"http://zotero.org/users/7902311/items/DQZYNMYJ"},"dateAdded":"2021-07-06T03:38:38Z","dateModified":"2022-06-13T16:19:49Z"}},{"key":"L5DPV57A","version":369,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/L5DPV57A","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/L5DPV57A","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y2QKW3AU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"L5DPV57A","version":369,"parentItem":"Y2QKW3AU","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-07-05T13:22:17Z","url":"https://arxiv.org/abs/2103.04379","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"1cbb04c23f736e35145608e8eee90f74","mtime":1625491337000,"tags":[],"relations":{},"dateAdded":"2021-07-05T13:22:17Z","dateModified":"2021-07-05T13:22:17Z"}},{"key":"ZLL94GJT","version":369,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZLL94GJT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZLL94GJT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y2QKW3AU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZLL94GJT","version":369,"parentItem":"Y2QKW3AU","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-07-05T13:22:13Z","url":"https://arxiv.org/pdf/2103.04379.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tritrong et al. - 2021 - Repurposing GANs for One-shot Semantic Part Segmen.pdf","md5":"b2c62d927be1d0c3b463dbdab34adcbf","mtime":1625491333000,"tags":[],"relations":{},"dateAdded":"2021-07-05T13:22:13Z","dateModified":"2021-07-05T13:22:13Z"}},{"key":"Y2QKW3AU","version":367,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y2QKW3AU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y2QKW3AU","type":"text/html"}},"meta":{"creatorSummary":"Tritrong et al.","parsedDate":"2021-04-12","numChildren":3},"data":{"key":"Y2QKW3AU","version":367,"itemType":"journalArticle","title":"Repurposing GANs for One-shot Semantic Part Segmentation","creators":[{"creatorType":"author","firstName":"Nontawat","lastName":"Tritrong"},{"creatorType":"author","firstName":"Pitchaporn","lastName":"Rewatbowornwong"},{"creatorType":"author","firstName":"Supasorn","lastName":"Suwajanakorn"}],"abstractNote":"While GANs have shown success in realistic image generation, the idea of using GANs for other tasks unrelated to synthesis is underexplored. Do GANs learn meaningful structural parts of objects during their attempt to reproduce those objects? In this work, we test this hypothesis and propose a simple and effective approach based on GANs for semantic part segmentation that requires as few as one label example along with an unlabeled dataset. Our key idea is to leverage a trained GAN to extract pixel-wise representation from the input image and use it as feature vectors for a segmentation network. Our experiments demonstrate that GANs representation is \"readily discriminative\" and produces surprisingly good results that are comparable to those from supervised baselines trained with significantly more labels. We believe this novel repurposing of GANs underlies a new class of unsupervised representation learning that is applicable to many other tasks. More results are available at https://repurposegans.github.io/.","publicationTitle":"arXiv:2103.04379 [cs]","volume":"","issue":"","pages":"","date":"2021-04-12","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2103.04379","accessDate":"2021-07-05T13:21:22Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"All rights reserved","extra":"arXiv: 2103.04379","inPublications":true,"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-07-05T13:21:22Z","dateModified":"2021-07-05T13:21:23Z"}},{"key":"9X6ZY6MT","version":367,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9X6ZY6MT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9X6ZY6MT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y2QKW3AU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9X6ZY6MT","version":367,"parentItem":"Y2QKW3AU","itemType":"note","note":"Comment: CVPR 2021 (Oral)","tags":[],"relations":{},"dateAdded":"2021-07-05T13:21:22Z","dateModified":"2021-07-05T13:21:22Z"}},{"key":"U7S4W674","version":383,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U7S4W674","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U7S4W674","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/D525LI7L","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"U7S4W674","version":383,"parentItem":"D525LI7L","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-07-05T13:19:16Z","url":"https://arxiv.org/abs/2106.01970","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"a28179c6e27b597549560ad3b5e196b0","mtime":1625491156000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/2JTLMC42"},"dateAdded":"2021-07-05T13:19:16Z","dateModified":"2021-07-05T13:19:16Z"}},{"key":"I3IHSPHI","version":383,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I3IHSPHI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I3IHSPHI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/D525LI7L","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"I3IHSPHI","version":383,"parentItem":"D525LI7L","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-07-05T13:18:55Z","url":"https://arxiv.org/pdf/2106.01970.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhang et al. - 2021 - NeRFactor Neural Factorization of Shape and Refle.pdf","md5":"b388b84e5dfd68adae7d485b1c948adf","mtime":1625491135000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/3BTLV3EI"},"dateAdded":"2021-07-05T13:18:55Z","dateModified":"2021-07-05T13:18:55Z"}},{"key":"NL38E4KS","version":383,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NL38E4KS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NL38E4KS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/D525LI7L","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NL38E4KS","version":383,"parentItem":"D525LI7L","itemType":"note","note":"Comment: Project Page: https://people.csail.mit.edu/xiuming/projects/nerfactor/","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/87ULPA88"},"dateAdded":"2021-07-05T13:17:37Z","dateModified":"2021-07-05T13:17:37Z"}},{"key":"6BVB7DKP","version":356,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6BVB7DKP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6BVB7DKP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HFJ4ZVXS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6BVB7DKP","version":356,"parentItem":"HFJ4ZVXS","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-07-01T09:40:45Z","url":"https://arxiv.org/abs/2004.15021","note":"","contentType":"text/html","charset":"utf-8","filename":"2004.html","md5":"decbb1c02d603cc46f6198413b001207","mtime":1625132445000,"tags":[],"relations":{},"dateAdded":"2021-07-01T09:40:45Z","dateModified":"2021-07-01T09:40:45Z"}},{"key":"QRAQQS2L","version":356,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QRAQQS2L","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QRAQQS2L","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HFJ4ZVXS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QRAQQS2L","version":356,"parentItem":"HFJ4ZVXS","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-07-01T09:40:40Z","url":"https://arxiv.org/pdf/2004.15021.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Luo et al. - 2020 - Consistent Video Depth Estimation.pdf","md5":"8759b943ddfa025281e25e716354a97d","mtime":1625132440000,"tags":[],"relations":{},"dateAdded":"2021-07-01T09:40:40Z","dateModified":"2021-07-01T09:40:40Z"}},{"key":"HFJ4ZVXS","version":354,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HFJ4ZVXS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HFJ4ZVXS","type":"text/html"}},"meta":{"creatorSummary":"Luo et al.","parsedDate":"2020-08-26","numChildren":3},"data":{"key":"HFJ4ZVXS","version":354,"itemType":"journalArticle","title":"Consistent Video Depth Estimation","creators":[{"creatorType":"author","firstName":"Xuan","lastName":"Luo"},{"creatorType":"author","firstName":"Jia-Bin","lastName":"Huang"},{"creatorType":"author","firstName":"Richard","lastName":"Szeliski"},{"creatorType":"author","firstName":"Kevin","lastName":"Matzen"},{"creatorType":"author","firstName":"Johannes","lastName":"Kopf"}],"abstractNote":"We present an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. We leverage a conventional structure-from-motion reconstruction to establish geometric constraints on pixels in the video. Unlike the ad-hoc priors in classical reconstruction, we use a learning-based prior, i.e., a convolutional neural network trained for single-image depth estimation. At test time, we fine-tune this network to satisfy the geometric constraints of a particular input video, while retaining its ability to synthesize plausible depth details in parts of the video that are less constrained. We show through quantitative validation that our method achieves higher accuracy and a higher degree of geometric consistency than previous monocular reconstruction methods. Visually, our results appear more stable. Our algorithm is able to handle challenging hand-held captured input videos with a moderate degree of dynamic motion. The improved quality of the reconstruction enables several applications, such as scene reconstruction and advanced video-based visual effects.","publicationTitle":"arXiv:2004.15021 [cs]","volume":"","issue":"","pages":"","date":"2020-08-26","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2004.15021","accessDate":"2021-07-01T09:38:55Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2004.15021","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2021-07-01T09:38:55Z","dateModified":"2021-07-01T09:39:00Z"}},{"key":"U7KJSRDF","version":354,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U7KJSRDF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U7KJSRDF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HFJ4ZVXS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"U7KJSRDF","version":354,"parentItem":"HFJ4ZVXS","itemType":"note","note":"Comment: SIGGRAPH 2020. Video: https://www.youtube.com/watch?v=5Tia2oblJAg Project page: https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/ Code: https://github.com/facebookresearch/consistent_depth","tags":[],"relations":{},"dateAdded":"2021-07-01T09:38:55Z","dateModified":"2021-07-01T09:38:55Z"}},{"key":"QCEPFTDM","version":348,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QCEPFTDM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QCEPFTDM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/SMZZ7DTF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QCEPFTDM","version":348,"parentItem":"SMZZ7DTF","itemType":"attachment","linkMode":"imported_url","title":"Wetzstein - EE 367  CS 448I Computational Imaging and Display.pdf","accessDate":"2021-06-29T13:33:24Z","url":"https://stanford.edu/class/ee367/reading/lecture6_notes.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wetzstein - EE 367  CS 448I Computational Imaging and Display.pdf","md5":"f09dd68b19b247181c7c739f3beac2bc","mtime":1624973607000,"tags":[],"relations":{},"dateAdded":"2021-06-29T13:33:24Z","dateModified":"2021-06-29T13:33:27Z"}},{"key":"SMZZ7DTF","version":348,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SMZZ7DTF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SMZZ7DTF","type":"text/html"}},"meta":{"creatorSummary":"Wetzstein","numChildren":1},"data":{"key":"SMZZ7DTF","version":348,"itemType":"journalArticle","title":"EE 367 / CS 448I Computational Imaging and Display Notes: Image Deconvolution (lecture 6)","creators":[{"creatorType":"author","firstName":"Gordon","lastName":"Wetzstein"}],"abstractNote":"","publicationTitle":"","volume":"","issue":"","pages":"8","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-29T13:33:26Z","dateModified":"2021-06-29T13:33:26Z"}},{"key":"4HWNANVR","version":345,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4HWNANVR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4HWNANVR","type":"text/html"}},"meta":{"numChildren":0},"data":{"key":"4HWNANVR","version":345,"itemType":"attachment","linkMode":"imported_url","title":"book-fall-07.pdf","accessDate":"2021-06-29T13:14:36Z","url":"https://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf","note":"","contentType":"application/pdf","charset":"","filename":"book-fall-07.pdf","md5":"efdb368e3544db43d33d1bd8c6befd09","mtime":1624972476000,"tags":[],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-29T13:14:36Z","dateModified":"2021-06-29T13:14:36Z"}},{"key":"R38Y95JI","version":341,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/R38Y95JI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/R38Y95JI","type":"text/html"}},"meta":{"numChildren":1},"data":{"key":"R38Y95JI","version":341,"itemType":"webpage","title":"Properties of Fourier Transform","creators":[],"abstractNote":"","websiteTitle":"","websiteType":"","date":"","shortTitle":"","url":"http://fourier.eng.hmc.edu/e101/lectures/handout3/node2.html","accessDate":"2021-06-29T12:39:13Z","language":"","rights":"","extra":"","tags":[],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-29T12:39:13Z","dateModified":"2021-06-29T12:39:15Z"}},{"key":"EDYNMIZ5","version":342,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EDYNMIZ5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EDYNMIZ5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/R38Y95JI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EDYNMIZ5","version":342,"parentItem":"R38Y95JI","itemType":"attachment","linkMode":"imported_url","title":"Properties of Fourier Transform","accessDate":"2021-06-29T12:39:14Z","url":"http://fourier.eng.hmc.edu/e101/lectures/handout3/node2.html","note":"","contentType":"text/html","charset":"utf-8","filename":"node2.html","md5":"5a28b4575cacf4a5c5400473026e9391","mtime":1624970354000,"tags":[],"relations":{},"dateAdded":"2021-06-29T12:39:14Z","dateModified":"2021-06-29T12:39:14Z"}},{"key":"KNW7FTLL","version":339,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KNW7FTLL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KNW7FTLL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IPMUGLKE","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KNW7FTLL","version":339,"parentItem":"IPMUGLKE","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-29T08:35:02Z","url":"https://arxiv.org/abs/2011.15126","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"e52e86bce500da8d6309cd4b6f8cc7d1","mtime":1624955702000,"tags":[],"relations":{},"dateAdded":"2021-06-29T08:35:02Z","dateModified":"2021-06-29T08:35:02Z"}},{"key":"PU72P4S7","version":339,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PU72P4S7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PU72P4S7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IPMUGLKE","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PU72P4S7","version":339,"parentItem":"IPMUGLKE","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-29T08:34:58Z","url":"https://arxiv.org/pdf/2011.15126.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wang et al. - 2021 - One-Shot Free-View Neural Talking-Head Synthesis f.pdf","md5":"bc22d03a6295e55d4e6ce75f2df9be82","mtime":1624955698000,"tags":[],"relations":{},"dateAdded":"2021-06-29T08:34:58Z","dateModified":"2021-06-29T08:34:58Z"}},{"key":"WC766PHJ","version":337,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WC766PHJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WC766PHJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IPMUGLKE","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WC766PHJ","version":337,"parentItem":"IPMUGLKE","itemType":"note","note":"Comment: CVPR 2021 camera ready (oral). Our project page can be found at https://nvlabs.github.io/face-vid2vid","tags":[],"relations":{},"dateAdded":"2021-06-29T08:34:13Z","dateModified":"2021-06-29T08:34:13Z"}},{"key":"2PMNRDPV","version":335,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2PMNRDPV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2PMNRDPV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/J2N3Z7QS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2PMNRDPV","version":335,"parentItem":"J2N3Z7QS","itemType":"attachment","linkMode":"imported_url","title":"face-vid2vid","accessDate":"2021-06-29T08:22:46Z","url":"https://nvlabs.github.io/face-vid2vid/","note":"","contentType":"text/html","charset":"utf-8","filename":"face-vid2vid.html","md5":"11343b2844985ea2d18e418a4955c699","mtime":1624954966000,"tags":[],"relations":{},"dateAdded":"2021-06-29T08:22:46Z","dateModified":"2021-06-29T08:22:46Z"}},{"key":"J2N3Z7QS","version":334,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/J2N3Z7QS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/J2N3Z7QS","type":"text/html"}},"meta":{"numChildren":1},"data":{"key":"J2N3Z7QS","version":334,"itemType":"webpage","title":"face-vid2vid","creators":[],"abstractNote":"","websiteTitle":"","websiteType":"","date":"","shortTitle":"","url":"https://nvlabs.github.io/face-vid2vid/","accessDate":"2021-06-29T08:22:37Z","language":"","rights":"","extra":"","tags":[],"collections":["F2HZKCVK"],"relations":{},"dateAdded":"2021-06-29T08:22:37Z","dateModified":"2021-06-29T08:22:37Z"}},{"key":"HXWFR4FM","version":332,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HXWFR4FM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HXWFR4FM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2RP3YE93","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HXWFR4FM","version":332,"parentItem":"2RP3YE93","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-29T05:16:42Z","url":"https://arxiv.org/abs/1912.05566","note":"","contentType":"text/html","charset":"utf-8","filename":"1912.html","md5":"1fd41b7c9a340da0f89eb5357657a685","mtime":1624943802000,"tags":[],"relations":{},"dateAdded":"2021-06-29T05:16:42Z","dateModified":"2021-06-29T05:16:42Z"}},{"key":"DBYFMXPX","version":331,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DBYFMXPX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DBYFMXPX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2RP3YE93","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DBYFMXPX","version":331,"parentItem":"2RP3YE93","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-29T05:16:22Z","url":"https://arxiv.org/pdf/1912.05566.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Thies et al. - 2020 - Neural Voice Puppetry Audio-driven Facial Reenact.pdf","md5":"a4dfadf035d4bb1e4eda3e1b584501de","mtime":1624943782000,"tags":[],"relations":{},"dateAdded":"2021-06-29T05:16:22Z","dateModified":"2021-06-29T05:16:22Z"}},{"key":"CYWBYTZK","version":331,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CYWBYTZK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CYWBYTZK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/43SSUSDX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CYWBYTZK","version":331,"parentItem":"43SSUSDX","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-29T05:16:12Z","url":"https://arxiv.org/abs/1906.01524","note":"","contentType":"text/html","charset":"utf-8","filename":"1906.html","md5":"8ba29cd7051f75e1be8e9c300d81f1bb","mtime":1624943772000,"tags":[],"relations":{},"dateAdded":"2021-06-29T05:16:12Z","dateModified":"2021-06-29T05:16:12Z"}},{"key":"9R86ZQ94","version":331,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9R86ZQ94","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9R86ZQ94","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/43SSUSDX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9R86ZQ94","version":331,"parentItem":"43SSUSDX","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-29T05:16:00Z","url":"https://arxiv.org/pdf/1906.01524.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Fried et al. - 2019 - Text-based Editing of Talking-head Video.pdf","md5":"31b604fb64aee2849327155a7e276db7","mtime":1624943760000,"tags":[],"relations":{},"dateAdded":"2021-06-29T05:16:00Z","dateModified":"2021-06-29T05:16:00Z"}},{"key":"3UMTVFE3","version":329,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3UMTVFE3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3UMTVFE3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2RP3YE93","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3UMTVFE3","version":329,"parentItem":"2RP3YE93","itemType":"note","note":"Comment: Video: https://youtu.be/s74_yQiJMXA Project/Demo/Code: https://justusthies.github.io/posts/neural-voice-puppetry/","tags":[],"relations":{},"dateAdded":"2021-06-29T05:15:03Z","dateModified":"2021-06-29T05:15:03Z"}},{"key":"2RP3YE93","version":329,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2RP3YE93","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2RP3YE93","type":"text/html"}},"meta":{"creatorSummary":"Thies et al.","parsedDate":"2020-07-29","numChildren":3},"data":{"key":"2RP3YE93","version":329,"itemType":"journalArticle","title":"Neural Voice Puppetry: Audio-driven Facial Reenactment","creators":[{"creatorType":"author","firstName":"Justus","lastName":"Thies"},{"creatorType":"author","firstName":"Mohamed","lastName":"Elgharib"},{"creatorType":"author","firstName":"Ayush","lastName":"Tewari"},{"creatorType":"author","firstName":"Christian","lastName":"Theobalt"},{"creatorType":"author","firstName":"Matthias","lastName":"Nießner"}],"abstractNote":"We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis. Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input. This audio-driven facial reenactment is driven by a deep neural network that employs a latent 3D face model space. Through the underlying 3D representation, the model inherently learns temporal stability while we leverage neural rendering to generate photo-realistic output frames. Our approach generalizes across different people, allowing us to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. Neural Voice Puppetry has a variety of use-cases, including audio-driven video avatars, video dubbing, and text-driven video synthesis of a talking head. We demonstrate the capabilities of our method in a series of audio- and text-based puppetry examples, including comparisons to state-of-the-art techniques and a user study.","publicationTitle":"arXiv:1912.05566 [cs]","volume":"","issue":"","pages":"","date":"2020-07-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Neural Voice Puppetry","url":"http://arxiv.org/abs/1912.05566","accessDate":"2021-06-29T05:15:03Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1912.05566","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":["F2HZKCVK"],"relations":{},"dateAdded":"2021-06-29T05:15:03Z","dateModified":"2021-06-29T05:15:03Z"}},{"key":"43SSUSDX","version":329,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/43SSUSDX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/43SSUSDX","type":"text/html"}},"meta":{"creatorSummary":"Fried et al.","parsedDate":"2019-06-04","numChildren":3},"data":{"key":"43SSUSDX","version":329,"itemType":"journalArticle","title":"Text-based Editing of Talking-head Video","creators":[{"creatorType":"author","firstName":"Ohad","lastName":"Fried"},{"creatorType":"author","firstName":"Ayush","lastName":"Tewari"},{"creatorType":"author","firstName":"Michael","lastName":"Zollhöfer"},{"creatorType":"author","firstName":"Adam","lastName":"Finkelstein"},{"creatorType":"author","firstName":"Eli","lastName":"Shechtman"},{"creatorType":"author","firstName":"Dan B.","lastName":"Goldman"},{"creatorType":"author","firstName":"Kyle","lastName":"Genova"},{"creatorType":"author","firstName":"Zeyu","lastName":"Jin"},{"creatorType":"author","firstName":"Christian","lastName":"Theobalt"},{"creatorType":"author","firstName":"Maneesh","lastName":"Agrawala"}],"abstractNote":"Editing talking-head video to change the speech content or to remove filler words is challenging. We propose a novel method to edit talking-head video based on its transcript to produce a realistic output video in which the dialogue of the speaker has been modified, while maintaining a seamless audio-visual flow (i.e. no jump cuts). Our method automatically annotates an input talking-head video with phonemes, visemes, 3D face pose and geometry, reflectance, expression and scene illumination per frame. To edit a video, the user has to only edit the transcript, and an optimization strategy then chooses segments of the input corpus as base material. The annotated parameters corresponding to the selected segments are seamlessly stitched together and used to produce an intermediate video representation in which the lower half of the face is rendered with a parametric face model. Finally, a recurrent video generation network transforms this representation to a photorealistic video that matches the edited transcript. We demonstrate a large variety of edits, such as the addition, removal, and alteration of words, as well as convincing language translation and full sentence synthesis.","publicationTitle":"arXiv:1906.01524 [cs]","volume":"","issue":"","pages":"","date":"2019-06-04","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1906.01524","accessDate":"2021-06-29T05:14:55Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1906.01524","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":["F2HZKCVK"],"relations":{},"dateAdded":"2021-06-29T05:14:55Z","dateModified":"2021-06-29T05:14:57Z"}},{"key":"QFX5KRBT","version":329,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QFX5KRBT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QFX5KRBT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/43SSUSDX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QFX5KRBT","version":329,"parentItem":"43SSUSDX","itemType":"note","note":"Comment: A version with higher resolution images can be downloaded from the authors' website","tags":[],"relations":{},"dateAdded":"2021-06-29T05:14:55Z","dateModified":"2021-06-29T05:14:55Z"}},{"key":"64KR9MND","version":328,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/64KR9MND","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/64KR9MND","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VIJCJGDN","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"64KR9MND","version":328,"parentItem":"VIJCJGDN","itemType":"attachment","linkMode":"imported_url","title":"Chan and Koo - 2008 - AN INTRODUCTION TO SYNTHETIC APERTURE RADAR (SAR).pdf","accessDate":"2021-06-28T17:58:49Z","url":"https://www.jpier.org/PIERB/pierb02/03.07110101.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chan and Koo - 2008 - AN INTRODUCTION TO SYNTHETIC APERTURE RADAR (SAR).pdf","md5":"1d8071e1dc7d266bc3a5797af90de918","mtime":1624903136000,"tags":[],"relations":{},"dateAdded":"2021-06-28T17:58:49Z","dateModified":"2021-06-28T17:58:57Z"}},{"key":"VIJCJGDN","version":327,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VIJCJGDN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VIJCJGDN","type":"text/html"}},"meta":{"creatorSummary":"Chan and Koo","parsedDate":"2008","numChildren":1},"data":{"key":"VIJCJGDN","version":327,"itemType":"journalArticle","title":"AN INTRODUCTION TO SYNTHETIC APERTURE RADAR (SAR)","creators":[{"creatorType":"author","firstName":"Yee Kit","lastName":"Chan"},{"creatorType":"author","firstName":"Voon Chet","lastName":"Koo"}],"abstractNote":"This paper outlines basic principle of Synthetic Aperture Radar (SAR). Matched ﬁlter approaches for processing the received data and pulse compression technique are presented. Besides the SAR radar equation, the linear frequency modulation (LFM) waveform and matched ﬁlter response are also discussed. Finally the system design consideration of various parameters and aspects are also highlighted.","publicationTitle":"Progress In Electromagnetics Research B","volume":"2","issue":"","pages":"27-60","date":"2008","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"PIER B","language":"en","DOI":"10.2528/PIERB07110101","ISSN":"1937-6472","shortTitle":"","url":"http://www.jpier.org/PIERB/pier.php?paper=07110101","accessDate":"2021-06-28T17:58:55Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-28T17:58:55Z","dateModified":"2021-06-28T17:58:55Z"}},{"key":"2VEMD2IM","version":325,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2VEMD2IM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2VEMD2IM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UYE7ST24","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2VEMD2IM","version":325,"parentItem":"UYE7ST24","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-28T16:41:46Z","url":"https://arxiv.org/abs/1711.10925","note":"","contentType":"text/html","charset":"utf-8","filename":"1711.html","md5":"7cc633c75460b1e5f09a50e3ae98d4e7","mtime":1624898506000,"tags":[],"relations":{},"dateAdded":"2021-06-28T16:41:46Z","dateModified":"2021-06-28T16:41:46Z"}},{"key":"G69MVIDJ","version":325,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/G69MVIDJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/G69MVIDJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UYE7ST24","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"G69MVIDJ","version":325,"parentItem":"UYE7ST24","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-28T16:41:33Z","url":"https://arxiv.org/pdf/1711.10925.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Ulyanov et al. - 2020 - Deep Image Prior.pdf","md5":"1e7414efa4f6ab2086ebb67cd2effb9c","mtime":1624898493000,"tags":[],"relations":{},"dateAdded":"2021-06-28T16:41:33Z","dateModified":"2021-06-28T16:41:33Z"}},{"key":"UYE7ST24","version":323,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UYE7ST24","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UYE7ST24","type":"text/html"}},"meta":{"creatorSummary":"Ulyanov et al.","parsedDate":"2020","numChildren":2},"data":{"key":"UYE7ST24","version":323,"itemType":"journalArticle","title":"Deep Image Prior","creators":[{"creatorType":"author","firstName":"Dmitry","lastName":"Ulyanov"},{"creatorType":"author","firstName":"Andrea","lastName":"Vedaldi"},{"creatorType":"author","firstName":"Victor","lastName":"Lempitsky"}],"abstractNote":"Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep_image_prior .","publicationTitle":"International Journal of Computer Vision","volume":"128","issue":"7","pages":"1867-1888","date":"07/2020","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"Int J Comput Vis","language":"","DOI":"10.1007/s11263-020-01303-4","ISSN":"0920-5691, 1573-1405","shortTitle":"","url":"http://arxiv.org/abs/1711.10925","accessDate":"2021-06-28T16:40:33Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1711.10925","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2021-06-28T16:40:33Z","dateModified":"2021-06-28T16:40:34Z"}},{"key":"6AEH2JJ9","version":321,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6AEH2JJ9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6AEH2JJ9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UUTNYPIJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6AEH2JJ9","version":321,"parentItem":"UUTNYPIJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-28T16:29:53Z","url":"https://arxiv.org/abs/1905.06723","note":"","contentType":"text/html","charset":"utf-8","filename":"1905.html","md5":"c3834aa8cb211adaa7e207ffeb4517c5","mtime":1624897793000,"tags":[],"relations":{},"dateAdded":"2021-06-28T16:29:53Z","dateModified":"2021-06-28T16:29:53Z"}},{"key":"IEZXPQBY","version":321,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IEZXPQBY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IEZXPQBY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y7XUZZP9","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"IEZXPQBY","version":321,"parentItem":"Y7XUZZP9","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-28T16:29:44Z","url":"https://arxiv.org/abs/2005.03991","note":"","contentType":"text/html","charset":"utf-8","filename":"2005.html","md5":"90b525e0c29ed5469c3a2fcb83e56123","mtime":1624897784000,"tags":[],"relations":{},"dateAdded":"2021-06-28T16:29:44Z","dateModified":"2021-06-28T16:29:44Z"}},{"key":"Z3LI4HXY","version":321,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z3LI4HXY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z3LI4HXY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UUTNYPIJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Z3LI4HXY","version":321,"parentItem":"UUTNYPIJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-28T16:29:41Z","url":"https://arxiv.org/pdf/1905.06723.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wu et al. - 2019 - Deep Compressed Sensing.pdf","md5":"7a07e819ba92ba2c0d2b2cc0333d7904","mtime":1624897781000,"tags":[],"relations":{},"dateAdded":"2021-06-28T16:29:41Z","dateModified":"2021-06-28T16:29:41Z"}},{"key":"U37885GZ","version":321,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U37885GZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U37885GZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y7XUZZP9","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"U37885GZ","version":321,"parentItem":"Y7XUZZP9","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-28T16:29:36Z","url":"https://arxiv.org/pdf/2005.03991.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Heckel and Soltanolkotabi - 2020 - Compressive sensing with un-trained neural network.pdf","md5":"e2470c04e7ea583ec97ed0a14e29d7b7","mtime":1624897776000,"tags":[],"relations":{},"dateAdded":"2021-06-28T16:29:36Z","dateModified":"2021-06-28T16:29:36Z"}},{"key":"RR4EX3JH","version":320,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RR4EX3JH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RR4EX3JH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UUTNYPIJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RR4EX3JH","version":320,"parentItem":"UUTNYPIJ","itemType":"note","note":"Comment: ICML 2019","tags":[],"relations":{},"dateAdded":"2021-06-28T16:29:34Z","dateModified":"2021-06-28T16:29:34Z"}},{"key":"UUTNYPIJ","version":320,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UUTNYPIJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UUTNYPIJ","type":"text/html"}},"meta":{"creatorSummary":"Wu et al.","parsedDate":"2019-05-18","numChildren":3},"data":{"key":"UUTNYPIJ","version":320,"itemType":"journalArticle","title":"Deep Compressed Sensing","creators":[{"creatorType":"author","firstName":"Yan","lastName":"Wu"},{"creatorType":"author","firstName":"Mihaela","lastName":"Rosca"},{"creatorType":"author","firstName":"Timothy","lastName":"Lillicrap"}],"abstractNote":"Compressed sensing (CS) provides an elegant framework for recovering sparse signals from compressed measurements. For example, CS can exploit the structure of natural images and recover an image from only a few random measurements. CS is flexible and data efficient, but its application has been restricted by the strong assumption of sparsity and costly reconstruction process. A recent approach that combines CS with neural network generators has removed the constraint of sparsity, but reconstruction remains slow. Here we propose a novel framework that significantly improves both the performance and speed of signal recovery by jointly training a generator and the optimisation process for reconstruction via meta-learning. We explore training the measurements with different objectives, and derive a family of models based on minimising measurement errors. We show that Generative Adversarial Nets (GANs) can be viewed as a special case in this family of models. Borrowing insights from the CS perspective, we develop a novel way of improving GANs using gradient information from the discriminator.","publicationTitle":"arXiv:1905.06723 [cs, eess, stat]","volume":"","issue":"","pages":"","date":"2019-05-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1905.06723","accessDate":"2021-06-28T16:29:34Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1905.06723","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Electrical Engineering and Systems Science - Signal Processing","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-28T16:29:34Z","dateModified":"2021-06-28T16:29:34Z"}},{"key":"P54CWT89","version":320,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/P54CWT89","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/P54CWT89","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Y7XUZZP9","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"P54CWT89","version":320,"parentItem":"Y7XUZZP9","itemType":"note","note":"Comment: arXiv admin note: text overlap with arXiv:1910.14634","tags":[],"relations":{},"dateAdded":"2021-06-28T16:29:29Z","dateModified":"2021-06-28T16:29:29Z"}},{"key":"Y7XUZZP9","version":320,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y7XUZZP9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y7XUZZP9","type":"text/html"}},"meta":{"creatorSummary":"Heckel and Soltanolkotabi","parsedDate":"2020-05-07","numChildren":3},"data":{"key":"Y7XUZZP9","version":320,"itemType":"journalArticle","title":"Compressive sensing with un-trained neural networks: Gradient descent finds the smoothest approximation","creators":[{"creatorType":"author","firstName":"Reinhard","lastName":"Heckel"},{"creatorType":"author","firstName":"Mahdi","lastName":"Soltanolkotabi"}],"abstractNote":"Un-trained convolutional neural networks have emerged as highly successful tools for image recovery and restoration. They are capable of solving standard inverse problems such as denoising and compressive sensing with excellent results by simply fitting a neural network model to measurements from a single image or signal without the need for any additional training data. For some applications, this critically requires additional regularization in the form of early stopping the optimization. For signal recovery from a few measurements, however, un-trained convolutional networks have an intriguing self-regularizing property: Even though the network can perfectly fit any image, the network recovers a natural image from few measurements when trained with gradient descent until convergence. In this paper, we provide numerical evidence for this property and study it theoretically. We show that---without any further regularization---an un-trained convolutional neural network can approximately reconstruct signals and images that are sufficiently structured, from a near minimal number of random measurements.","publicationTitle":"arXiv:2005.03991 [cs, eess, stat]","volume":"","issue":"","pages":"","date":"2020-05-07","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Compressive sensing with un-trained neural networks","url":"http://arxiv.org/abs/2005.03991","accessDate":"2021-06-28T16:29:29Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2005.03991","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-28T16:29:29Z","dateModified":"2021-06-28T16:29:29Z"}},{"key":"AEZ55M4Q","version":318,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AEZ55M4Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AEZ55M4Q","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NQ87R5AB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AEZ55M4Q","version":318,"parentItem":"NQ87R5AB","itemType":"attachment","linkMode":"imported_file","title":"Patole and Torlak - 2013 - Two Dimensional Array Imaging With Beam Steered Da.pdf","accessDate":"","url":"","note":"","contentType":"application/pdf","charset":"","filename":"Patole and Torlak - 2013 - Two Dimensional Array Imaging With Beam Steered Da.pdf","md5":"eb9e03e4c98772a671c710d001abed2e","mtime":1624890903000,"tags":[],"relations":{},"dateAdded":"2021-06-28T14:34:58Z","dateModified":"2021-06-28T14:35:03Z"}},{"key":"NQ87R5AB","version":422,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NQ87R5AB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NQ87R5AB","type":"text/html"}},"meta":{"creatorSummary":"Patole and Torlak","parsedDate":"2013","numChildren":1},"data":{"key":"NQ87R5AB","version":422,"itemType":"journalArticle","title":"Two Dimensional Array Imaging With Beam Steered Data","creators":[{"creatorType":"author","firstName":"Sujeet","lastName":"Patole"},{"creatorType":"author","firstName":"Murat","lastName":"Torlak"}],"abstractNote":"This paper discusses different approaches used for millimeter wave imaging of two-dimensional objects. Imaging of a two dimensional object requires reﬂected wave data to be collected across two distinct dimensions. In this paper, we propose a reconstruction method that uses narrowband waveforms along with two dimensional beam steering. The beam is steered in azimuthal and elevation direction, which forms the two distinct dimensions required for the reconstruction. The Reconstruction technique uses inverse Fourier transform along with amplitude and phase correction factors. In addition, this reconstruction technique does not require interpolation of the data in either wavenumber or spatial domain. Use of the two dimensional beam steering offers better performance in the presence of noise compared with the existing methods, such as switched array imaging system. Effects of RF impairments such as quantization of the phase of beam steering weights and timing jitter which add to phase noise, are analyzed.","publicationTitle":"IEEE Transactions on Image Processing","volume":"22","issue":"12","pages":"5181-5189","date":"12/2013","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"IEEE Trans. on Image Process.","language":"en","DOI":"10.1109/TIP.2013.2282115","ISSN":"1057-7149, 1941-0042","shortTitle":"","url":"http://ieeexplore.ieee.org/document/6600891/","accessDate":"2021-06-28T14:35:02Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[{"tag":"psf"}],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-28T14:35:02Z","dateModified":"2021-06-28T14:35:02Z"}},{"key":"ZX7SEQEG","version":316,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZX7SEQEG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZX7SEQEG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5KJ7MVJ5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZX7SEQEG","version":316,"parentItem":"5KJ7MVJ5","itemType":"note","note":"<p>matched filter</p>","tags":[],"relations":{},"dateAdded":"2021-06-28T14:33:07Z","dateModified":"2021-06-28T14:33:12Z"}},{"key":"H428IPFC","version":314,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/H428IPFC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/H428IPFC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5KJ7MVJ5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"H428IPFC","version":314,"parentItem":"5KJ7MVJ5","itemType":"note","note":"","tags":[],"relations":{},"dateAdded":"2021-06-28T14:33:06Z","dateModified":"2021-06-28T14:33:06Z"}},{"key":"XXTMIPIC","version":310,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XXTMIPIC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XXTMIPIC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/2FY2I56Q","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XXTMIPIC","version":310,"parentItem":"2FY2I56Q","itemType":"attachment","linkMode":"imported_url","title":"Snapshot","accessDate":"2021-06-28T14:28:12Z","url":"https://en.wikipedia.org/w/index.php?title=Weyl_expansion&oldid=1008286290","note":"","contentType":"text/html","charset":"utf-8","filename":"index.html","md5":"aeba7a091b774e7032951da60ce40975","mtime":1624890492000,"tags":[],"relations":{},"dateAdded":"2021-06-28T14:28:12Z","dateModified":"2021-06-28T14:28:12Z"}},{"key":"2FY2I56Q","version":309,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2FY2I56Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2FY2I56Q","type":"text/html"}},"meta":{"parsedDate":"2021-02-22","numChildren":1},"data":{"key":"2FY2I56Q","version":309,"itemType":"encyclopediaArticle","title":"Weyl expansion","creators":[],"abstractNote":"In physics, Weyl expansion, also known as Weyl identity or angular spectrum expansion, expresses an outgoing spherical wave as a linear combination of plane waves. In Cartesian coordinate system, it can be denoted as\n\n  \n    \n      \n        \n          \n            \n              e\n              \n                −\n                j\n                \n                  k\n                  \n                    0\n                  \n                \n                r\n              \n            \n            r\n          \n        \n        =\n        \n          \n            1\n            \n              j\n              2\n              π\n            \n          \n        \n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            ∞\n          \n        \n        \n          ∫\n          \n            −\n            ∞\n          \n          \n            ∞\n          \n        \n        d\n        \n          k\n          \n            x\n          \n        \n        d\n        \n          k\n          \n            y\n          \n        \n        \n          e\n          \n            −\n            j\n            (\n            \n              k\n              \n                x\n              \n            \n            x\n            +\n            \n              k\n              \n                y\n              \n            \n            y\n            )\n          \n        \n        \n          \n            \n              e\n              \n                −\n                j\n                \n                  k\n                  \n                    z\n                  \n                \n                \n                  |\n                \n                z\n                \n                  |\n                \n              \n            \n            \n              k\n              \n                z\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {e^{-jk_{0}r}}{r}}={\\frac {1}{j2\\pi }}\\int _{-\\infty }^{\\infty }\\int _{-\\infty }^{\\infty }dk_{x}dk_{y}e^{-j(k_{x}x+k_{y}y)}{\\frac {e^{-jk_{z}|z|}}{k_{z}}}}\n  where \n  \n    \n      \n        \n          k\n          \n            x\n          \n        \n      \n    \n    {\\displaystyle k_{x}}\n  , \n  \n    \n      \n        \n          k\n          \n            y\n          \n        \n      \n    \n    {\\displaystyle k_{y}}\n   and \n  \n    \n      \n        \n          k\n          \n            z\n          \n        \n      \n    \n    {\\displaystyle k_{z}}\n   are the wavenumbers in their respective coordinate axes:\n\n  \n    \n      \n        \n          k\n          \n            0\n          \n        \n        =\n        \n          \n            \n              k\n              \n                x\n              \n              \n                2\n              \n            \n            +\n            \n              k\n              \n                y\n              \n              \n                2\n              \n            \n            +\n            \n              k\n              \n                z\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle k_{0}={\\sqrt {k_{x}^{2}+k_{y}^{2}+k_{z}^{2}}}}\n  The expansion is named after Hermann Weyl, who published it in 1919. Weyl identity is largely used to characterize the reflection and transmission of spherical waves at planar interfaces; thus, it is often used to derive the Green's functions for Helmholtz equation in layered media. The expansion also covers evanescent wave components. It is often preferred to the Sommerfeld identity when the field representation is needed to be in Cartesian coordinates.The resulting Weyl integral is commonly encountered in microwave integrated circuit analysis and electromagnetic radiation over a stratified medium; as in the case for Sommerfeld integral, it is numerically evaluated. As a result, it is used in calculation of Green's functions for method of moments for such geometries. Other uses include the descriptions of dipolar emissions near surfaces in nanophotonics, holographic inverse scattering problems, Green's functions in quantum electrodynamics and acoustic or seismic waves.","encyclopediaTitle":"Wikipedia","series":"","seriesNumber":"","volume":"","numberOfVolumes":"","edition":"","place":"","publisher":"","date":"2021-02-22T15:01:55Z","pages":"","ISBN":"","shortTitle":"","url":"https://en.wikipedia.org/w/index.php?title=Weyl_expansion&oldid=1008286290","accessDate":"2021-06-28T14:27:57Z","language":"en","archive":"","archiveLocation":"","libraryCatalog":"Wikipedia","callNumber":"","rights":"Creative Commons Attribution-ShareAlike License","extra":"Page Version ID: 1008286290","tags":[],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-28T14:27:57Z","dateModified":"2021-06-28T14:27:57Z"}},{"key":"CZREVLVP","version":303,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CZREVLVP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CZREVLVP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6E7M3H3R","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CZREVLVP","version":303,"parentItem":"6E7M3H3R","itemType":"attachment","linkMode":"imported_url","title":"Yanik and Torlak - 2019 - Near-Field MIMO-SAR Millimeter-Wave Imaging With S.pdf","accessDate":"2021-06-28T14:02:15Z","url":"https://utd-ir.tdl.org/bitstream/handle/10735.1/7207/JECS-7171-260764.95.pdf?sequence=1&isAllowed=y","note":"","contentType":"application/pdf","charset":"","filename":"Yanik and Torlak - 2019 - Near-Field MIMO-SAR Millimeter-Wave Imaging With S.pdf","md5":"b5be276b6f293e2418bc50b329e032a9","mtime":1624888938000,"tags":[],"relations":{},"dateAdded":"2021-06-28T14:02:15Z","dateModified":"2021-06-28T14:02:18Z"}},{"key":"6E7M3H3R","version":302,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6E7M3H3R","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6E7M3H3R","type":"text/html"}},"meta":{"creatorSummary":"Yanik and Torlak","parsedDate":"2019","numChildren":1},"data":{"key":"6E7M3H3R","version":302,"itemType":"journalArticle","title":"Near-Field MIMO-SAR Millimeter-Wave Imaging With Sparsely Sampled Aperture Data","creators":[{"creatorType":"author","firstName":"Muhammet Emin","lastName":"Yanik"},{"creatorType":"author","firstName":"Murat","lastName":"Torlak"}],"abstractNote":"The primary challenge of a cost-effective and low-complexity near-ﬁeld millimeter-wave (mmWave) imaging system is to achieve high resolution with a few antenna elements as possible. Multiple-input multiple-output (MIMO) radar using simultaneous operation of spatially diverse transmit and receive antennas is a good candidate to increase the number of available degrees of freedom. On the other hand, higher integration complexity of extremely dense transceiver electronics limits the use of MIMO only solutions within a relatively large imaging aperture. Hybrid concepts combining synthetic aperture radar (SAR) techniques and sparse MIMO arrays present a good compromise to achieve short data acquisition time and low complexity. However, compared with conventional monostatic sampling schemes, image reconstruction methods for MIMO-SAR are more complicated. In this paper, we propose a high-resolution mmWave imaging system combining 2-D MIMO arrays with SAR, along with a novel Fourier-based image reconstruction algorithm using sparsely sampled aperture data. The proposed algorithm is veriﬁed by both simulation and processing real data collected with our mmWave imager prototype utilizing commercially available 77-GHz MIMO radar sensors. The experimental results conﬁrm that our complete solution presents a strong potential in high-resolution imaging with a signiﬁcantly reduced number of antenna elements.","publicationTitle":"IEEE Access","volume":"7","issue":"","pages":"31801-31819","date":"2019","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"IEEE Access","language":"en","DOI":"10.1109/ACCESS.2019.2902859","ISSN":"2169-3536","shortTitle":"","url":"https://ieeexplore.ieee.org/document/8658136/","accessDate":"2021-06-28T14:02:17Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-28T14:02:17Z","dateModified":"2021-06-28T14:02:17Z"}},{"key":"5KJ7MVJ5","version":313,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5KJ7MVJ5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5KJ7MVJ5","type":"text/html"}},"meta":{"creatorSummary":"Yanik and Torlak","numChildren":3},"data":{"key":"5KJ7MVJ5","version":313,"itemType":"journalArticle","title":"Millimeter-Wave Near-Field Imaging with Two-Dimensional SAR Data","creators":[{"creatorType":"author","firstName":"Muhammet Emin","lastName":"Yanik"},{"creatorType":"author","firstName":"Murat","lastName":"Torlak"}],"abstractNote":"In this paper, we present a low-cost high-resolution millimeter-wave (mmWave) imager that combines commercially available 77 GHz system-on-chip radar sensors and synthetic aperture radar (SAR) signal processing techniques. To create a synthetic aperture over the target scene, the imager is built with two-axis motorized rail system which can synthesize a large aperture in both the azimuth and elevation. For image reconstruction, we employ two signal processing techniques based on frequency-modulated continuous-wave (FMCW) radar signal model. Our prototype system is described in detail along with various imaging results. The experimental results conﬁrm that our low-cost system design demonstrates a great potential for high-resolution imaging tasks in various applications.","publicationTitle":"","volume":"","issue":"","pages":"6","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-28T13:31:14Z","dateModified":"2021-06-28T13:31:23Z"}},{"key":"CKFJUU23","version":300,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CKFJUU23","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CKFJUU23","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5KJ7MVJ5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CKFJUU23","version":300,"parentItem":"5KJ7MVJ5","itemType":"attachment","linkMode":"imported_file","title":"Yanik and Torlak - Millimeter-Wave Near-Field Imaging with Two-Dimens.pdf","accessDate":"","url":"","note":"","contentType":"application/pdf","charset":"","filename":"Yanik and Torlak - Millimeter-Wave Near-Field Imaging with Two-Dimens.pdf","md5":"42b6d35422b5471eb5997b872b9fe997","mtime":1624887075000,"tags":[],"relations":{},"dateAdded":"2021-06-28T13:31:11Z","dateModified":"2021-06-28T13:31:15Z"}},{"key":"4PVFZZV7","version":296,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4PVFZZV7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4PVFZZV7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/T5TS9HIW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4PVFZZV7","version":296,"parentItem":"T5TS9HIW","itemType":"note","note":"<p>good model evidence tutorial</p>","tags":[],"relations":{},"dateAdded":"2021-06-28T05:13:31Z","dateModified":"2021-06-28T05:13:37Z"}},{"key":"J87GYFT3","version":297,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/J87GYFT3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/J87GYFT3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/T5TS9HIW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"J87GYFT3","version":297,"parentItem":"T5TS9HIW","itemType":"attachment","linkMode":"imported_url","title":"Bayes in the sky: Bayesian inference and model selection in cosmology - Roberto Trotta","accessDate":"2021-06-28T05:13:17Z","url":"https://ned.ipac.caltech.edu/level5/Sept13/Trotta/Trotta4.html","note":"","contentType":"text/html","charset":"utf-8","filename":"Trotta4.html","md5":"4199449fba222584708c1bb4694a9981","mtime":1624857197000,"tags":[],"relations":{},"dateAdded":"2021-06-28T05:13:17Z","dateModified":"2021-06-28T05:13:17Z"}},{"key":"T5TS9HIW","version":296,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/T5TS9HIW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/T5TS9HIW","type":"text/html"}},"meta":{"numChildren":2},"data":{"key":"T5TS9HIW","version":296,"itemType":"webpage","title":"Bayes in the sky: Bayesian inference and model selection in cosmology - Roberto Trotta","creators":[],"abstractNote":"","websiteTitle":"","websiteType":"","date":"","shortTitle":"","url":"https://ned.ipac.caltech.edu/level5/Sept13/Trotta/Trotta4.html","accessDate":"2021-06-28T05:13:16Z","language":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-28T05:13:16Z","dateModified":"2021-06-28T05:13:16Z"}},{"key":"AXY33PT3","version":651,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AXY33PT3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AXY33PT3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6BIBVQ7P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AXY33PT3","version":651,"parentItem":"6BIBVQ7P","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-28T04:27:18Z","url":"https://arxiv.org/abs/2106.00132","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"90b68eb779edde4a82e013efd1887e3a","mtime":1624854438000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/V4BCQTRN"},"dateAdded":"2021-06-28T04:27:18Z","dateModified":"2021-06-28T04:27:18Z"}},{"key":"FGSIAWJF","version":651,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FGSIAWJF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FGSIAWJF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6BIBVQ7P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FGSIAWJF","version":651,"parentItem":"6BIBVQ7P","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-28T04:26:57Z","url":"https://arxiv.org/pdf/2106.00132.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kong and Ping - 2021 - On Fast Sampling of Diffusion Probabilistic Models.pdf","md5":"a42c6e5bcda9bc1f059ec08589a540f7","mtime":1624854416000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/9YGEHTJR"},"dateAdded":"2021-06-28T04:26:57Z","dateModified":"2021-06-28T04:26:57Z"}},{"key":"RMVTJTR2","version":650,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RMVTJTR2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RMVTJTR2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6BIBVQ7P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RMVTJTR2","version":650,"parentItem":"6BIBVQ7P","itemType":"note","note":"Comment: Code is released","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/2TWAYSA8"},"dateAdded":"2021-06-28T04:26:24Z","dateModified":"2021-06-28T04:26:24Z"}},{"key":"7U7DU9A5","version":286,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7U7DU9A5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7U7DU9A5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6Q2MWQIW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7U7DU9A5","version":286,"parentItem":"6Q2MWQIW","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-26T16:33:26Z","url":"https://arxiv.org/abs/2104.14951","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"5cd66c326d5a6f936e43443ce3e3545d","mtime":1624725206000,"tags":[],"relations":{},"dateAdded":"2021-06-26T16:33:26Z","dateModified":"2021-06-26T16:33:26Z"}},{"key":"IXKKDKTB","version":286,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IXKKDKTB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IXKKDKTB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6Q2MWQIW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"IXKKDKTB","version":286,"parentItem":"6Q2MWQIW","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-26T16:33:21Z","url":"https://arxiv.org/pdf/2104.14951.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Li et al. - 2021 - SRDiff Single Image Super-Resolution with Diffusi.pdf","md5":"48fa4d32e947e182cb28ce48021e56bd","mtime":1624725201000,"tags":[],"relations":{},"dateAdded":"2021-06-26T16:33:21Z","dateModified":"2021-06-26T16:33:21Z"}},{"key":"6Q2MWQIW","version":285,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6Q2MWQIW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6Q2MWQIW","type":"text/html"}},"meta":{"creatorSummary":"Li et al.","parsedDate":"2021-05-18","numChildren":2},"data":{"key":"6Q2MWQIW","version":285,"itemType":"journalArticle","title":"SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models","creators":[{"creatorType":"author","firstName":"Haoying","lastName":"Li"},{"creatorType":"author","firstName":"Yifan","lastName":"Yang"},{"creatorType":"author","firstName":"Meng","lastName":"Chang"},{"creatorType":"author","firstName":"Huajun","lastName":"Feng"},{"creatorType":"author","firstName":"Zhihai","lastName":"Xu"},{"creatorType":"author","firstName":"Qi","lastName":"Li"},{"creatorType":"author","firstName":"Yueting","lastName":"Chen"}],"abstractNote":"Single image super-resolution (SISR) aims to reconstruct high-resolution (HR) images from the given low-resolution (LR) ones, which is an ill-posed problem because one LR image corresponds to multiple HR images. Recently, learning-based SISR methods have greatly outperformed traditional ones, while suffering from over-smoothing, mode collapse or large model footprint issues for PSNR-oriented, GAN-driven and flow-based methods respectively. To solve these problems, we propose a novel single image super-resolution diffusion probabilistic model (SRDiff), which is the first diffusion-based model for SISR. SRDiff is optimized with a variant of the variational bound on the data likelihood and can provide diverse and realistic SR predictions by gradually transforming the Gaussian noise into a super-resolution (SR) image conditioned on an LR input through a Markov chain. In addition, we introduce residual prediction to the whole framework to speed up convergence. Our extensive experiments on facial and general benchmarks (CelebA and DIV2K datasets) show that 1) SRDiff can generate diverse SR results in rich details with state-of-the-art performance, given only one LR input; 2) SRDiff is easy to train with a small footprint; and 3) SRDiff can perform flexible image manipulation including latent space interpolation and content fusion.","publicationTitle":"arXiv:2104.14951 [cs]","volume":"","issue":"","pages":"","date":"2021-05-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"SRDiff","url":"http://arxiv.org/abs/2104.14951","accessDate":"2021-06-26T16:33:13Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.14951","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["TNWL7M5C"],"relations":{},"dateAdded":"2021-06-26T16:33:13Z","dateModified":"2021-06-26T16:33:13Z"}},{"key":"5XHGNVPE","version":650,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5XHGNVPE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5XHGNVPE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HT98Y9ZU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5XHGNVPE","version":650,"parentItem":"HT98Y9ZU","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-26T16:19:42Z","url":"https://arxiv.org/abs/2011.13456","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"cab213678215ff9ea1a147c11b7f32ad","mtime":1624724382000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/A5FNVWRX"},"dateAdded":"2021-06-26T16:19:42Z","dateModified":"2021-06-26T16:19:42Z"}},{"key":"5DADQZJK","version":650,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5DADQZJK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5DADQZJK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HT98Y9ZU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5DADQZJK","version":650,"parentItem":"HT98Y9ZU","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-26T16:19:29Z","url":"https://arxiv.org/pdf/2011.13456.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf","md5":"3327d1e3196026d81e8ee11b39c49943","mtime":1624724369000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/6FLJ6CNZ"},"dateAdded":"2021-06-26T16:19:29Z","dateModified":"2021-06-26T16:19:29Z"}},{"key":"YGTQJMVX","version":650,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YGTQJMVX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YGTQJMVX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HT98Y9ZU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YGTQJMVX","version":650,"parentItem":"HT98Y9ZU","itemType":"note","note":"Comment: ICLR 2021 (Oral)","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/LGXXREBJ"},"dateAdded":"2021-06-26T16:16:48Z","dateModified":"2021-06-26T16:16:48Z"}},{"key":"FHCGNVP2","version":361,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FHCGNVP2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FHCGNVP2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/SZ3C29IB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FHCGNVP2","version":361,"parentItem":"SZ3C29IB","itemType":"attachment","linkMode":"imported_url","title":"Flynn et al. - 2019 - DeepView View Synthesis With Learned Gradient Des.pdf","accessDate":"2021-06-26T16:09:29Z","url":"https://openaccess.thecvf.com/content_CVPR_2019/papers/Flynn_DeepView_View_Synthesis_With_Learned_Gradient_Descent_CVPR_2019_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Flynn et al. - 2019 - DeepView View Synthesis With Learned Gradient Des.pdf","md5":"0597be7428b61a438b75c708a2fa662d","mtime":1624723773000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/2K7LTI4D"},"dateAdded":"2021-06-26T16:09:29Z","dateModified":"2021-06-26T16:09:33Z"}},{"key":"SZ3C29IB","version":360,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SZ3C29IB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SZ3C29IB","type":"text/html"}},"meta":{"creatorSummary":"Flynn et al.","parsedDate":"2019","numChildren":1},"data":{"key":"SZ3C29IB","version":360,"itemType":"conferencePaper","title":"DeepView: View Synthesis With Learned Gradient Descent","creators":[{"creatorType":"author","firstName":"John","lastName":"Flynn"},{"creatorType":"author","firstName":"Michael","lastName":"Broxton"},{"creatorType":"author","firstName":"Paul","lastName":"Debevec"},{"creatorType":"author","firstName":"Matthew","lastName":"DuVall"},{"creatorType":"author","firstName":"Graham","lastName":"Fyffe"},{"creatorType":"author","firstName":"Ryan","lastName":"Overbeck"},{"creatorType":"author","firstName":"Noah","lastName":"Snavely"},{"creatorType":"author","firstName":"Richard","lastName":"Tucker"}],"abstractNote":"We present a novel approach to view synthesis using multiplane images (MPIs). Building on recent advances in learned gradient descent, our algorithm generates an MPI from a set of sparse camera viewpoints. The resulting method incorporates occlusion reasoning, improving performance on challenging scene features such as object boundaries, lighting reﬂections, thin structures, and scenes with high depth complexity. We show that our method achieves high-quality, state-of-the-art results on two datasets: the Kalantari light ﬁeld dataset, and a new camera array dataset, Spaces, which we make publicly available.","date":"6/2019","proceedingsTitle":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","place":"Long Beach, CA, USA","publisher":"IEEE","volume":"","pages":"2362-2371","series":"","language":"en","DOI":"10.1109/CVPR.2019.00247","ISBN":"978-1-72813-293-8","shortTitle":"DeepView","url":"https://ieeexplore.ieee.org/document/8953705/","accessDate":"2021-06-26T16:09:33Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":["CPYKW3PF"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/VSRM4YS5"},"dateAdded":"2021-06-26T16:09:33Z","dateModified":"2021-06-26T16:09:33Z"}},{"key":"UTETBLZE","version":273,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UTETBLZE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UTETBLZE","type":"text/html"}},"meta":{"creatorSummary":"Niklasson et al.","parsedDate":"2021-02-11","numChildren":0},"data":{"key":"UTETBLZE","version":273,"itemType":"journalArticle","title":"Self-Organising Textures","creators":[{"creatorType":"author","firstName":"Eyvind","lastName":"Niklasson"},{"creatorType":"author","firstName":"Alexander","lastName":"Mordvintsev"},{"creatorType":"author","firstName":"Ettore","lastName":"Randazzo"},{"creatorType":"author","firstName":"Michael","lastName":"Levin"}],"abstractNote":"Neural Cellular Automata learn to generate textures, exhibiting surprising properties.","publicationTitle":"Distill","volume":"6","issue":"2","pages":"e00027.003","date":"2021/02/11","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"Distill","language":"en","DOI":"10.23915/distill.00027.003","ISSN":"2476-0757","shortTitle":"","url":"https://distill.pub/selforg/2021/textures","accessDate":"2021-06-26T15:58:08Z","archive":"","archiveLocation":"","libraryCatalog":"distill.pub","callNumber":"","rights":"","extra":"","tags":[],"collections":["TNWL7M5C"],"relations":{},"dateAdded":"2021-06-26T15:58:08Z","dateModified":"2021-06-26T15:58:14Z"}},{"key":"Z9E2MY3L","version":1183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z9E2MY3L","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z9E2MY3L","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4EKYAFYR","type":"application/json"}},"meta":{},"data":{"key":"Z9E2MY3L","version":1183,"parentItem":"4EKYAFYR","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-25T18:32:35Z","url":"https://arxiv.org/abs/2106.13228","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"e020b3d70e976d57c8c6f61f1be04e3c","mtime":1624645955000,"tags":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/NCMFZLXM"},"dateAdded":"2021-06-25T18:32:35Z","dateModified":"2022-06-13T16:19:40Z"}},{"key":"3QW57CHD","version":271,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3QW57CHD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3QW57CHD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4EKYAFYR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3QW57CHD","version":271,"parentItem":"4EKYAFYR","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-25T18:32:24Z","url":"https://arxiv.org/pdf/2106.13228.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Park et al. - 2021 - HyperNeRF A Higher-Dimensional Representation for.pdf","md5":"48e78973634f99c7f901e8974b319582","mtime":1624645944000,"tags":[],"relations":{},"dateAdded":"2021-06-25T18:32:24Z","dateModified":"2021-06-25T18:32:24Z"}},{"key":"4EKYAFYR","version":1183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4EKYAFYR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4EKYAFYR","type":"text/html"}},"meta":{"creatorSummary":"Park et al.","parsedDate":"2021-06-24","numChildren":5},"data":{"key":"4EKYAFYR","version":1183,"itemType":"journalArticle","title":"HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields","creators":[{"creatorType":"author","firstName":"Keunhong","lastName":"Park"},{"creatorType":"author","firstName":"Utkarsh","lastName":"Sinha"},{"creatorType":"author","firstName":"Peter","lastName":"Hedman"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Sofien","lastName":"Bouaziz"},{"creatorType":"author","firstName":"Dan B.","lastName":"Goldman"},{"creatorType":"author","firstName":"Ricardo","lastName":"Martin-Brualla"},{"creatorType":"author","firstName":"Steven M.","lastName":"Seitz"}],"abstractNote":"Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this \"hyper-space\". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between \"moments\", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks by significant margins. Compared to Nerfies, HyperNeRF reduces average error rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as measured by LPIPS.","publicationTitle":"arXiv:2106.13228 [cs]","volume":"","issue":"","pages":"","date":"2021-06-24","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"HyperNeRF","url":"http://arxiv.org/abs/2106.13228","accessDate":"2021-06-25T18:30:30Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.13228","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":["CPYKW3PF"],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/KD64JT86"},"dateAdded":"2021-06-25T18:30:30Z","dateModified":"2022-06-13T16:19:40Z"}},{"key":"3JSGU7NV","version":269,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3JSGU7NV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3JSGU7NV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4EKYAFYR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3JSGU7NV","version":269,"parentItem":"4EKYAFYR","itemType":"note","note":"Comment: Project page: https://hypernerf.github.io/","tags":[],"relations":{},"dateAdded":"2021-06-25T18:30:30Z","dateModified":"2021-06-25T18:30:30Z"}},{"key":"P45NIJ2A","version":650,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/P45NIJ2A","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/P45NIJ2A","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CYEZKAXX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"P45NIJ2A","version":650,"parentItem":"CYEZKAXX","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-25T08:14:02Z","url":"https://arxiv.org/abs/2102.09672","note":"","contentType":"text/html","charset":"utf-8","filename":"2102.html","md5":"cd0d672ba355b90da5e69b5796f9967c","mtime":1624608842000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/NHF3ZNW8","http://zotero.org/groups/4458581/items/B4T8TVNK"]},"dateAdded":"2021-06-25T08:14:02Z","dateModified":"2021-06-25T08:14:02Z"}},{"key":"J4GY55HL","version":650,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/J4GY55HL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/J4GY55HL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CYEZKAXX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"J4GY55HL","version":650,"parentItem":"CYEZKAXX","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-25T08:13:58Z","url":"https://arxiv.org/pdf/2102.09672.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf","md5":"9badaa74e386d9400a7223c7d8567af2","mtime":1624608838000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/DHGRCY4L","http://zotero.org/groups/4458581/items/57C67JFP"]},"dateAdded":"2021-06-25T08:13:58Z","dateModified":"2021-06-25T08:13:58Z"}},{"key":"CYEZKAXX","version":648,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CYEZKAXX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CYEZKAXX","type":"text/html"}},"meta":{"creatorSummary":"Nichol and Dhariwal","parsedDate":"2021-02-18","numChildren":2},"data":{"key":"CYEZKAXX","version":648,"itemType":"journalArticle","title":"Improved Denoising Diffusion Probabilistic Models","creators":[{"creatorType":"author","firstName":"Alex","lastName":"Nichol"},{"creatorType":"author","firstName":"Prafulla","lastName":"Dhariwal"}],"abstractNote":"Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion","publicationTitle":"arXiv:2102.09672 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-02-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2102.09672","accessDate":"2021-06-25T08:12:48Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2102.09672","tags":[],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/KWNUZBK2","http://zotero.org/groups/4458581/items/PPWTMREM"]},"dateAdded":"2021-06-25T08:12:49Z","dateModified":"2021-06-25T08:12:49Z"}},{"key":"VP6J65TE","version":488,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VP6J65TE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VP6J65TE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PVGU9IG5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VP6J65TE","version":488,"parentItem":"PVGU9IG5","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-24T12:53:53Z","url":"https://arxiv.org/abs/2009.00713","note":"","contentType":"text/html","charset":"utf-8","filename":"2009.html","md5":"dcb9a56c8c53fd69d932350053e3ae3b","mtime":1624539233068,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/TUFEVR6S"},"dateAdded":"2021-06-24T12:53:53Z","dateModified":"2021-06-24T12:53:53Z"}},{"key":"QR2NTT9J","version":488,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QR2NTT9J","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QR2NTT9J","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PVGU9IG5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QR2NTT9J","version":488,"parentItem":"PVGU9IG5","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-24T12:53:45Z","url":"https://arxiv.org/pdf/2009.00713.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chen et al. - 2020 - WaveGrad Estimating Gradients for Waveform Genera.pdf","md5":"e0d5333a3654254f674b2f1e3dffa20d","mtime":1624539225628,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/6TLZ93H2"},"dateAdded":"2021-06-24T12:53:45Z","dateModified":"2021-06-24T12:53:45Z"}},{"key":"PVGU9IG5","version":488,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PVGU9IG5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PVGU9IG5","type":"text/html"}},"meta":{"creatorSummary":"Chen et al.","parsedDate":"2020-10-09","numChildren":2},"data":{"key":"PVGU9IG5","version":488,"itemType":"journalArticle","title":"WaveGrad: Estimating Gradients for Waveform Generation","creators":[{"creatorType":"author","firstName":"Nanxin","lastName":"Chen"},{"creatorType":"author","firstName":"Yu","lastName":"Zhang"},{"creatorType":"author","firstName":"Heiga","lastName":"Zen"},{"creatorType":"author","firstName":"Ron J.","lastName":"Weiss"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"},{"creatorType":"author","firstName":"William","lastName":"Chan"}],"abstractNote":"This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts from a Gaussian white noise signal and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to trade inference speed for sample quality by adjusting the number of refinement steps, and bridges the gap between non-autoregressive and autoregressive models in terms of audio quality. We find that it can generate high fidelity audio samples using as few as six iterations. Experiments reveal WaveGrad to generate high fidelity audio, outperforming adversarial non-autoregressive baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations. Audio samples are available at https://wavegrad.github.io/.","publicationTitle":"arXiv:2009.00713 [cs, eess, stat]","volume":"","issue":"","pages":"","date":"2020-10-09","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"WaveGrad","url":"http://arxiv.org/abs/2009.00713","accessDate":"2021-06-24T12:53:38Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2009.00713","tags":[],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/2M8T36BS"},"dateAdded":"2021-06-24T12:53:38Z","dateModified":"2021-06-24T12:53:38Z"}},{"key":"S79BH8QE","version":649,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/S79BH8QE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/S79BH8QE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5H9I5CSF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"S79BH8QE","version":649,"parentItem":"5H9I5CSF","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:26:15Z","url":"https://arxiv.org/pdf/2006.11239.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf","md5":"a8c5d8c82b677e4324460abc053254b4","mtime":1624206375000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/RHLFG3YA","http://zotero.org/groups/4458581/items/BP26FR7W"]},"dateAdded":"2021-06-20T16:26:15Z","dateModified":"2021-06-24T12:16:59Z"}},{"key":"4BF49JYZ","version":650,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4BF49JYZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4BF49JYZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IMIGHLZD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4BF49JYZ","version":650,"parentItem":"IMIGHLZD","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-24T10:40:44Z","url":"https://arxiv.org/abs/2106.02808","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"580d0ed7659ba0ae37b737838ec2d834","mtime":1624531243000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/78QSKVRG"},"dateAdded":"2021-06-24T10:40:44Z","dateModified":"2021-06-24T10:40:44Z"}},{"key":"VI4VEXBI","version":650,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VI4VEXBI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VI4VEXBI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IMIGHLZD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VI4VEXBI","version":650,"parentItem":"IMIGHLZD","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-24T10:40:31Z","url":"https://arxiv.org/pdf/2106.02808.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Huang et al. - 2021 - A Variational Perspective on Diffusion-Based Gener.pdf","md5":"bb847c4a09febea7425a736f5b1ee246","mtime":1624531231000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/DYZH68PJ"},"dateAdded":"2021-06-24T10:40:31Z","dateModified":"2021-06-24T10:40:31Z"}},{"key":"IMIGHLZD","version":648,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IMIGHLZD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IMIGHLZD","type":"text/html"}},"meta":{"creatorSummary":"Huang et al.","parsedDate":"2021-06-05","numChildren":2},"data":{"key":"IMIGHLZD","version":648,"itemType":"journalArticle","title":"A Variational Perspective on Diffusion-Based Generative Models and Score Matching","creators":[{"creatorType":"author","firstName":"Chin-Wei","lastName":"Huang"},{"creatorType":"author","firstName":"Jae Hyun","lastName":"Lim"},{"creatorType":"author","firstName":"Aaron","lastName":"Courville"}],"abstractNote":"Discrete-time diffusion-based generative models and score matching methods have shown promising results in modeling high-dimensional image data. Recently, Song et al. (2021) show that diffusion processes that transform data into noise can be reversed via learning the score function, i.e. the gradient of the log-density of the perturbed data. They propose to plug the learned score function into an inverse formula to define a generative diffusion process. Despite the empirical success, a theoretical underpinning of this procedure is still lacking. In this work, we approach the (continuous-time) generative diffusion directly and derive a variational framework for likelihood estimation, which includes continuous-time normalizing flows as a special case, and can be seen as an infinitely deep variational autoencoder. Under this framework, we show that minimizing the score-matching loss is equivalent to maximizing a lower bound of the likelihood of the plug-in reverse SDE proposed by Song et al. (2021), bridging the theoretical gap.","publicationTitle":"arXiv:2106.02808 [cs]","volume":"","issue":"","pages":"","date":"2021-06-05","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2106.02808","accessDate":"2021-06-24T10:40:11Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.02808","tags":[],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/ZYVSXR5S"},"dateAdded":"2021-06-24T10:40:11Z","dateModified":"2021-06-24T10:40:14Z"}},{"key":"ILRZA9ZA","version":255,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ILRZA9ZA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ILRZA9ZA","type":"text/html"}},"meta":{"numChildren":1},"data":{"key":"ILRZA9ZA","version":255,"itemType":"webpage","title":"Variational Inference - Deriving the ELBO · Infinite n♾rm","creators":[],"abstractNote":"","websiteTitle":"","websiteType":"","date":"","shortTitle":"","url":"https://chrisorm.github.io/VI-ELBO.html","accessDate":"2021-06-24T10:34:47Z","language":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-24T10:34:47Z","dateModified":"2021-06-24T10:34:47Z"}},{"key":"5UVIQRPG","version":253,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5UVIQRPG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5UVIQRPG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/JRR7M9JQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5UVIQRPG","version":253,"parentItem":"JRR7M9JQ","itemType":"attachment","linkMode":"imported_url","title":"Snapshot","accessDate":"2021-06-24T10:20:18Z","url":"https://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/","note":"","contentType":"text/html","charset":"utf-8","filename":"variational-bayes-and-the-mean-field-approximation.html","md5":"bef3b456f2e88346bb70d8ff4d51923f","mtime":1624530018000,"tags":[],"relations":{},"dateAdded":"2021-06-24T10:20:18Z","dateModified":"2021-06-24T10:20:18Z"}},{"key":"JRR7M9JQ","version":252,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JRR7M9JQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JRR7M9JQ","type":"text/html"}},"meta":{"creatorSummary":"Keng","parsedDate":"2017-04-03","numChildren":1},"data":{"key":"JRR7M9JQ","version":252,"itemType":"webpage","title":"Variational Bayes and The Mean-Field Approximation","creators":[{"creatorType":"author","firstName":"Brian","lastName":"Keng"}],"abstractNote":"A brief introduction to variational Bayes and the mean-field approximation.","websiteTitle":"Bounded Rationality","websiteType":"","date":"2017-04-03T08:02:46-05:00","shortTitle":"","url":"http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/","accessDate":"2021-06-24T10:20:02Z","language":"en","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-24T10:20:02Z","dateModified":"2021-06-24T10:20:02Z"}},{"key":"N3WPMV45","version":250,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/N3WPMV45","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/N3WPMV45","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PIV5FK3P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"N3WPMV45","version":250,"parentItem":"PIV5FK3P","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-24T08:59:31Z","url":"https://arxiv.org/abs/2106.12423","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"ea3f8d6f45777c421cb036d02051a48a","mtime":1624525171000,"tags":[],"relations":{},"dateAdded":"2021-06-24T08:59:31Z","dateModified":"2021-06-24T08:59:31Z"}},{"key":"SMABT7MF","version":250,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SMABT7MF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SMABT7MF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PIV5FK3P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SMABT7MF","version":250,"parentItem":"PIV5FK3P","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-24T08:59:26Z","url":"https://arxiv.org/pdf/2106.12423.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Karras et al. - 2021 - Alias-Free Generative Adversarial Networks.pdf","md5":"93ef3611b5d9753f9ec5f41e7a9e220f","mtime":1624525166000,"tags":[],"relations":{},"dateAdded":"2021-06-24T08:59:26Z","dateModified":"2021-06-24T08:59:26Z"}},{"key":"PIV5FK3P","version":285,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PIV5FK3P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PIV5FK3P","type":"text/html"}},"meta":{"creatorSummary":"Karras et al.","parsedDate":"2021-06-23","numChildren":2},"data":{"key":"PIV5FK3P","version":285,"itemType":"journalArticle","title":"Alias-Free Generative Adversarial Networks","creators":[{"creatorType":"author","firstName":"Tero","lastName":"Karras"},{"creatorType":"author","firstName":"Miika","lastName":"Aittala"},{"creatorType":"author","firstName":"Samuli","lastName":"Laine"},{"creatorType":"author","firstName":"Erik","lastName":"Härkönen"},{"creatorType":"author","firstName":"Janne","lastName":"Hellsten"},{"creatorType":"author","firstName":"Jaakko","lastName":"Lehtinen"},{"creatorType":"author","firstName":"Timo","lastName":"Aila"}],"abstractNote":"We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.","publicationTitle":"arXiv:2106.12423 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-06-23","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2106.12423","accessDate":"2021-06-24T08:56:28Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.12423","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-24T08:56:28Z","dateModified":"2021-06-24T08:56:28Z"}},{"key":"25VTJXL8","version":246,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/25VTJXL8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/25VTJXL8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/AAHHRFU4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"25VTJXL8","version":246,"parentItem":"AAHHRFU4","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-23T17:36:37Z","url":"https://arxiv.org/abs/1905.01164","note":"","contentType":"text/html","charset":"utf-8","filename":"1905.html","md5":"84638f6574b79c81fab2853ebbe712b8","mtime":1624469797000,"tags":[],"relations":{},"dateAdded":"2021-06-23T17:36:37Z","dateModified":"2021-06-23T17:36:37Z"}},{"key":"RA5K97H8","version":246,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RA5K97H8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RA5K97H8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/AAHHRFU4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RA5K97H8","version":246,"parentItem":"AAHHRFU4","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-23T17:36:32Z","url":"https://arxiv.org/pdf/1905.01164.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Shaham et al. - 2019 - SinGAN Learning a Generative Model from a Single .pdf","md5":"ea2931f8d879c9a9de5888f8c67c68ed","mtime":1624469792000,"tags":[],"relations":{},"dateAdded":"2021-06-23T17:36:32Z","dateModified":"2021-06-23T17:36:32Z"}},{"key":"AAHHRFU4","version":285,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AAHHRFU4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AAHHRFU4","type":"text/html"}},"meta":{"creatorSummary":"Shaham et al.","parsedDate":"2019-09-04","numChildren":3},"data":{"key":"AAHHRFU4","version":285,"itemType":"journalArticle","title":"SinGAN: Learning a Generative Model from a Single Natural Image","creators":[{"creatorType":"author","firstName":"Tamar Rott","lastName":"Shaham"},{"creatorType":"author","firstName":"Tali","lastName":"Dekel"},{"creatorType":"author","firstName":"Tomer","lastName":"Michaeli"}],"abstractNote":"We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.","publicationTitle":"arXiv:1905.01164 [cs]","volume":"","issue":"","pages":"","date":"2019-09-04","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"SinGAN","url":"http://arxiv.org/abs/1905.01164","accessDate":"2021-06-23T17:35:44Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1905.01164","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-23T17:35:44Z","dateModified":"2021-06-23T17:35:44Z"}},{"key":"DRS227PG","version":244,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DRS227PG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DRS227PG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/AAHHRFU4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DRS227PG","version":244,"parentItem":"AAHHRFU4","itemType":"note","note":"Comment: ICCV 2019","tags":[],"relations":{},"dateAdded":"2021-06-23T17:35:44Z","dateModified":"2021-06-23T17:35:44Z"}},{"key":"AKRTYW6G","version":243,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AKRTYW6G","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AKRTYW6G","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/W2SDJE56","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AKRTYW6G","version":243,"parentItem":"W2SDJE56","itemType":"attachment","linkMode":"imported_file","title":"Sheen et al. - 2001 - Three-dimensional millimeter-wave imaging for conc.pdf","accessDate":"","url":"","note":"","contentType":"application/pdf","charset":"","filename":"Sheen et al. - 2001 - Three-dimensional millimeter-wave imaging for conc.pdf","md5":"b24d91eb76b44cd275e962372a3d7082","mtime":1624206597000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:29:54Z","dateModified":"2021-06-23T13:38:38Z"}},{"key":"AKDMQLEM","version":243,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AKDMQLEM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AKDMQLEM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/I976C3LC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AKDMQLEM","version":243,"parentItem":"I976C3LC","itemType":"attachment","linkMode":"imported_file","title":"Yanik and Torlak - 2019 - Near-Field 2-D SAR Imaging by Millimeter-Wave Rada.pdf","accessDate":"","url":"","note":"","contentType":"application/pdf","charset":"","filename":"Yanik and Torlak - 2019 - Near-Field 2-D SAR Imaging by Millimeter-Wave Rada.pdf","md5":"c986779aa1a91df4d53734df4e0abcd7","mtime":1624206609000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:30:05Z","dateModified":"2021-06-23T13:38:31Z"}},{"key":"K99WRYEI","version":308,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/K99WRYEI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/K99WRYEI","type":"text/html"}},"meta":{"creatorSummary":"Smith et al.","parsedDate":"2020-09-21","numChildren":1},"data":{"key":"K99WRYEI","version":308,"itemType":"conferencePaper","title":"Near-Field MIMO-ISAR Millimeter-Wave Imaging","creators":[{"creatorType":"author","firstName":"Josiah Wayl","lastName":"Smith"},{"creatorType":"author","firstName":"Muhammet Emin","lastName":"Yanik"},{"creatorType":"author","firstName":"Murat","lastName":"Torlak"}],"abstractNote":"Multiple-input-multiple-output (MIMO) millimeterwave (mmWave) sensors for synthetic aperture radar (SAR) and inverse SAR (ISAR) address the fundamental challenges of costeffectiveness and scalability inherent to near-ﬁeld imaging. In this paper, near-ﬁeld MIMO-ISAR mmWave imaging systems are discussed and developed. The rotational ISAR (R-ISAR) regime investigated in this paper requires rotating the target at a constant radial distance from the transceiver and scanning the transceiver along a vertical track. Using a 77GHz mmWave radar, a high resolution three-dimensional (3-D) image can be reconstructed from this two-dimensional scanning taking into account the spherical near-ﬁeld wavefront. While prior work in literature consists of single-input-single-output circular synthetic aperture radar (SISO-CSAR) algorithms or computationally sluggish MIMO-CSAR image reconstruction algorithms, this paper proposes a novel algorithm for efﬁcient MIMO 3-D holographic imaging and details the design of a MIMO R-ISAR imaging system. The proposed algorithm applies a multistatic-tomonostatic phase compensation to the R-ISAR regime allowing for use of highly efﬁcient monostatic algorithms. We demonstrate the algorithm’s performance in real-world imaging scenarios on a prototyped MIMO R-ISAR platform. Our fully integrated system, consisting of an mechanical scanner and efﬁcient imaging algorithm, is capable of pairing the scanning efﬁciency of the MIMO regime with the computational efﬁciency of single pixel image reconstruction algorithms.","date":"2020-9-21","proceedingsTitle":"2020 IEEE Radar Conference (RadarConf20)","conferenceName":"2020 IEEE Radar Conference (RadarConf20)","place":"Florence, Italy","publisher":"IEEE","volume":"","pages":"1-6","series":"","language":"en","DOI":"10.1109/RadarConf2043947.2020.9266412","ISBN":"978-1-72818-942-0","shortTitle":"","url":"https://ieeexplore.ieee.org/document/9266412/","accessDate":"2021-06-23T10:23:16Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-23T10:23:16Z","dateModified":"2021-06-23T10:23:16Z"}},{"key":"68TEZVFV","version":239,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/68TEZVFV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/68TEZVFV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/K99WRYEI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"68TEZVFV","version":239,"parentItem":"K99WRYEI","itemType":"attachment","linkMode":"imported_file","title":"Smith et al. - 2020 - Near-Field MIMO-ISAR Millimeter-Wave Imaging.pdf","accessDate":"","url":"","note":"","contentType":"application/pdf","charset":"","filename":"Smith et al. - 2020 - Near-Field MIMO-ISAR Millimeter-Wave Imaging.pdf","md5":"cb2694d78241f912c3338f6ee84c0f5e","mtime":1624443796000,"tags":[],"relations":{},"dateAdded":"2021-06-23T10:23:11Z","dateModified":"2021-06-23T10:23:16Z"}},{"key":"3BKWE2SR","version":236,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3BKWE2SR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3BKWE2SR","type":"text/html"}},"meta":{"creatorSummary":"Lin et al.","parsedDate":"2021-04-13","numChildren":2},"data":{"key":"3BKWE2SR","version":236,"itemType":"journalArticle","title":"BARF: Bundle-Adjusting Neural Radiance Fields","creators":[{"creatorType":"author","firstName":"Chen-Hsuan","lastName":"Lin"},{"creatorType":"author","firstName":"Wei-Chiu","lastName":"Ma"},{"creatorType":"author","firstName":"Antonio","lastName":"Torralba"},{"creatorType":"author","firstName":"Simon","lastName":"Lucey"}],"abstractNote":"Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses -- the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na\\\"ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.","publicationTitle":"arXiv:2104.06405 [cs]","volume":"","issue":"","pages":"","date":"2021-04-13","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"BARF","url":"http://arxiv.org/abs/2104.06405","accessDate":"2021-06-23T09:02:54Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.06405","tags":[{"tag":"nerf-no pose"}],"collections":[],"relations":{},"dateAdded":"2021-06-23T09:02:54Z","dateModified":"2021-06-23T09:04:10Z"}},{"key":"LTAMWVUW","version":235,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LTAMWVUW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LTAMWVUW","type":"text/html"}},"meta":{"creatorSummary":"Meng et al.","parsedDate":"2021-03-30","numChildren":2},"data":{"key":"LTAMWVUW","version":235,"itemType":"journalArticle","title":"GNeRF: GAN-based Neural Radiance Field without Posed Camera","creators":[{"creatorType":"author","firstName":"Quan","lastName":"Meng"},{"creatorType":"author","firstName":"Anpei","lastName":"Chen"},{"creatorType":"author","firstName":"Haimin","lastName":"Luo"},{"creatorType":"author","firstName":"Minye","lastName":"Wu"},{"creatorType":"author","firstName":"Hao","lastName":"Su"},{"creatorType":"author","firstName":"Lan","lastName":"Xu"},{"creatorType":"author","firstName":"Xuming","lastName":"He"},{"creatorType":"author","firstName":"Jingyi","lastName":"Yu"}],"abstractNote":"We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field reconstruction for the complex scenarios with unknown and even randomly initialized camera poses. Recent NeRF-based advances have gained popularity for remarkable realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initialization. Differently, our GNeRF only utilizes randomly initialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for coarse camera poses and radiance fields jointly optimization, while the second phase refines them with additional photometric loss. We overcome local minima using a hybrid and iterative optimization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated patterns or even low textures that are regarded as extremely challenging before.","publicationTitle":"arXiv:2103.15606 [cs]","volume":"","issue":"","pages":"","date":"2021-03-30","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"GNeRF","url":"http://arxiv.org/abs/2103.15606","accessDate":"2021-06-20T16:04:03Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.15606","tags":[{"tag":"nerf-no pose"}],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:04:03Z","dateModified":"2021-06-23T09:03:44Z"}},{"key":"H2IL5GC8","version":236,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/H2IL5GC8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/H2IL5GC8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/3BKWE2SR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"H2IL5GC8","version":236,"parentItem":"3BKWE2SR","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-23T09:03:31Z","url":"https://arxiv.org/abs/2104.06405","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"b13af07d5e3ad86e85e3e58e20f56377","mtime":1624439011000,"tags":[],"relations":{},"dateAdded":"2021-06-23T09:03:31Z","dateModified":"2021-06-23T09:03:31Z"}},{"key":"TSJVNAHU","version":236,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TSJVNAHU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TSJVNAHU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/3BKWE2SR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TSJVNAHU","version":236,"parentItem":"3BKWE2SR","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-23T09:03:27Z","url":"https://arxiv.org/pdf/2104.06405.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf","md5":"e39281b24706fae556b27bcccc15001e","mtime":1624439007000,"tags":[],"relations":{},"dateAdded":"2021-06-23T09:03:27Z","dateModified":"2021-06-23T09:03:27Z"}},{"key":"S9RRBVBF","version":235,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/S9RRBVBF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/S9RRBVBF","type":"text/html"}},"meta":{"creatorSummary":"Wang et al.","parsedDate":"2021-02-19","numChildren":3},"data":{"key":"S9RRBVBF","version":235,"itemType":"journalArticle","title":"NeRF--: Neural Radiance Fields Without Known Camera Parameters","creators":[{"creatorType":"author","firstName":"Zirui","lastName":"Wang"},{"creatorType":"author","firstName":"Shangzhe","lastName":"Wu"},{"creatorType":"author","firstName":"Weidi","lastName":"Xie"},{"creatorType":"author","firstName":"Min","lastName":"Chen"},{"creatorType":"author","firstName":"Victor Adrian","lastName":"Prisacariu"}],"abstractNote":"This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses and intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences, such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF--, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, including both intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFF benchmark, our model achieves comparable novel view synthesis results compared to the baseline trained with COLMAP pre-computed camera parameters. We also conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAP fails, our model still produces robust results.","publicationTitle":"arXiv:2102.07064 [cs]","volume":"","issue":"","pages":"","date":"2021-02-19","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"NeRF--","url":"http://arxiv.org/abs/2102.07064","accessDate":"2021-06-20T16:03:58Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2102.07064","tags":[{"tag":"nerf-no pose"}],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:58Z","dateModified":"2021-06-23T09:03:24Z"}},{"key":"7DTXF9QA","version":233,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7DTXF9QA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7DTXF9QA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KRKX539K","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7DTXF9QA","version":233,"parentItem":"KRKX539K","itemType":"note","note":"<p>obsurf</p>","tags":[],"relations":{},"dateAdded":"2021-06-23T08:05:16Z","dateModified":"2021-06-23T08:05:49Z"}},{"key":"R8IQJF66","version":228,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/R8IQJF66","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/R8IQJF66","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DQHUP86T","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"R8IQJF66","version":228,"parentItem":"DQHUP86T","itemType":"note","note":"<p>pigan</p>","tags":[],"relations":{},"dateAdded":"2021-06-23T04:58:37Z","dateModified":"2021-06-23T04:58:41Z"}},{"key":"AKL6I9PK","version":225,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AKL6I9PK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AKL6I9PK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/AZZC3G4L","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AKL6I9PK","version":225,"parentItem":"AZZC3G4L","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-22T10:22:51Z","url":"https://arxiv.org/abs/2006.15055","note":"","contentType":"text/html","charset":"utf-8","filename":"2006.html","md5":"ffd9b7876f585588c4bfca0f4f4baae6","mtime":1624357371000,"tags":[],"relations":{},"dateAdded":"2021-06-22T10:22:51Z","dateModified":"2021-06-22T10:22:51Z"}},{"key":"975CNYDP","version":225,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/975CNYDP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/975CNYDP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/AZZC3G4L","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"975CNYDP","version":225,"parentItem":"AZZC3G4L","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-22T10:22:28Z","url":"https://arxiv.org/pdf/2006.15055.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Locatello et al. - 2020 - Object-Centric Learning with Slot Attention.pdf","md5":"978eb0570fa35575ff852fad7f3bc3c6","mtime":1624357348000,"tags":[],"relations":{},"dateAdded":"2021-06-22T10:22:28Z","dateModified":"2021-06-22T10:22:28Z"}},{"key":"AZZC3G4L","version":236,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AZZC3G4L","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AZZC3G4L","type":"text/html"}},"meta":{"creatorSummary":"Locatello et al.","parsedDate":"2020-10-14","numChildren":3},"data":{"key":"AZZC3G4L","version":236,"itemType":"journalArticle","title":"Object-Centric Learning with Slot Attention","creators":[{"creatorType":"author","firstName":"Francesco","lastName":"Locatello"},{"creatorType":"author","firstName":"Dirk","lastName":"Weissenborn"},{"creatorType":"author","firstName":"Thomas","lastName":"Unterthiner"},{"creatorType":"author","firstName":"Aravindh","lastName":"Mahendran"},{"creatorType":"author","firstName":"Georg","lastName":"Heigold"},{"creatorType":"author","firstName":"Jakob","lastName":"Uszkoreit"},{"creatorType":"author","firstName":"Alexey","lastName":"Dosovitskiy"},{"creatorType":"author","firstName":"Thomas","lastName":"Kipf"}],"abstractNote":"Learning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.","publicationTitle":"arXiv:2006.15055 [cs, stat]","volume":"","issue":"","pages":"","date":"2020-10-14","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2006.15055","accessDate":"2021-06-22T10:22:10Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2006.15055","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-22T10:22:10Z","dateModified":"2021-06-22T10:22:10Z"}},{"key":"RKT2I7XB","version":224,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RKT2I7XB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RKT2I7XB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/AZZC3G4L","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RKT2I7XB","version":224,"parentItem":"AZZC3G4L","itemType":"note","note":"Comment: NeurIPS 2020. Code available at https://github.com/google-research/google-research/tree/master/slot_attention","tags":[],"relations":{},"dateAdded":"2021-06-22T10:22:10Z","dateModified":"2021-06-22T10:22:10Z"}},{"key":"BW39ZU3E","version":222,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BW39ZU3E","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BW39ZU3E","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KRKX539K","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BW39ZU3E","version":222,"parentItem":"KRKX539K","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-22T10:15:27Z","url":"https://arxiv.org/abs/2104.01148","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"158ce68cdb8747e8e547fb4bc0b8b7e6","mtime":1624356927000,"tags":[],"relations":{},"dateAdded":"2021-06-22T10:15:27Z","dateModified":"2021-06-22T10:15:27Z"}},{"key":"7EW22NZY","version":222,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7EW22NZY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7EW22NZY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KRKX539K","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7EW22NZY","version":222,"parentItem":"KRKX539K","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-22T10:15:04Z","url":"https://arxiv.org/pdf/2104.01148.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Stelzner et al. - 2021 - Decomposing 3D Scenes into Objects via Unsupervise.pdf","md5":"9879d9567b9ff341d7837bf429639d80","mtime":1624356904000,"tags":[],"relations":{},"dateAdded":"2021-06-22T10:15:04Z","dateModified":"2021-06-22T10:15:04Z"}},{"key":"KRKX539K","version":236,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KRKX539K","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KRKX539K","type":"text/html"}},"meta":{"creatorSummary":"Stelzner et al.","parsedDate":"2021-04-02","numChildren":4},"data":{"key":"KRKX539K","version":236,"itemType":"journalArticle","title":"Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation","creators":[{"creatorType":"author","firstName":"Karl","lastName":"Stelzner"},{"creatorType":"author","firstName":"Kristian","lastName":"Kersting"},{"creatorType":"author","firstName":"Adam R.","lastName":"Kosiorek"}],"abstractNote":"We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.","publicationTitle":"arXiv:2104.01148 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-04-02","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2104.01148","accessDate":"2021-06-22T10:14:53Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.01148","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-22T10:14:53Z","dateModified":"2021-06-22T10:14:53Z"}},{"key":"X7LQ97EN","version":221,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/X7LQ97EN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/X7LQ97EN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KRKX539K","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"X7LQ97EN","version":221,"parentItem":"KRKX539K","itemType":"note","note":"Comment: 15 pages, 3 figures. For project page with videos, see http://stelzner.github.io/obsurf/","tags":[],"relations":{},"dateAdded":"2021-06-22T10:14:53Z","dateModified":"2021-06-22T10:14:53Z"}},{"key":"FH5HLPD3","version":219,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FH5HLPD3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FH5HLPD3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/TRKCUJS5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FH5HLPD3","version":219,"parentItem":"TRKCUJS5","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-22T10:13:18Z","url":"https://arxiv.org/abs/2003.08593","note":"","contentType":"text/html","charset":"utf-8","filename":"2003.html","md5":"d701c6af98054e88a472ac1618d64a6c","mtime":1624356798000,"tags":[],"relations":{},"dateAdded":"2021-06-22T10:13:18Z","dateModified":"2021-06-22T10:13:18Z"}},{"key":"MJ7JLBKI","version":219,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MJ7JLBKI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MJ7JLBKI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/TRKCUJS5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MJ7JLBKI","version":219,"parentItem":"TRKCUJS5","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-22T10:12:56Z","url":"https://arxiv.org/pdf/2003.08593.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Duan et al. - 2020 - Curriculum DeepSDF.pdf","md5":"2ebaa4aa6d39db1b3ac425dbd981efe9","mtime":1624356776000,"tags":[],"relations":{},"dateAdded":"2021-06-22T10:12:56Z","dateModified":"2021-06-22T10:12:56Z"}},{"key":"TRKCUJS5","version":236,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TRKCUJS5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TRKCUJS5","type":"text/html"}},"meta":{"creatorSummary":"Duan et al.","parsedDate":"2020-07-16","numChildren":3},"data":{"key":"TRKCUJS5","version":236,"itemType":"journalArticle","title":"Curriculum DeepSDF","creators":[{"creatorType":"author","firstName":"Yueqi","lastName":"Duan"},{"creatorType":"author","firstName":"Haidong","lastName":"Zhu"},{"creatorType":"author","firstName":"He","lastName":"Wang"},{"creatorType":"author","firstName":"Li","lastName":"Yi"},{"creatorType":"author","firstName":"Ram","lastName":"Nevatia"},{"creatorType":"author","firstName":"Leonidas J.","lastName":"Guibas"}],"abstractNote":"When learning to sketch, beginners start with simple and flexible shapes, and then gradually strive for more complex and accurate ones in the subsequent training sessions. In this paper, we design a \"shape curriculum\" for learning continuous Signed Distance Function (SDF) on shapes, namely Curriculum DeepSDF. Inspired by how humans learn, Curriculum DeepSDF organizes the learning task in ascending order of difficulty according to the following two criteria: surface accuracy and sample difficulty. The former considers stringency in supervising with ground truth, while the latter regards the weights of hard training samples near complex geometry and fine structure. More specifically, Curriculum DeepSDF learns to reconstruct coarse shapes at first, and then gradually increases the accuracy and focuses more on complex local details. Experimental results show that a carefully-designed curriculum leads to significantly better shape reconstructions with the same training data, training epochs and network architecture as DeepSDF. We believe that the application of shape curricula can benefit the training process of a wide variety of 3D shape representation learning methods.","publicationTitle":"arXiv:2003.08593 [cs]","volume":"","issue":"","pages":"","date":"2020-07-16","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2003.08593","accessDate":"2021-06-22T10:12:49Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2003.08593","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-22T10:12:49Z","dateModified":"2021-06-22T10:12:49Z"}},{"key":"4743IM4D","version":218,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4743IM4D","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4743IM4D","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/TRKCUJS5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4743IM4D","version":218,"parentItem":"TRKCUJS5","itemType":"note","note":"Comment: ECCV 2020","tags":[],"relations":{},"dateAdded":"2021-06-22T10:12:49Z","dateModified":"2021-06-22T10:12:49Z"}},{"key":"FYS7UAA3","version":216,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FYS7UAA3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FYS7UAA3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9E7TRNAB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FYS7UAA3","version":216,"parentItem":"9E7TRNAB","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-22T09:55:23Z","url":"https://arxiv.org/abs/2106.06561","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"38245f04078273364c89391a9df7e3ab","mtime":1624355723000,"tags":[],"relations":{},"dateAdded":"2021-06-22T09:55:23Z","dateModified":"2021-06-22T09:55:23Z"}},{"key":"EUUMZ7DQ","version":216,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EUUMZ7DQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EUUMZ7DQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9E7TRNAB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EUUMZ7DQ","version":216,"parentItem":"9E7TRNAB","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-22T09:55:12Z","url":"https://arxiv.org/pdf/2106.06561.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chong and Forsyth - 2021 - GANs N' Roses Stable, Controllable, Diverse Image.pdf","md5":"81cb1c77775419f0875fea9f06e1e95e","mtime":1624355712000,"tags":[],"relations":{},"dateAdded":"2021-06-22T09:55:12Z","dateModified":"2021-06-22T09:55:12Z"}},{"key":"9E7TRNAB","version":236,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9E7TRNAB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9E7TRNAB","type":"text/html"}},"meta":{"creatorSummary":"Chong and Forsyth","parsedDate":"2021-06-11","numChildren":3},"data":{"key":"9E7TRNAB","version":236,"itemType":"journalArticle","title":"GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)","creators":[{"creatorType":"author","firstName":"Min Jin","lastName":"Chong"},{"creatorType":"author","firstName":"David","lastName":"Forsyth"}],"abstractNote":"We show how to learn a map that takes a content code, derived from a face image, and a randomly chosen style code to an anime image. We derive an adversarial loss from our simple and effective definitions of style and content. This adversarial loss guarantees the map is diverse -- a very wide range of anime can be produced from a single content code. Under plausible assumptions, the map is not just diverse, but also correctly represents the probability of an anime, conditioned on an input face. In contrast, current multimodal generation procedures cannot capture the complex styles that appear in anime. Extensive quantitative experiments support the idea the map is correct. Extensive qualitative results show that the method can generate a much more diverse range of styles than SOTA comparisons. Finally, we show that our formalization of content and style allows us to perform video to video translation without ever training on videos.","publicationTitle":"arXiv:2106.06561 [cs]","volume":"","issue":"","pages":"","date":"2021-06-11","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"GANs N' Roses","url":"http://arxiv.org/abs/2106.06561","accessDate":"2021-06-22T09:53:32Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.06561","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-22T09:53:32Z","dateModified":"2021-06-22T09:53:32Z"}},{"key":"D45SRI8J","version":214,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/D45SRI8J","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/D45SRI8J","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9E7TRNAB","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"D45SRI8J","version":214,"parentItem":"9E7TRNAB","itemType":"note","note":"Comment: code is here https://github.com/mchong6/GANsNRoses","tags":[],"relations":{},"dateAdded":"2021-06-22T09:53:32Z","dateModified":"2021-06-22T09:53:32Z"}},{"key":"YNWI67BZ","version":360,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YNWI67BZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YNWI67BZ","type":"text/html"}},"meta":{"creatorSummary":"Barron et al.","parsedDate":"2021-05-06","numChildren":2},"data":{"key":"YNWI67BZ","version":360,"itemType":"journalArticle","title":"Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields","creators":[{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Ben","lastName":"Mildenhall"},{"creatorType":"author","firstName":"Matthew","lastName":"Tancik"},{"creatorType":"author","firstName":"Peter","lastName":"Hedman"},{"creatorType":"author","firstName":"Ricardo","lastName":"Martin-Brualla"},{"creatorType":"author","firstName":"Pratul P.","lastName":"Srinivasan"}],"abstractNote":"The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.","publicationTitle":"arXiv:2103.13415 [cs]","volume":"","issue":"","pages":"","date":"2021-05-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Mip-NeRF","url":"http://arxiv.org/abs/2103.13415","accessDate":"2021-06-20T16:04:02Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.13415","tags":[],"collections":["CPYKW3PF"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/C7NV4DL3"},"dateAdded":"2021-06-20T16:04:02Z","dateModified":"2021-06-22T07:00:06Z"}},{"key":"HSQJR92J","version":396,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HSQJR92J","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HSQJR92J","type":"text/html"}},"meta":{"creatorSummary":"Lin et al.","parsedDate":"2020-12-14","numChildren":2},"data":{"key":"HSQJR92J","version":396,"itemType":"journalArticle","title":"Real-Time High-Resolution Background Matting","creators":[{"creatorType":"author","firstName":"Shanchuan","lastName":"Lin"},{"creatorType":"author","firstName":"Andrey","lastName":"Ryabtsev"},{"creatorType":"author","firstName":"Soumyadip","lastName":"Sengupta"},{"creatorType":"author","firstName":"Brian","lastName":"Curless"},{"creatorType":"author","firstName":"Steve","lastName":"Seitz"},{"creatorType":"author","firstName":"Ira","lastName":"Kemelmacher-Shlizerman"}],"abstractNote":"We introduce a real-time, high-resolution background replacement technique which operates at 30fps in 4K resolution, and 60fps for HD on a modern GPU. Our technique is based on background matting, where an additional frame of the background is captured and used in recovering the alpha matte and the foreground layer. The main challenge is to compute a high-quality alpha matte, preserving strand-level hair details, while processing high-resolution images in real-time. To achieve this goal, we employ two neural networks; a base network computes a low-resolution result which is refined by a second network operating at high-resolution on selective patches. We introduce two largescale video and image matting datasets: VideoMatte240K and PhotoMatte13K/85. Our approach yields higher quality results compared to the previous state-of-the-art in background matting, while simultaneously yielding a dramatic boost in both speed and resolution.","publicationTitle":"arXiv:2012.07810 [cs]","volume":"","issue":"","pages":"","date":"2020-12-14","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2012.07810","accessDate":"2021-06-22T05:53:26Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.07810","tags":[{"tag":"seminar"}],"collections":["F2HZKCVK"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/4F7Q5SQQ"},"dateAdded":"2021-06-22T05:53:26Z","dateModified":"2021-06-22T05:54:19Z"}},{"key":"HK9ACWP3","version":396,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HK9ACWP3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HK9ACWP3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HSQJR92J","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HK9ACWP3","version":396,"parentItem":"HSQJR92J","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-22T05:54:18Z","url":"https://arxiv.org/abs/2012.07810","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"be8e5c74a22d08142f61a5ea8317fc4d","mtime":1624341258000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/3ZRLE895"},"dateAdded":"2021-06-22T05:54:18Z","dateModified":"2021-06-22T05:54:18Z"}},{"key":"CSVSMXND","version":396,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CSVSMXND","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CSVSMXND","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HSQJR92J","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CSVSMXND","version":396,"parentItem":"HSQJR92J","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-22T05:54:14Z","url":"https://arxiv.org/pdf/2012.07810.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Lin et al. - 2020 - Real-Time High-Resolution Background Matting.pdf","md5":"0fe1c86db1492115cd1206632e229d39","mtime":1624341254000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/2EY62MBR"},"dateAdded":"2021-06-22T05:54:14Z","dateModified":"2021-06-22T05:54:14Z"}},{"key":"38R4EMQ7","version":194,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/38R4EMQ7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/38R4EMQ7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WNN73FRH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"38R4EMQ7","version":194,"parentItem":"WNN73FRH","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-22T03:42:03Z","url":"https://arxiv.org/abs/1703.03400","note":"","contentType":"text/html","charset":"utf-8","filename":"1703.html","md5":"525e40ece5aef74831b0bcec695fba0f","mtime":1624333323000,"tags":[],"relations":{},"dateAdded":"2021-06-22T03:42:03Z","dateModified":"2021-06-22T03:42:03Z"}},{"key":"588GUYKM","version":194,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/588GUYKM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/588GUYKM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WNN73FRH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"588GUYKM","version":194,"parentItem":"WNN73FRH","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-22T03:41:58Z","url":"https://arxiv.org/pdf/1703.03400.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf","md5":"6f05654c8b9a8480ab2785f883473c08","mtime":1624333318000,"tags":[],"relations":{},"dateAdded":"2021-06-22T03:41:58Z","dateModified":"2021-06-22T03:41:58Z"}},{"key":"WNN73FRH","version":205,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WNN73FRH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WNN73FRH","type":"text/html"}},"meta":{"creatorSummary":"Finn et al.","parsedDate":"2017-07-18","numChildren":3},"data":{"key":"WNN73FRH","version":205,"itemType":"journalArticle","title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks","creators":[{"creatorType":"author","firstName":"Chelsea","lastName":"Finn"},{"creatorType":"author","firstName":"Pieter","lastName":"Abbeel"},{"creatorType":"author","firstName":"Sergey","lastName":"Levine"}],"abstractNote":"We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.","publicationTitle":"arXiv:1703.03400 [cs]","volume":"","issue":"","pages":"","date":"2017-07-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1703.03400","accessDate":"2021-06-22T03:41:34Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1703.03400","tags":[],"collections":["PKH8WMHP"],"relations":{},"dateAdded":"2021-06-22T03:41:34Z","dateModified":"2021-06-22T03:41:34Z"}},{"key":"T46KFY4Y","version":191,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/T46KFY4Y","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/T46KFY4Y","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WNN73FRH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"T46KFY4Y","version":191,"parentItem":"WNN73FRH","itemType":"note","note":"Comment: ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL results at https://sites.google.com/view/maml, Blog post at http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/","tags":[],"relations":{},"dateAdded":"2021-06-22T03:41:34Z","dateModified":"2021-06-22T03:41:34Z"}},{"key":"5JXL5TAU","version":187,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5JXL5TAU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5JXL5TAU","type":"text/html"}},"meta":{"creatorSummary":"Suwajanakorn et al.","parsedDate":"2015","numChildren":1},"data":{"key":"5JXL5TAU","version":187,"itemType":"journalArticle","title":"What Makes Tom Hanks Look Like Tom Hanks","creators":[{"creatorType":"author","firstName":"Supasorn","lastName":"Suwajanakorn"},{"creatorType":"author","firstName":"Steven M.","lastName":"Seitz"},{"creatorType":"author","firstName":"Ira","lastName":"Kemelmacher-Shlizerman"}],"abstractNote":"We reconstruct a controllable model of a person from a large photo collection that captures his or her persona, i.e., physical appearance and behavior. The ability to operate on unstructured photo collections enables modeling a huge number of people, including celebrities and other well photographed people without requiring them to be scanned. Moreover, we show the ability to drive or puppeteer the captured person B using any other video of a different person A. In this scenario, B acts out the role of person A, but retains his/her own personality and character. Our system is based on a novel combination of 3D face reconstruction, tracking, alignment, and multi-texture modeling, applied to the puppeteering problem. We demonstrate convincing results on a large variety of celebrities derived from Internet imagery and video.","publicationTitle":"2015 IEEE International Conference on Computer Vision (ICCV)","volume":"","issue":"","pages":"3952-3960","date":"12/2015","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"10.1109/ICCV.2015.450","ISSN":"","shortTitle":"","url":"http://ieeexplore.ieee.org/document/7410807/","accessDate":"2021-06-21T18:37:06Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"All rights reserved","extra":"","inPublications":true,"tags":[],"collections":["F2HZKCVK"],"relations":{},"dateAdded":"2021-06-21T18:37:06Z","dateModified":"2021-06-21T18:45:13Z"}},{"key":"FS5FQDCV","version":158,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FS5FQDCV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FS5FQDCV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5JXL5TAU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FS5FQDCV","version":158,"parentItem":"5JXL5TAU","itemType":"attachment","linkMode":"imported_url","title":"Suwajanakorn et al. - 2015 - What Makes Tom Hanks Look Like Tom Hanks.pdf","accessDate":"2021-06-21T18:37:03Z","url":"https://courses.cs.washington.edu/courses/cse455/16wi/notes/Suwajanakorn.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Suwajanakorn et al. - 2015 - What Makes Tom Hanks Look Like Tom Hanks.pdf","md5":"5ab6051095c08ac282d3fe07c734062c","mtime":1624300627000,"tags":[],"relations":{},"dateAdded":"2021-06-21T18:37:03Z","dateModified":"2021-06-21T18:37:07Z"}},{"key":"Y8HTEJV4","version":158,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y8HTEJV4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y8HTEJV4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GKRDIJD5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Y8HTEJV4","version":158,"parentItem":"GKRDIJD5","itemType":"attachment","linkMode":"imported_url","title":"Suwajanakorn et al. - 2017 - Synthesizing Obama learning lip sync from audio.pdf","accessDate":"2021-06-21T18:34:03Z","url":"https://grail.cs.washington.edu/projects/AudioToObama/siggraph17_obama.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Suwajanakorn et al. - 2017 - Synthesizing Obama learning lip sync from audio.pdf","md5":"e2062fdc334df72f76d01e468e630f35","mtime":1624300449000,"tags":[],"relations":{},"dateAdded":"2021-06-21T18:34:03Z","dateModified":"2021-06-21T18:34:09Z"}},{"key":"GKRDIJD5","version":187,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GKRDIJD5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GKRDIJD5","type":"text/html"}},"meta":{"creatorSummary":"Suwajanakorn et al.","parsedDate":"2017-07-20","numChildren":1},"data":{"key":"GKRDIJD5","version":187,"itemType":"journalArticle","title":"Synthesizing Obama: learning lip sync from audio","creators":[{"creatorType":"author","firstName":"Supasorn","lastName":"Suwajanakorn"},{"creatorType":"author","firstName":"Steven M.","lastName":"Seitz"},{"creatorType":"author","firstName":"Ira","lastName":"Kemelmacher-Shlizerman"}],"abstractNote":"","publicationTitle":"ACM Transactions on Graphics","volume":"36","issue":"4","pages":"1-13","date":"2017-07-20","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"ACM Trans. Graph.","language":"en","DOI":"10.1145/3072959.3073640","ISSN":"0730-0301, 1557-7368","shortTitle":"Synthesizing Obama","url":"https://dl.acm.org/doi/10.1145/3072959.3073640","accessDate":"2021-06-21T18:34:08Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"All rights reserved","extra":"","inPublications":true,"tags":[],"collections":["F2HZKCVK"],"relations":{},"dateAdded":"2021-06-21T18:34:08Z","dateModified":"2021-06-21T18:34:08Z"}},{"key":"DLNTUCSH","version":150,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DLNTUCSH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DLNTUCSH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Z9RMBP5X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DLNTUCSH","version":150,"parentItem":"Z9RMBP5X","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-06-21T16:23:47Z","url":"https://arxiv.org/pdf/2003.09852.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Yariv et al. - 2020 - Multiview Neural Surface Reconstruction by Disenta.pdf","md5":"ed98dcae9f06c39d168f2e9fce5fe2ad","mtime":1624292627000,"tags":[],"relations":{},"dateAdded":"2021-06-21T16:23:47Z","dateModified":"2021-06-21T16:23:47Z"}},{"key":"KZWM976P","version":146,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KZWM976P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KZWM976P","type":"text/html"}},"meta":{"numChildren":0},"data":{"key":"KZWM976P","version":146,"itemType":"attachment","linkMode":"imported_file","title":"TPAMI-2021-05-0852_Proof_hi.pdf","accessDate":"","url":"","note":"","contentType":"application/pdf","charset":"","filename":"TPAMI-2021-05-0852_Proof_hi.pdf","md5":"2782955e41fdb136c22039f2d5f37c2d","mtime":1624259395383,"tags":[{"tag":"review"},{"tag":"tpami"}],"collections":[],"relations":{},"dateAdded":"2021-06-21T07:34:41Z","dateModified":"2021-06-21T12:18:37Z"}},{"key":"RCAS5DV9","version":146,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RCAS5DV9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RCAS5DV9","type":"text/html"}},"meta":{"numChildren":0},"data":{"key":"RCAS5DV9","version":146,"itemType":"attachment","linkMode":"imported_file","title":"TPAMISI-2021-03-0432_Proof_hi.pdf","accessDate":"","url":"","note":"","contentType":"application/pdf","charset":"","filename":"TPAMISI-2021-03-0432_Proof_hi.pdf","md5":"089e9942625a5b6ba3fbb8531a93cc74","mtime":1624259169691,"tags":[{"tag":"review"},{"tag":"tpami"}],"collections":[],"relations":{},"dateAdded":"2021-06-21T07:35:07Z","dateModified":"2021-06-21T12:18:31Z"}},{"key":"QWRQKTJA","version":648,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QWRQKTJA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QWRQKTJA","type":"text/html"},"attachment":{"href":"https://api.zotero.org/users/7902311/items/HGQ9WVQ7","type":"application/json","attachmentType":"application/pdf","attachmentSize":27985746}},"meta":{"creatorSummary":"Ho et al.","numChildren":3},"data":{"key":"QWRQKTJA","version":648,"itemType":"journalArticle","title":"Cascaded Diﬀusion Models for High Fidelity Image Generation","creators":[{"creatorType":"author","firstName":"Jonathan","lastName":"Ho"},{"creatorType":"author","firstName":"Chitwan","lastName":"Saharia"},{"creatorType":"author","firstName":"William","lastName":"Chan"},{"creatorType":"author","firstName":"David J","lastName":"Fleet"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"},{"creatorType":"author","firstName":"Tim","lastName":"Salimans"}],"abstractNote":"We show that cascaded diﬀusion models are capable of generating high ﬁdelity images on the class-conditional ImageNet generation challenge, without any assistance from auxiliary image classiﬁers to boost sample quality. A cascaded diﬀusion model comprises a pipeline of multiple diﬀusion models that generate images of increasing resolution, beginning with a standard diﬀusion model at the lowest resolution, followed by one or more super-resolution diﬀusion models that successively upsample the image and add higher resolution details. We ﬁnd that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64×64, 3.52 at 128×128 and 4.88 at 256×256 resolutions, outperforming BigGAN-deep, and classiﬁcation accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256×256, outperforming VQ-VAE-2.","publicationTitle":"","volume":"","issue":"","pages":"28","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/VBJP47EP","http://zotero.org/groups/4458581/items/2VZZUXPX"]},"dateAdded":"2021-06-20T07:01:40Z","dateModified":"2021-06-20T16:53:22Z"}},{"key":"GXUECHN2","version":94,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GXUECHN2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GXUECHN2","type":"text/html"}},"meta":{"creatorSummary":"Zhang et al.","parsedDate":"2020-10-21","numChildren":4},"data":{"key":"GXUECHN2","version":94,"itemType":"journalArticle","title":"NeRF++: Analyzing and Improving Neural Radiance Fields","creators":[{"creatorType":"author","firstName":"Kai","lastName":"Zhang"},{"creatorType":"author","firstName":"Gernot","lastName":"Riegler"},{"creatorType":"author","firstName":"Noah","lastName":"Snavely"},{"creatorType":"author","firstName":"Vladlen","lastName":"Koltun"}],"abstractNote":"Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.","publicationTitle":"arXiv:2010.07492 [cs]","volume":"","issue":"","pages":"","date":"2020-10-21","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"NeRF++","url":"http://arxiv.org/abs/2010.07492","accessDate":"2021-06-20T16:03:46Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2010.07492","tags":[{"tag":"view-synthesis"}],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:46Z","dateModified":"2021-06-20T16:47:50Z"}},{"key":"DK93ZEF2","version":358,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DK93ZEF2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DK93ZEF2","type":"text/html"}},"meta":{"creatorSummary":"Wizadwongsa et al.","parsedDate":"2021-04-12","numChildren":3},"data":{"key":"DK93ZEF2","version":358,"itemType":"journalArticle","title":"NeX: Real-time View Synthesis with Neural Basis Expansion","creators":[{"creatorType":"author","firstName":"Suttisak","lastName":"Wizadwongsa"},{"creatorType":"author","firstName":"Pakkapon","lastName":"Phongthawee"},{"creatorType":"author","firstName":"Jiraphon","lastName":"Yenphraphai"},{"creatorType":"author","firstName":"Supasorn","lastName":"Suwajanakorn"}],"abstractNote":"We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects -- in real time. Unlike traditional MPI that uses a set of simple RGB$\\alpha$ planes, our technique models view-dependent effects by instead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon fine detail and produces state-of-the-art results. Our method is evaluated on benchmark forward-facing datasets as well as our newly-introduced dataset designed to test the limit of view-dependent modeling with significantly more challenging effects such as rainbow reflections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000$\\times$ faster rendering time than the state of the art. For real-time demos, visit https://nex-mpi.github.io/","publicationTitle":"arXiv:2103.05606 [cs]","volume":"","issue":"","pages":"","date":"2021-04-12","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"NeX","url":"http://arxiv.org/abs/2103.05606","accessDate":"2021-06-20T16:04:00Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"All rights reserved","extra":"arXiv: 2103.05606","inPublications":true,"tags":[{"tag":"real-time"},{"tag":"view-synthesis"}],"collections":["CPYKW3PF"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/7AFV33RH"},"dateAdded":"2021-06-20T16:04:00Z","dateModified":"2021-06-20T16:47:40Z"}},{"key":"3JBZMNIE","version":360,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3JBZMNIE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3JBZMNIE","type":"text/html"}},"meta":{"creatorSummary":"Yu et al.","parsedDate":"2021-03-25","numChildren":2},"data":{"key":"3JBZMNIE","version":360,"itemType":"journalArticle","title":"PlenOctrees for Real-time Rendering of Neural Radiance Fields","creators":[{"creatorType":"author","firstName":"Alex","lastName":"Yu"},{"creatorType":"author","firstName":"Ruilong","lastName":"Li"},{"creatorType":"author","firstName":"Matthew","lastName":"Tancik"},{"creatorType":"author","firstName":"Hao","lastName":"Li"},{"creatorType":"author","firstName":"Ren","lastName":"Ng"},{"creatorType":"author","firstName":"Angjoo","lastName":"Kanazawa"}],"abstractNote":"We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees","publicationTitle":"arXiv:2103.14024 [cs]","volume":"","issue":"","pages":"","date":"2021-03-25","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2103.14024","accessDate":"2021-06-20T16:04:02Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.14024","tags":[{"tag":"nerf"},{"tag":"real-time"}],"collections":["8CLBFGR2","CPYKW3PF"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/B2UUWCG9"},"dateAdded":"2021-06-20T16:04:02Z","dateModified":"2021-06-20T16:47:26Z"}},{"key":"4JSQ85CD","version":360,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4JSQ85CD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4JSQ85CD","type":"text/html"}},"meta":{"creatorSummary":"Reiser et al.","parsedDate":"2021-03-25","numChildren":2},"data":{"key":"4JSQ85CD","version":360,"itemType":"journalArticle","title":"KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs","creators":[{"creatorType":"author","firstName":"Christian","lastName":"Reiser"},{"creatorType":"author","firstName":"Songyou","lastName":"Peng"},{"creatorType":"author","firstName":"Yiyi","lastName":"Liao"},{"creatorType":"author","firstName":"Andreas","lastName":"Geiger"}],"abstractNote":"NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that significant speed-ups are possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by two orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.","publicationTitle":"arXiv:2103.13744 [cs]","volume":"","issue":"","pages":"","date":"2021-03-25","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"KiloNeRF","url":"http://arxiv.org/abs/2103.13744","accessDate":"2021-06-20T16:04:03Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.13744","tags":[{"tag":"nerf"},{"tag":"real-time"}],"collections":["8CLBFGR2","CPYKW3PF"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/B3M6EGUK"},"dateAdded":"2021-06-20T16:04:04Z","dateModified":"2021-06-20T16:46:57Z"}},{"key":"RNT8QSRD","version":649,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RNT8QSRD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RNT8QSRD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/SEIXRRIR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RNT8QSRD","version":649,"parentItem":"SEIXRRIR","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:44:07Z","url":"https://arxiv.org/abs/2105.05233","note":"","contentType":"text/html","charset":"utf-8","filename":"2105.html","md5":"7fd1bea8f3d8ded5387e124d2c9ba9e8","mtime":1624207447000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/X4DYUX3D"},"dateAdded":"2021-06-20T16:44:07Z","dateModified":"2021-06-20T16:44:07Z"}},{"key":"GUCCBNJF","version":649,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GUCCBNJF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GUCCBNJF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/SEIXRRIR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GUCCBNJF","version":649,"parentItem":"SEIXRRIR","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:44:03Z","url":"https://arxiv.org/pdf/2105.05233.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf","md5":"fe96dd1d08efcaf94a680c1c7bfa9a58","mtime":1624207443000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/3CLTD4LV"},"dateAdded":"2021-06-20T16:44:03Z","dateModified":"2021-06-20T16:44:03Z"}},{"key":"EPHY5DHY","version":104,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EPHY5DHY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EPHY5DHY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BKNP8W5V","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EPHY5DHY","version":104,"parentItem":"BKNP8W5V","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:41:45Z","url":"https://arxiv.org/abs/2103.12352","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"b5c5bf7f22266463e50c7434897bd1fc","mtime":1624207305000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:41:45Z","dateModified":"2021-06-20T16:41:45Z"}},{"key":"UNKMMGNR","version":104,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UNKMMGNR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UNKMMGNR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BKNP8W5V","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UNKMMGNR","version":104,"parentItem":"BKNP8W5V","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:41:41Z","url":"https://arxiv.org/pdf/2103.12352.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Sucar et al. - 2021 - iMAP Implicit Mapping and Positioning in Real-Tim.pdf","md5":"12681d7f346f724584428d8f67c4a8cd","mtime":1624207301000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:41:41Z","dateModified":"2021-06-20T16:41:41Z"}},{"key":"5SHECBKS","version":102,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5SHECBKS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5SHECBKS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S8D7ZN3X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5SHECBKS","version":102,"parentItem":"S8D7ZN3X","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:40:35Z","url":"https://arxiv.org/abs/2104.05358","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"9e957093ca9577372796e910fe611ec4","mtime":1624207235000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:40:35Z","dateModified":"2021-06-20T16:40:35Z"}},{"key":"C3MDVZBZ","version":84,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/C3MDVZBZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/C3MDVZBZ","type":"text/html"}},"meta":{"creatorSummary":"Kingma and Welling","parsedDate":"2014-05-01","numChildren":2},"data":{"key":"C3MDVZBZ","version":84,"itemType":"journalArticle","title":"Auto-Encoding Variational Bayes","creators":[{"creatorType":"author","firstName":"Diederik P.","lastName":"Kingma"},{"creatorType":"author","firstName":"Max","lastName":"Welling"}],"abstractNote":"How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.","publicationTitle":"arXiv:1312.6114 [cs, stat]","volume":"","issue":"","pages":"","date":"2014-05-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1312.6114","accessDate":"2021-06-20T16:38:23Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1312.6114","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:38:23Z","dateModified":"2021-06-20T16:40:30Z"}},{"key":"UVJFD2EP","version":102,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UVJFD2EP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UVJFD2EP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/C3MDVZBZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UVJFD2EP","version":102,"parentItem":"C3MDVZBZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:40:29Z","url":"https://arxiv.org/abs/1312.6114","note":"","contentType":"text/html","charset":"utf-8","filename":"1312.html","md5":"4430c944ef5e744637c61374b1fa8409","mtime":1624207229000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:40:29Z","dateModified":"2021-06-20T16:40:29Z"}},{"key":"TBPDF5RX","version":102,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TBPDF5RX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TBPDF5RX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/C3MDVZBZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TBPDF5RX","version":102,"parentItem":"C3MDVZBZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:40:25Z","url":"https://arxiv.org/pdf/1312.6114.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf","md5":"e8088963a9bacf2db729bc01834ff6d9","mtime":1624207225000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:40:25Z","dateModified":"2021-06-20T16:40:25Z"}},{"key":"Q2JLNY2X","version":361,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Q2JLNY2X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Q2JLNY2X","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4JSQ85CD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Q2JLNY2X","version":361,"parentItem":"4JSQ85CD","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:40:17Z","url":"https://arxiv.org/abs/2103.13744","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"d40020299bb5daa81913e3e43710834a","mtime":1624207217000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/VANAFHUI"},"dateAdded":"2021-06-20T16:40:17Z","dateModified":"2021-06-20T16:40:17Z"}},{"key":"MTNJ2GKG","version":361,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MTNJ2GKG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MTNJ2GKG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4JSQ85CD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MTNJ2GKG","version":361,"parentItem":"4JSQ85CD","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:40:13Z","url":"https://arxiv.org/pdf/2103.13744.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Reiser et al. - 2021 - KiloNeRF Speeding up Neural Radiance Fields with .pdf","md5":"52e9df6eecfce7354fb396915409ddca","mtime":1624207212000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/R8H8U6LB"},"dateAdded":"2021-06-20T16:40:13Z","dateModified":"2021-06-20T16:40:13Z"}},{"key":"XZE7M7LL","version":361,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XZE7M7LL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XZE7M7LL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/3JBZMNIE","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XZE7M7LL","version":361,"parentItem":"3JBZMNIE","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:40:12Z","url":"https://arxiv.org/abs/2103.14024","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"a47bbe58a369f7fb8801961cc4397cf5","mtime":1624207212000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/333JG2IA"},"dateAdded":"2021-06-20T16:40:12Z","dateModified":"2021-06-20T16:40:12Z"}},{"key":"H352AKCW","version":361,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/H352AKCW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/H352AKCW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/3JBZMNIE","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"H352AKCW","version":361,"parentItem":"3JBZMNIE","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:40:08Z","url":"https://arxiv.org/pdf/2103.14024.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Yu et al. - 2021 - PlenOctrees for Real-time Rendering of Neural Radi.pdf","md5":"c19b5319b72db41bd95900c95a5981d1","mtime":1624207208000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/FAESGIYI"},"dateAdded":"2021-06-20T16:40:08Z","dateModified":"2021-06-20T16:40:08Z"}},{"key":"8W4AKQ3K","version":361,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8W4AKQ3K","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8W4AKQ3K","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/74A95424","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8W4AKQ3K","version":361,"parentItem":"74A95424","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:40:07Z","url":"https://arxiv.org/abs/2103.14645","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"d5049607f836d74ef511876af97cbe04","mtime":1624207207000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/VYMQC5YQ"},"dateAdded":"2021-06-20T16:40:07Z","dateModified":"2021-06-20T16:40:07Z"}},{"key":"3GGIQ98X","version":361,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3GGIQ98X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3GGIQ98X","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/74A95424","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3GGIQ98X","version":361,"parentItem":"74A95424","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:40:03Z","url":"https://arxiv.org/pdf/2103.14645.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Hedman et al. - 2021 - Baking Neural Radiance Fields for Real-Time View S.pdf","md5":"ef46c5db17d0e8c05bb03d94fba09dd6","mtime":1624207203000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/PQERICHA"},"dateAdded":"2021-06-20T16:40:03Z","dateModified":"2021-06-20T16:40:03Z"}},{"key":"R6TNML75","version":101,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/R6TNML75","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/R6TNML75","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LTAMWVUW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"R6TNML75","version":101,"parentItem":"LTAMWVUW","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:39:44Z","url":"https://arxiv.org/abs/2103.15606","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"47bb454cd473155ebbdc5740d7258847","mtime":1624207184000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:39:44Z","dateModified":"2021-06-20T16:39:44Z"}},{"key":"VXUXT97R","version":101,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VXUXT97R","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VXUXT97R","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LTAMWVUW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VXUXT97R","version":101,"parentItem":"LTAMWVUW","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:39:39Z","url":"https://arxiv.org/pdf/2103.15606.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Meng et al. - 2021 - GNeRF GAN-based Neural Radiance Field without Pos.pdf","md5":"d1dcb86ee530325de072ffb3bc7f9fac","mtime":1624207179000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:39:39Z","dateModified":"2021-06-20T16:39:39Z"}},{"key":"YJ2MLME3","version":101,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YJ2MLME3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YJ2MLME3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MCKXIXQG","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YJ2MLME3","version":101,"parentItem":"MCKXIXQG","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:39:30Z","url":"https://arxiv.org/abs/2104.00670","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"3de53b38af1f77a762f476b1fec0406e","mtime":1624207170000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:39:30Z","dateModified":"2021-06-20T16:39:30Z"}},{"key":"UYMQHUYF","version":101,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UYMQHUYF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UYMQHUYF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/MCKXIXQG","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UYMQHUYF","version":101,"parentItem":"MCKXIXQG","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:39:25Z","url":"https://arxiv.org/pdf/2104.00670.pdf","note":"","contentType":"application/pdf","charset":"","filename":"DeVries et al. - 2021 - Unconstrained Scene Generation with Locally Condit.pdf","md5":"05b035db5b8ed81cdb7ee40058e7053b","mtime":1624207165000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:39:25Z","dateModified":"2021-06-20T16:39:25Z"}},{"key":"3U5CG7XE","version":358,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3U5CG7XE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3U5CG7XE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DK93ZEF2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3U5CG7XE","version":358,"parentItem":"DK93ZEF2","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:39:19Z","url":"https://arxiv.org/abs/2103.05606","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"6a943368c463bde4a4c4308de7bd3a87","mtime":1624207159000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/WUIPIF4F"},"dateAdded":"2021-06-20T16:39:19Z","dateModified":"2021-06-20T16:39:19Z"}},{"key":"CDBH5F6C","version":358,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CDBH5F6C","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CDBH5F6C","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DK93ZEF2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CDBH5F6C","version":358,"parentItem":"DK93ZEF2","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:39:13Z","url":"https://arxiv.org/pdf/2103.05606.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wizadwongsa et al. - 2021 - NeX Real-time View Synthesis with Neural Basis Ex.pdf","md5":"d4564e67d2056974e6b18746b0db74ed","mtime":1624207153000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/VAX27KT5"},"dateAdded":"2021-06-20T16:39:13Z","dateModified":"2021-06-20T16:39:13Z"}},{"key":"KCBDN2JP","version":101,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KCBDN2JP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KCBDN2JP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/EDN42UE6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KCBDN2JP","version":101,"parentItem":"EDN42UE6","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:38:56Z","url":"https://arxiv.org/abs/2103.15595","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"56a86d444c6f21585eec3a0888f4c508","mtime":1624207136000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:38:56Z","dateModified":"2021-06-20T16:38:56Z"}},{"key":"ZVVCFK4R","version":101,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZVVCFK4R","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZVVCFK4R","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/EDN42UE6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZVVCFK4R","version":101,"parentItem":"EDN42UE6","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:38:52Z","url":"https://arxiv.org/pdf/2103.15595.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chen et al. - 2021 - MVSNeRF Fast Generalizable Radiance Field Reconst.pdf","md5":"af385289f843d823174a720b02272847","mtime":1624207132000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:38:52Z","dateModified":"2021-06-20T16:38:52Z"}},{"key":"BHMVDPJL","version":360,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BHMVDPJL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BHMVDPJL","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/J9YJJ2H5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BHMVDPJL","version":360,"parentItem":"J9YJJ2H5","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:38:47Z","url":"https://arxiv.org/abs/2103.10380","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"ad438a8f9239768d4ac7990e615ff81f","mtime":1624207127000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/DWHTUFRA"},"dateAdded":"2021-06-20T16:38:47Z","dateModified":"2021-06-20T16:38:47Z"}},{"key":"4XR5M2N2","version":360,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4XR5M2N2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4XR5M2N2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/J9YJJ2H5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4XR5M2N2","version":360,"parentItem":"J9YJJ2H5","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:38:42Z","url":"https://arxiv.org/pdf/2103.10380.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Garbin et al. - 2021 - FastNeRF High-Fidelity Neural Rendering at 200FPS.pdf","md5":"c50d248ef212d2204d30abd5614a4370","mtime":1624207122000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/8JVA3EIY"},"dateAdded":"2021-06-20T16:38:42Z","dateModified":"2021-06-20T16:38:42Z"}},{"key":"WINCRCNM","version":100,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WINCRCNM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WINCRCNM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Q3DXFQ7X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WINCRCNM","version":100,"parentItem":"Q3DXFQ7X","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:38:41Z","url":"https://arxiv.org/abs/2103.03231","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"270ba8c7163a243856d38fc9ebac272d","mtime":1624207121000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:38:41Z","dateModified":"2021-06-20T16:38:41Z"}},{"key":"4637HTA6","version":361,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4637HTA6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4637HTA6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/YNWI67BZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4637HTA6","version":361,"parentItem":"YNWI67BZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:38:38Z","url":"https://arxiv.org/abs/2103.13415","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"56c44f08550966cf523a00ceda989e13","mtime":1624207118000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/N2AKU84F"},"dateAdded":"2021-06-20T16:38:38Z","dateModified":"2021-06-20T16:38:38Z"}},{"key":"VX6MN8B8","version":100,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VX6MN8B8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VX6MN8B8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Q3DXFQ7X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VX6MN8B8","version":100,"parentItem":"Q3DXFQ7X","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:38:36Z","url":"https://arxiv.org/pdf/2103.03231.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Neff et al. - 2021 - DONeRF Towards Real-Time Rendering of Compact Neu.pdf","md5":"b94413f19a156e343834b5e39e408034","mtime":1624207116000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:38:36Z","dateModified":"2021-06-20T16:38:36Z"}},{"key":"PZPWCXFF","version":360,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PZPWCXFF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PZPWCXFF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/YNWI67BZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PZPWCXFF","version":360,"parentItem":"YNWI67BZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:38:33Z","url":"https://arxiv.org/pdf/2103.13415.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf","md5":"7adbb0dc3e274fecb94d8d92f9210276","mtime":1624207113000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/YQV7GDV3"},"dateAdded":"2021-06-20T16:38:33Z","dateModified":"2021-06-20T16:38:33Z"}},{"key":"BXE9WE7G","version":99,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BXE9WE7G","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BXE9WE7G","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/N9TACYGJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BXE9WE7G","version":99,"parentItem":"N9TACYGJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:38:29Z","url":"https://arxiv.org/abs/2103.00762","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"b3eb7ace83dcdcb1a7cf851d99d05823","mtime":1624207109000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:38:29Z","dateModified":"2021-06-20T16:38:29Z"}},{"key":"YXR856JD","version":99,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YXR856JD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YXR856JD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/N9TACYGJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YXR856JD","version":99,"parentItem":"N9TACYGJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:38:24Z","url":"https://arxiv.org/pdf/2103.00762.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Xiang et al. - 2021 - NeuTex Neural Texture Mapping for Volumetric Neur.pdf","md5":"4fa49f0859b9c761be3c3914c2eb9e66","mtime":1624207104000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:38:24Z","dateModified":"2021-06-20T16:38:24Z"}},{"key":"KW2WUUNK","version":99,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KW2WUUNK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KW2WUUNK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QNXL2EIJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KW2WUUNK","version":99,"parentItem":"QNXL2EIJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:38:01Z","url":"https://arxiv.org/abs/2103.01954","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"7ca39b17446021b21be789536ed58d72","mtime":1624207081000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:38:01Z","dateModified":"2021-06-20T16:38:01Z"}},{"key":"9QNXEUXP","version":99,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9QNXEUXP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9QNXEUXP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QNXL2EIJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9QNXEUXP","version":99,"parentItem":"QNXL2EIJ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:37:56Z","url":"https://arxiv.org/pdf/2103.01954.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Lombardi et al. - 2021 - Mixture of Volumetric Primitives for Efficient Neu.pdf","md5":"f09cf4f788984d87cfbb512e03d364d5","mtime":1624207076000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:37:56Z","dateModified":"2021-06-20T16:37:56Z"}},{"key":"FQUXGMDF","version":99,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FQUXGMDF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FQUXGMDF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BE56GYH3","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FQUXGMDF","version":99,"parentItem":"BE56GYH3","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:37:48Z","url":"https://arxiv.org/abs/2102.13090","note":"","contentType":"text/html","charset":"utf-8","filename":"2102.html","md5":"0ac852292feaca2136b33e3aef7238a0","mtime":1624207068000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:37:48Z","dateModified":"2021-06-20T16:37:48Z"}},{"key":"93GCMLC7","version":98,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/93GCMLC7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/93GCMLC7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VDKL49H5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"93GCMLC7","version":98,"parentItem":"VDKL49H5","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:37:45Z","url":"https://arxiv.org/abs/2101.10994","note":"","contentType":"text/html","charset":"utf-8","filename":"2101.html","md5":"9cae0ef8f9693243094b0173f5be27d1","mtime":1624207065000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:37:45Z","dateModified":"2021-06-20T16:37:45Z"}},{"key":"8W5CQA6E","version":98,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8W5CQA6E","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8W5CQA6E","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WMXXCCEA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8W5CQA6E","version":98,"parentItem":"WMXXCCEA","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:37:45Z","url":"https://arxiv.org/abs/2012.12247","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"596d1c00948705528997626667a5f190","mtime":1624207065000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:37:45Z","dateModified":"2021-06-20T16:37:45Z"}},{"key":"U3HGGXX8","version":99,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U3HGGXX8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U3HGGXX8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BE56GYH3","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"U3HGGXX8","version":99,"parentItem":"BE56GYH3","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:37:43Z","url":"https://arxiv.org/pdf/2102.13090.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wang et al. - 2021 - IBRNet Learning Multi-View Image-Based Rendering.pdf","md5":"a4c138c7718de5a94a638b964e10475f","mtime":1624207063000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:37:43Z","dateModified":"2021-06-20T16:37:43Z"}},{"key":"VHDFA2M7","version":98,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VHDFA2M7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VHDFA2M7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VDKL49H5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VHDFA2M7","version":98,"parentItem":"VDKL49H5","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:37:40Z","url":"https://arxiv.org/pdf/2101.10994.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Takikawa et al. - 2021 - Neural Geometric Level of Detail Real-time Render.pdf","md5":"7c8885f3d38e719846701f122080ca0d","mtime":1624207060000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:37:40Z","dateModified":"2021-06-20T16:37:40Z"}},{"key":"8TSUMA2M","version":98,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8TSUMA2M","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8TSUMA2M","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WMXXCCEA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8TSUMA2M","version":98,"parentItem":"WMXXCCEA","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:37:39Z","url":"https://arxiv.org/pdf/2012.12247.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tretschk et al. - 2021 - Non-Rigid Neural Radiance Fields Reconstruction a.pdf","md5":"43d0f573fc7540c4bf9d80df92af8af4","mtime":1624207059000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:37:39Z","dateModified":"2021-06-20T16:37:39Z"}},{"key":"KPJXEJFJ","version":102,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KPJXEJFJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KPJXEJFJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/29Y6HCH6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KPJXEJFJ","version":102,"parentItem":"29Y6HCH6","itemType":"attachment","linkMode":"imported_url","title":"Kellnhofer et al. - Neural Lumigraph Rendering.pdf","accessDate":"2021-06-20T16:37:24Z","url":"http://www.computationalimaging.org/wp-content/uploads/2021/03/neural_lumigraph_rendering_cvpr2021.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kellnhofer et al. - Neural Lumigraph Rendering.pdf","md5":"d7fafa1720ee6ed9c808a149d458fb03","mtime":1624207046000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:37:24Z","dateModified":"2021-06-20T16:37:26Z"}},{"key":"29Y6HCH6","version":83,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/29Y6HCH6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/29Y6HCH6","type":"text/html"}},"meta":{"creatorSummary":"Kellnhofer et al.","numChildren":1},"data":{"key":"29Y6HCH6","version":83,"itemType":"journalArticle","title":"Neural Lumigraph Rendering","creators":[{"creatorType":"author","firstName":"Petr","lastName":"Kellnhofer"},{"creatorType":"author","firstName":"Lars C","lastName":"Jebe"},{"creatorType":"author","firstName":"Andrew","lastName":"Jones"},{"creatorType":"author","firstName":"Ryan","lastName":"Spicer"},{"creatorType":"author","firstName":"Kari","lastName":"Pulli"},{"creatorType":"author","firstName":"Gordon","lastName":"Wetzstein"}],"abstractNote":"Novel view synthesis is a challenging and ill-posed inverse rendering problem. Neural rendering techniques have recently achieved photorealistic image quality for this task. State-of-the-art (SOTA) neural volume rendering approaches, however, are slow to train and require minutes of inference (i.e., rendering) time for high image resolutions. We adopt high-capacity neural scene representations with periodic activations for jointly optimizing an implicit surface and a radiance ﬁeld of a scene supervised exclusively with posed 2D images. Our neural rendering pipeline accelerates SOTA neural volume rendering by about two orders of magnitude and our implicit surface representation is unique in allowing us to export a mesh with view-dependent texture information. Thus, like other implicit surface representations, ours is compatible with traditional graphics pipelines, enabling real-time rendering rates, while achieving unprecedented image quality compared to other surface methods. We assess the quality of our approach using existing datasets as well as high-quality 3D face data captured with a custom multi-camera rig.","publicationTitle":"","volume":"","issue":"","pages":"11","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:37:26Z","dateModified":"2021-06-20T16:37:26Z"}},{"key":"ARKAQYWH","version":98,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ARKAQYWH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ARKAQYWH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IWDM4ZF5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ARKAQYWH","version":98,"parentItem":"IWDM4ZF5","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:37:21Z","url":"https://arxiv.org/abs/2012.09955","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"96c09b5a8be9bf6bb0551e3cec6834ee","mtime":1624207041000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:37:21Z","dateModified":"2021-06-20T16:37:21Z"}},{"key":"EIXF8LLF","version":98,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EIXF8LLF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EIXF8LLF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IWDM4ZF5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EIXF8LLF","version":98,"parentItem":"IWDM4ZF5","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:37:16Z","url":"https://arxiv.org/pdf/2012.09955.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wang et al. - 2020 - Learning Compositional Radiance Fields of Dynamic .pdf","md5":"89f3e624802288add87e30afdc529500","mtime":1624207036000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:37:16Z","dateModified":"2021-06-20T16:37:16Z"}},{"key":"MXC9AUWF","version":99,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MXC9AUWF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MXC9AUWF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UQPB3TH8","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MXC9AUWF","version":99,"parentItem":"UQPB3TH8","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:36:51Z","url":"https://arxiv.org/abs/2102.08860","note":"","contentType":"text/html","charset":"utf-8","filename":"2102.html","md5":"1cb6651203191433d50c517bfa4f52f7","mtime":1624207011000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:36:51Z","dateModified":"2021-06-20T16:36:51Z"}},{"key":"A95FB9Z5","version":99,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A95FB9Z5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A95FB9Z5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UQPB3TH8","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"A95FB9Z5","version":99,"parentItem":"UQPB3TH8","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:36:47Z","url":"https://arxiv.org/pdf/2102.08860.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Rematas et al. - 2021 - ShaRF Shape-conditioned Radiance Fields from a Si.pdf","md5":"94e0a4fb61f747fb23b84627ff8fb003","mtime":1624207007000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:36:47Z","dateModified":"2021-06-20T16:36:47Z"}},{"key":"KK2ZA9GH","version":98,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KK2ZA9GH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KK2ZA9GH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S9RRBVBF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KK2ZA9GH","version":98,"parentItem":"S9RRBVBF","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:36:38Z","url":"https://arxiv.org/abs/2102.07064","note":"","contentType":"text/html","charset":"utf-8","filename":"2102.html","md5":"3c607288432dfb99361fdaa219355d2f","mtime":1624206998000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:36:38Z","dateModified":"2021-06-20T16:36:38Z"}},{"key":"JU5R4ACQ","version":98,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JU5R4ACQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JU5R4ACQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S9RRBVBF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JU5R4ACQ","version":98,"parentItem":"S9RRBVBF","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:36:33Z","url":"https://arxiv.org/pdf/2102.07064.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wang et al. - 2021 - NeRF-- Neural Radiance Fields Without Known Camer.pdf","md5":"e20ec78eda254ee9589e5588ad0f1c2a","mtime":1624206993000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:36:33Z","dateModified":"2021-06-20T16:36:33Z"}},{"key":"PDRM7Y2K","version":699,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PDRM7Y2K","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PDRM7Y2K","type":"text/html"}},"meta":{"creatorSummary":"Riegler and Koltun","parsedDate":"2020","numChildren":1},"data":{"key":"PDRM7Y2K","version":699,"itemType":"conferencePaper","title":"Free View Synthesis","creators":[{"creatorType":"editor","firstName":"Andrea","lastName":"Vedaldi"},{"creatorType":"editor","firstName":"Horst","lastName":"Bischof"},{"creatorType":"editor","firstName":"Thomas","lastName":"Brox"},{"creatorType":"editor","firstName":"Jan-Michael","lastName":"Frahm"},{"creatorType":"author","firstName":"Gernot","lastName":"Riegler"},{"creatorType":"author","firstName":"Vladlen","lastName":"Koltun"}],"abstractNote":"We present a method for novel view synthesis from input images that are freely distributed around a scene. Our method does not rely on a regular arrangement of input views, can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. We calibrate the input images via SfM and erect a coarse geometric scaﬀold via MVS. This scaﬀold is used to create a proxy depth map for a novel view of the scene. Based on this depth map, a recurrent encoder-decoder network processes reprojected features from nearby views and synthesizes the new view. Our network does not need to be optimized for a given scene. After training on a dataset, it works in previously unseen environments with no ﬁnetuning or per-scene optimization. We evaluate the presented approach on challenging real-world datasets, including Tanks and Temples, where we demonstrate successful view synthesis for the ﬁrst time and substantially outperform prior and concurrent work.","date":"2020","proceedingsTitle":"Computer Vision – ECCV 2020","conferenceName":"","place":"Cham","publisher":"Springer International Publishing","volume":"12364","pages":"623-640","series":"","language":"en","DOI":"10.1007/978-3-030-58529-7_37","ISBN":"978-3-030-58528-0 978-3-030-58529-7","shortTitle":"","url":"https://link.springer.com/10.1007/978-3-030-58529-7_37","accessDate":"2021-06-20T16:35:45Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"Series Title: Lecture Notes in Computer Science","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:35:45Z","dateModified":"2021-06-20T16:36:24Z"}},{"key":"Z7HJI4Z7","version":97,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z7HJI4Z7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z7HJI4Z7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9YFDKDRF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Z7HJI4Z7","version":97,"parentItem":"9YFDKDRF","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:36:03Z","url":"https://arxiv.org/abs/2012.09854","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"8b5891d536a08c1ddecba5a18f9333dd","mtime":1624206963000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:36:03Z","dateModified":"2021-06-20T16:36:03Z"}},{"key":"U28HUXCI","version":97,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U28HUXCI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U28HUXCI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VYTC7KJ6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"U28HUXCI","version":97,"parentItem":"VYTC7KJ6","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:36:00Z","url":"https://arxiv.org/abs/2012.09855","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"f978e19563585c415b285375bf3043c2","mtime":1624206960000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:36:00Z","dateModified":"2021-06-20T16:36:00Z"}},{"key":"GFT5LQQ2","version":97,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GFT5LQQ2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GFT5LQQ2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9YFDKDRF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GFT5LQQ2","version":97,"parentItem":"9YFDKDRF","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:35:58Z","url":"https://arxiv.org/pdf/2012.09854.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Hu et al. - 2021 - Worldsheet Wrapping the World in a 3D Sheet for V.pdf","md5":"3ff8f249edf7e6f555d23907e432d704","mtime":1624206958000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:35:58Z","dateModified":"2021-06-20T16:35:58Z"}},{"key":"HRHXD8DU","version":83,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HRHXD8DU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HRHXD8DU","type":"text/html"}},"meta":{"creatorSummary":"Bertel et al.","parsedDate":"2020-12-04","numChildren":0},"data":{"key":"HRHXD8DU","version":83,"itemType":"conferencePaper","title":"Deferred Neural Rendering for View Extrapolation","creators":[{"creatorType":"author","firstName":"Tobias","lastName":"Bertel"},{"creatorType":"author","firstName":"Yusuke","lastName":"Tomoto"},{"creatorType":"author","firstName":"Srinivas","lastName":"Rao"},{"creatorType":"author","firstName":"Rodrigo","lastName":"Ortiz-Cayon"},{"creatorType":"author","firstName":"Stefan","lastName":"Holzer"},{"creatorType":"author","firstName":"Christian","lastName":"Richardt"}],"abstractNote":"","date":"2020-12-04","proceedingsTitle":"SIGGRAPH Asia 2020 Posters","conferenceName":"SA '20: SIGGRAPH Asia 2020","place":"Virtual Event Republic of Korea","publisher":"ACM","volume":"","pages":"1-2","series":"","language":"en","DOI":"10.1145/3415264.3425441","ISBN":"978-1-4503-8113-0","shortTitle":"","url":"https://dl.acm.org/doi/10.1145/3415264.3425441","accessDate":"2021-06-20T16:35:55Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:35:55Z","dateModified":"2021-06-20T16:35:55Z"}},{"key":"G4FRZXP9","version":97,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/G4FRZXP9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/G4FRZXP9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VYTC7KJ6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"G4FRZXP9","version":97,"parentItem":"VYTC7KJ6","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:35:54Z","url":"https://arxiv.org/pdf/2012.09855.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Liu et al. - 2020 - Infinite Nature Perpetual View Generation of Natu.pdf","md5":"600e4ac92451e4a6b56ff414947c2297","mtime":1624206954000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:35:54Z","dateModified":"2021-06-20T16:35:54Z"}},{"key":"UH9ATHAH","version":102,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UH9ATHAH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UH9ATHAH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PDRM7Y2K","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UH9ATHAH","version":102,"parentItem":"PDRM7Y2K","itemType":"attachment","linkMode":"imported_url","title":"Riegler and Koltun - 2020 - Free View Synthesis.pdf","accessDate":"2021-06-20T16:35:43Z","url":"http://vladlen.info/papers/FVS.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Riegler and Koltun - 2020 - Free View Synthesis.pdf","md5":"b28e194d464fdf99b7d400255fef69f6","mtime":1624206946000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:35:43Z","dateModified":"2021-06-20T16:35:46Z"}},{"key":"8H7AGWAS","version":98,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8H7AGWAS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8H7AGWAS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/75JSFXK4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8H7AGWAS","version":98,"parentItem":"75JSFXK4","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:35:04Z","url":"https://arxiv.org/abs/2012.15838","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"20be236a663c4554c25496bb8f05789f","mtime":1624206904000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:35:04Z","dateModified":"2021-06-20T16:35:04Z"}},{"key":"J9F5ED69","version":98,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/J9F5ED69","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/J9F5ED69","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/75JSFXK4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"J9F5ED69","version":98,"parentItem":"75JSFXK4","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:34:59Z","url":"https://arxiv.org/pdf/2012.15838.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Peng et al. - 2021 - Neural Body Implicit Neural Representations with .pdf","md5":"39b22df8def8c0e7e2f4fa375ccf1333","mtime":1624206899000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:59Z","dateModified":"2021-06-20T16:34:59Z"}},{"key":"T4IF5W6J","version":97,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/T4IF5W6J","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/T4IF5W6J","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/EQVWR6WG","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"T4IF5W6J","version":97,"parentItem":"EQVWR6WG","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:34:42Z","url":"https://arxiv.org/abs/2012.05217","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"71e18354f27f2d31693c7ec97b44d3db","mtime":1624206882000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:42Z","dateModified":"2021-06-20T16:34:42Z"}},{"key":"H94SZGPX","version":97,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/H94SZGPX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/H94SZGPX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/EQVWR6WG","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"H94SZGPX","version":97,"parentItem":"EQVWR6WG","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:34:37Z","url":"https://arxiv.org/pdf/2012.05217.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Xu et al. - 2020 - Positional Encoding as Spatial Inductive Bias in G.pdf","md5":"f40014b137eff078f5c4a40062dab38f","mtime":1624206877000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:37Z","dateModified":"2021-06-20T16:34:37Z"}},{"key":"YUPY3979","version":97,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YUPY3979","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YUPY3979","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VMJXU2HQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YUPY3979","version":97,"parentItem":"VMJXU2HQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:34:32Z","url":"https://arxiv.org/abs/2012.03927","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"0aa08ddf8278b1e1f269ae6bfe075e95","mtime":1624206872000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:32Z","dateModified":"2021-06-20T16:34:32Z"}},{"key":"L7IQHNLZ","version":97,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/L7IQHNLZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/L7IQHNLZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6L8GNRIL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"L7IQHNLZ","version":97,"parentItem":"6L8GNRIL","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:34:32Z","url":"https://arxiv.org/abs/2012.05903","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"be631e6bc7d0cae0aedca938d27235df","mtime":1624206872000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:32Z","dateModified":"2021-06-20T16:34:32Z"}},{"key":"X5SN963F","version":97,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/X5SN963F","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/X5SN963F","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VMJXU2HQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"X5SN963F","version":97,"parentItem":"VMJXU2HQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:34:28Z","url":"https://arxiv.org/pdf/2012.03927.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Srinivasan et al. - 2020 - NeRV Neural Reflectance and Visibility Fields for.pdf","md5":"9c5f3f5a428028df95605c203b764647","mtime":1624206868000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:28Z","dateModified":"2021-06-20T16:34:28Z"}},{"key":"RSBJIS2D","version":97,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RSBJIS2D","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RSBJIS2D","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6L8GNRIL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RSBJIS2D","version":97,"parentItem":"6L8GNRIL","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:34:27Z","url":"https://arxiv.org/pdf/2012.05903.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Gao et al. - 2021 - Portrait Neural Radiance Fields from a Single Imag.pdf","md5":"63ae063fad03015eea282ab524fe9c4e","mtime":1624206867000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:27Z","dateModified":"2021-06-20T16:34:27Z"}},{"key":"JKN7YZ2K","version":86,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JKN7YZ2K","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JKN7YZ2K","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4BWLNEMH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JKN7YZ2K","version":86,"parentItem":"4BWLNEMH","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:34:20Z","url":"https://arxiv.org/abs/2012.05877","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"b1f8492cc0bfd51defc20352090a31c1","mtime":1624206860000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:20Z","dateModified":"2021-06-20T16:34:20Z"}},{"key":"RKWFB855","version":86,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RKWFB855","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RKWFB855","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4BWLNEMH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RKWFB855","version":86,"parentItem":"4BWLNEMH","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:34:15Z","url":"https://arxiv.org/pdf/2012.05877.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf","md5":"95dda2fee9bff40c82e04dadcb3e1bf7","mtime":1624206855000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:15Z","dateModified":"2021-06-20T16:34:15Z"}},{"key":"RWTHDTVR","version":87,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RWTHDTVR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RWTHDTVR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HTAUNQ6X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RWTHDTVR","version":87,"parentItem":"HTAUNQ6X","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:34:11Z","url":"https://arxiv.org/abs/2012.08503","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"ab94d14afdbd719b1a0d4675d23a45ef","mtime":1624206851000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:11Z","dateModified":"2021-06-20T16:34:11Z"}},{"key":"6FP9U3SH","version":87,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6FP9U3SH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6FP9U3SH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HTAUNQ6X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6FP9U3SH","version":87,"parentItem":"HTAUNQ6X","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:34:06Z","url":"https://arxiv.org/pdf/2012.08503.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Guo et al. - 2020 - Object-Centric Neural Scene Rendering.pdf","md5":"c0b98fb0890bca09c1f8eb4d6c8d5c92","mtime":1624206846000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:06Z","dateModified":"2021-06-20T16:34:06Z"}},{"key":"A9QX6V2G","version":87,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A9QX6V2G","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A9QX6V2G","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZT43H7LK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"A9QX6V2G","version":87,"parentItem":"ZT43H7LK","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:34:04Z","url":"https://arxiv.org/abs/2012.03065","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"54ab051e757d16f9703d0e80c877e3fc","mtime":1624206844000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:04Z","dateModified":"2021-06-20T16:34:04Z"}},{"key":"6CJCJ7L9","version":85,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6CJCJ7L9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6CJCJ7L9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5W2FTFLY","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6CJCJ7L9","version":85,"parentItem":"5W2FTFLY","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:34:03Z","url":"https://arxiv.org/abs/2012.03918","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"49768871ac80f311a6cc8ed1bd6879b7","mtime":1624206843000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:03Z","dateModified":"2021-06-20T16:34:03Z"}},{"key":"GBXZSTFV","version":86,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GBXZSTFV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GBXZSTFV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZT43H7LK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GBXZSTFV","version":86,"parentItem":"ZT43H7LK","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:34:00Z","url":"https://arxiv.org/pdf/2012.03065.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Gafni et al. - 2020 - Dynamic Neural Radiance Fields for Monocular 4D Fa.pdf","md5":"86e8837a94d41317a99b3ee2dae88afc","mtime":1624206840000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:34:00Z","dateModified":"2021-06-20T16:34:00Z"}},{"key":"TYXKW6FJ","version":85,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TYXKW6FJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TYXKW6FJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5W2FTFLY","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TYXKW6FJ","version":85,"parentItem":"5W2FTFLY","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:33:58Z","url":"https://arxiv.org/pdf/2012.03918.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Boss et al. - 2021 - NeRD Neural Reflectance Decomposition from Image .pdf","md5":"2eb6f3a36878debec8eee880fd273bfe","mtime":1624206838000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:33:58Z","dateModified":"2021-06-20T16:33:58Z"}},{"key":"7NCX55HR","version":85,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7NCX55HR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7NCX55HR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/43G9UVNV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7NCX55HR","version":85,"parentItem":"43G9UVNV","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:33:54Z","url":"https://arxiv.org/abs/2012.02189","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"a6ffee8a7c044c58808182203409418a","mtime":1624206834000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:33:54Z","dateModified":"2021-06-20T16:33:54Z"}},{"key":"DKGZ2SGG","version":85,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DKGZ2SGG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DKGZ2SGG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/43G9UVNV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DKGZ2SGG","version":85,"parentItem":"43G9UVNV","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:33:49Z","url":"https://arxiv.org/pdf/2012.02189.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tancik et al. - 2021 - Learned Initializations for Optimizing Coordinate-.pdf","md5":"8e0ed6840815c4b991cffd295fd37634","mtime":1624206829000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:33:49Z","dateModified":"2021-06-20T16:33:49Z"}},{"key":"DU438X8W","version":84,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DU438X8W","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DU438X8W","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/93SRFTTC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DU438X8W","version":84,"parentItem":"93SRFTTC","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:33:22Z","url":"https://arxiv.org/abs/2011.13084","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"4bbb2e648d6867efa60bfd3f36f3e465","mtime":1624206802000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:33:22Z","dateModified":"2021-06-20T16:33:22Z"}},{"key":"QFDM3C8K","version":84,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QFDM3C8K","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QFDM3C8K","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/93SRFTTC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QFDM3C8K","version":84,"parentItem":"93SRFTTC","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:33:16Z","url":"https://arxiv.org/pdf/2011.13084.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Li et al. - 2021 - Neural Scene Flow Fields for Space-Time View Synth.pdf","md5":"263367a88fda324ad0d0ac3eea96e4a9","mtime":1624206796000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:33:16Z","dateModified":"2021-06-20T16:33:16Z"}},{"key":"JWTDXZ6N","version":84,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JWTDXZ6N","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JWTDXZ6N","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RXML26ZL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JWTDXZ6N","version":84,"parentItem":"RXML26ZL","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:33:04Z","url":"https://arxiv.org/abs/2011.12950","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"14529fcc8961daf3ef7f7d1f93bd307f","mtime":1624206784000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:33:04Z","dateModified":"2021-06-20T16:33:04Z"}},{"key":"4UPT8VI5","version":84,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4UPT8VI5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4UPT8VI5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RXML26ZL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4UPT8VI5","version":84,"parentItem":"RXML26ZL","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:33:00Z","url":"https://arxiv.org/pdf/2011.12950.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Xian et al. - 2020 - Space-time Neural Irradiance Fields for Free-Viewp.pdf","md5":"7e36ec0f67308a8776a0ad1f5e1a1c6b","mtime":1624206780000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:33:00Z","dateModified":"2021-06-20T16:33:00Z"}},{"key":"SQRCAPHK","version":84,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SQRCAPHK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SQRCAPHK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RF7ID2P5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SQRCAPHK","version":84,"parentItem":"RF7ID2P5","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:32:59Z","url":"https://arxiv.org/abs/2011.13775","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"5b2783c7a6c2f7e99b0adba78e86b52d","mtime":1624206779000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:32:59Z","dateModified":"2021-06-20T16:32:59Z"}},{"key":"3C88KXZU","version":84,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3C88KXZU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3C88KXZU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RF7ID2P5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3C88KXZU","version":84,"parentItem":"RF7ID2P5","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:32:54Z","url":"https://arxiv.org/pdf/2011.13775.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Anokhin et al. - 2020 - Image Generators with Conditionally-Independent Pi.pdf","md5":"924f16fa45221a69fec7439ee827a585","mtime":1624206774000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:32:54Z","dateModified":"2021-06-20T16:32:54Z"}},{"key":"CK9H8CDZ","version":85,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CK9H8CDZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CK9H8CDZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WHVCIMNZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CK9H8CDZ","version":85,"parentItem":"WHVCIMNZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:32:52Z","url":"https://arxiv.org/abs/2012.01714","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"21f292c9bbc7869aa2971c924ba431a6","mtime":1624206772000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:32:52Z","dateModified":"2021-06-20T16:32:52Z"}},{"key":"Z5CX3ARW","version":85,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z5CX3ARW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z5CX3ARW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WHVCIMNZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Z5CX3ARW","version":85,"parentItem":"WHVCIMNZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:32:49Z","url":"https://arxiv.org/pdf/2012.01714.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Lindell et al. - 2021 - AutoInt Automatic Integration for Fast Neural Vol.pdf","md5":"6752db7e3b01a1148eb0ab0ce0be302d","mtime":1624206769000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:32:49Z","dateModified":"2021-06-20T16:32:49Z"}},{"key":"WBFIFN4M","version":84,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WBFIFN4M","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WBFIFN4M","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VSEJ2GUV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WBFIFN4M","version":84,"parentItem":"VSEJ2GUV","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:32:28Z","url":"https://arxiv.org/abs/2011.12948","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"eae2fd70d811af22afa22ecab786b1df","mtime":1624206748000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:32:28Z","dateModified":"2021-06-20T16:32:28Z"}},{"key":"8DYQ5MTT","version":84,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8DYQ5MTT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8DYQ5MTT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VSEJ2GUV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8DYQ5MTT","version":84,"parentItem":"VSEJ2GUV","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:32:23Z","url":"https://arxiv.org/pdf/2011.12948.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Park et al. - 2021 - Nerfies Deformable Neural Radiance Fields.pdf","md5":"af6bb34446947cd12327d0e9fe88f378","mtime":1624206743000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:32:23Z","dateModified":"2021-06-20T16:32:23Z"}},{"key":"XSCT9QIM","version":85,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XSCT9QIM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XSCT9QIM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DQHUP86T","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XSCT9QIM","version":85,"parentItem":"DQHUP86T","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:32:08Z","url":"https://arxiv.org/abs/2012.00926","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"7f91e8ecbefbabcb3471f504f8ac4c31","mtime":1624206728000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:32:08Z","dateModified":"2021-06-20T16:32:08Z"}},{"key":"Q7JXT9WF","version":84,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Q7JXT9WF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Q7JXT9WF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DQHUP86T","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Q7JXT9WF","version":84,"parentItem":"DQHUP86T","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:32:03Z","url":"https://arxiv.org/pdf/2012.00926.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chan et al. - 2021 - pi-GAN Periodic Implicit Generative Adversarial N.pdf","md5":"88c5683a4de5c6af2a769acc256058e8","mtime":1624206723000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:32:03Z","dateModified":"2021-06-20T16:32:03Z"}},{"key":"HP7IYM2L","version":85,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HP7IYM2L","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HP7IYM2L","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/B5TC93EQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HP7IYM2L","version":85,"parentItem":"B5TC93EQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:31:46Z","url":"https://arxiv.org/abs/2011.13961","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"49fdf32814f2f93e1ff35999ceecc25a","mtime":1624206706000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:31:46Z","dateModified":"2021-06-20T16:31:46Z"}},{"key":"7CLV5ZVF","version":85,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7CLV5ZVF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7CLV5ZVF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/B5TC93EQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7CLV5ZVF","version":85,"parentItem":"B5TC93EQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:31:41Z","url":"https://arxiv.org/pdf/2011.13961.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Pumarola et al. - 2020 - D-NeRF Neural Radiance Fields for Dynamic Scenes.pdf","md5":"cc8b5f1825b1edcfdfba2ab8dd366f53","mtime":1624206701000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:31:41Z","dateModified":"2021-06-20T16:31:41Z"}},{"key":"W2SDJE56","version":308,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/W2SDJE56","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/W2SDJE56","type":"text/html"}},"meta":{"creatorSummary":"Sheen et al.","parsedDate":"2001-09","numChildren":1},"data":{"key":"W2SDJE56","version":308,"itemType":"journalArticle","title":"Three-dimensional millimeter-wave imaging for concealed weapon detection","creators":[{"creatorType":"author","firstName":"D.M.","lastName":"Sheen"},{"creatorType":"author","firstName":"D.L.","lastName":"McMakin"},{"creatorType":"author","firstName":"T.E.","lastName":"Hall"}],"abstractNote":"Millimeter-wave imaging techniques and systems have been developed at the Pacific Northwest National Laboratory (PNNL), Richland, WA, for the detection of concealed weapons and contraband at airports and other secure locations. These techniques were derived from microwave holography techniques that utilize phase and amplitude information recorded over a two-dimensional aperture to reconstruct a focused image of the target. Millimeter-wave imaging is well suited for the detection of concealed weapons or other contraband carried on personnel since millimeter-waves are nonionizing, readily penetrate common clothing material, and are reflected from the human body and any concealed items. In this paper, a wide-bandwidth three-dimensional holographic microwave imaging technique is described. Practical weapon detection systems for airport or other high-throughput applications require high-speed scanning on the order of 3 to 10 s. To achieve this goal, a prototype imaging system utilizing a 27–33 GHz linear sequentially switched array and a high-speed linear scanner has been developed and tested. This system is described in detail along with numerous imaging results.","publicationTitle":"IEEE Transactions on Microwave Theory and Techniques","volume":"49","issue":"9","pages":"1581-1592","date":"Sept./2001","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"IEEE Trans. Microwave Theory Techn.","language":"en","DOI":"10.1109/22.942570","ISSN":"00189480","shortTitle":"","url":"http://ieeexplore.ieee.org/document/942570/","accessDate":"2021-06-20T16:29:57Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[{"tag":"star"}],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-20T16:29:57Z","dateModified":"2021-06-20T16:30:45Z"}},{"key":"ZUYG4IUM","version":74,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZUYG4IUM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZUYG4IUM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WHVCIMNZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZUYG4IUM","version":74,"parentItem":"WHVCIMNZ","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-06-20T16:30:38Z","url":"https://arxiv.org/pdf/2012.01714.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Lindell et al. - 2021 - AutoInt Automatic Integration for Fast Neural Vol.pdf","md5":"6752db7e3b01a1148eb0ab0ce0be302d","mtime":1624206638000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:30:38Z","dateModified":"2021-06-20T16:30:38Z"}},{"key":"F4HYKWMU","version":72,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/F4HYKWMU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/F4HYKWMU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BVV9955T","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"F4HYKWMU","version":72,"parentItem":"BVV9955T","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:30:35Z","url":"https://arxiv.org/abs/2011.07233","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"3c0101b4c165319c9c792eaf73188042","mtime":1624206635000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:30:35Z","dateModified":"2021-06-20T16:30:35Z"}},{"key":"RQIHPVC8","version":72,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RQIHPVC8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RQIHPVC8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BVV9955T","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RQIHPVC8","version":72,"parentItem":"BVV9955T","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:30:30Z","url":"https://arxiv.org/pdf/2011.07233.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Riegler and Koltun - 2021 - Stable View Synthesis.pdf","md5":"ae6fecca56af96d4bca958806134b68d","mtime":1624206630000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:30:30Z","dateModified":"2021-06-20T16:30:30Z"}},{"key":"WJS9R2L2","version":72,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WJS9R2L2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WJS9R2L2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XCPDD5N9","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WJS9R2L2","version":72,"parentItem":"XCPDD5N9","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:30:22Z","url":"https://arxiv.org/abs/2011.10007","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"de54fa9d273ea5a59ea29b4a77f81432","mtime":1624206622000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:30:22Z","dateModified":"2021-06-20T16:30:22Z"}},{"key":"P6DY2R6J","version":72,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/P6DY2R6J","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/P6DY2R6J","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XCPDD5N9","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"P6DY2R6J","version":72,"parentItem":"XCPDD5N9","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:30:16Z","url":"https://arxiv.org/pdf/2011.10007.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Li et al. - 2020 - Multi-Plane Program Induction with 3D Box Priors.pdf","md5":"4b983f4683c9de0f6174d58a18361327","mtime":1624206616000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:30:16Z","dateModified":"2021-06-20T16:30:16Z"}},{"key":"I976C3LC","version":308,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I976C3LC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I976C3LC","type":"text/html"}},"meta":{"creatorSummary":"Yanik and Torlak","parsedDate":"2019","numChildren":1},"data":{"key":"I976C3LC","version":308,"itemType":"conferencePaper","title":"Near-Field 2-D SAR Imaging by Millimeter-Wave Radar for Concealed Item Detection","creators":[{"creatorType":"author","firstName":"Muhammet Emin","lastName":"Yanik"},{"creatorType":"author","firstName":"Murat","lastName":"Torlak"}],"abstractNote":"Recent progress in complementary metaloxide semiconductor (CMOS) based frequency-modulated continuous-wave (FMCW) radars has made it possible to design low-cost and low-power millimeter-wave (mmWave) sensors. As a result, there is a strong desire to exploit the progress in mmWave sensors in wide range of imaging applications including medical, automotive, and security. In this paper, we present a low-cost high-resolution mmWave imager prototype that combines commercially available 77 GHz system-on-chip FMCW radar sensors and synthetic aperture radar (SAR) signal processing techniques for concealed item detection. To create a synthetic aperture over a target scene, the imager is constructed with a two-axis motorized rail system which can synthesize a large aperture in both horizontal and vertical directions. Our prototype system is described in detail along with signal processing techniques for two-dimensional (2-D) image reconstruction. The imaging examples of concealed items in various scenarios conﬁrm that our low-cost prototype has a great potential for high-resolution imaging tasks in security applications.","date":"1/2019","proceedingsTitle":"2019 IEEE Radio and Wireless Symposium (RWS)","conferenceName":"2019 IEEE Radio and Wireless Symposium (RWS)","place":"Orlando, FL, USA","publisher":"IEEE","volume":"","pages":"1-4","series":"","language":"en","DOI":"10.1109/RWS.2019.8714552","ISBN":"978-1-5386-5944-1","shortTitle":"","url":"https://ieeexplore.ieee.org/document/8714552/","accessDate":"2021-06-20T16:30:09Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":["QP76V5CN"],"relations":{},"dateAdded":"2021-06-20T16:30:09Z","dateModified":"2021-06-20T16:30:14Z"}},{"key":"2MCHWYYP","version":362,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2MCHWYYP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2MCHWYYP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/86AX9V9R","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2MCHWYYP","version":362,"parentItem":"86AX9V9R","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:30:00Z","url":"https://arxiv.org/abs/2007.11571","note":"","contentType":"text/html","charset":"utf-8","filename":"2007.html","md5":"e03a515cda922e2dca33244b512a3d9d","mtime":1624206600000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/V5S3G9UE"},"dateAdded":"2021-06-20T16:30:00Z","dateModified":"2021-06-20T16:30:00Z"}},{"key":"359EK8W8","version":362,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/359EK8W8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/359EK8W8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/86AX9V9R","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"359EK8W8","version":362,"parentItem":"86AX9V9R","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:29:55Z","url":"https://arxiv.org/pdf/2007.11571.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Liu et al. - 2021 - Neural Sparse Voxel Fields.pdf","md5":"d71c22be1d1ee6b1faaf709f5c4fef62","mtime":1624206595000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/FVZBQIE2"},"dateAdded":"2021-06-20T16:29:55Z","dateModified":"2021-06-20T16:29:55Z"}},{"key":"J3T8VKS9","version":73,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/J3T8VKS9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/J3T8VKS9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZNDAS9TD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"J3T8VKS9","version":73,"parentItem":"ZNDAS9TD","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:29:48Z","url":"https://arxiv.org/abs/2011.12490","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"64c4c5880fd8ac84385f9e06e896dfee","mtime":1624206588000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:29:48Z","dateModified":"2021-06-20T16:29:48Z"}},{"key":"7XJ5MLR7","version":72,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7XJ5MLR7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7XJ5MLR7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZNDAS9TD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7XJ5MLR7","version":72,"parentItem":"ZNDAS9TD","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:29:40Z","url":"https://arxiv.org/pdf/2011.12490.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Rebain et al. - 2020 - DeRF Decomposed Radiance Fields.pdf","md5":"5e2f0cdbba9eb457d66b3707cb82c6bd","mtime":1624206580000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:29:40Z","dateModified":"2021-06-20T16:29:40Z"}},{"key":"PNMXMP4A","version":72,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PNMXMP4A","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PNMXMP4A","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/U962VCXI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PNMXMP4A","version":72,"parentItem":"U962VCXI","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:29:11Z","url":"https://arxiv.org/abs/2010.10505","note":"","contentType":"text/html","charset":"utf-8","filename":"2010.html","md5":"c3a96e61cc8d2eacbd851dc485f46c8d","mtime":1624206551000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:29:11Z","dateModified":"2021-06-20T16:29:11Z"}},{"key":"FLZTN2GG","version":72,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FLZTN2GG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FLZTN2GG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/U962VCXI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FLZTN2GG","version":72,"parentItem":"U962VCXI","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:29:07Z","url":"https://arxiv.org/pdf/2010.10505.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Lin et al. - 2020 - SDF-SRN Learning Signed Distance 3D Object Recons.pdf","md5":"a824d0ddcb586a89aba1ae49bddbca7a","mtime":1624206547000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:29:07Z","dateModified":"2021-06-20T16:29:07Z"}},{"key":"NR2RPN2M","version":70,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NR2RPN2M","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NR2RPN2M","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HSN56RWV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NR2RPN2M","version":70,"parentItem":"HSN56RWV","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:28:52Z","url":"https://arxiv.org/abs/2011.12026","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"658f2ece6dba49f34b23b3ed1dbdfda4","mtime":1624206532000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:28:52Z","dateModified":"2021-06-20T16:28:52Z"}},{"key":"PCFFMVVU","version":70,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PCFFMVVU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PCFFMVVU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HSN56RWV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PCFFMVVU","version":70,"parentItem":"HSN56RWV","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:28:48Z","url":"https://arxiv.org/pdf/2011.12026.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Skorokhodov et al. - 2020 - Adversarial Generation of Continuous Images.pdf","md5":"76edddfb642116de15253322ec1cb476","mtime":1624206528000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:28:48Z","dateModified":"2021-06-20T16:28:48Z"}},{"key":"4XVGFMI8","version":70,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4XVGFMI8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4XVGFMI8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/V5KTHHYD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4XVGFMI8","version":70,"parentItem":"V5KTHHYD","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:28:45Z","url":"https://arxiv.org/abs/2011.12100","note":"","contentType":"text/html","charset":"utf-8","filename":"2011.html","md5":"23a8cfbf1abddbcc738a0d2a73a41dcb","mtime":1624206525000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:28:45Z","dateModified":"2021-06-20T16:28:45Z"}},{"key":"QFE2XQSU","version":70,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QFE2XQSU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QFE2XQSU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/V5KTHHYD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QFE2XQSU","version":70,"parentItem":"V5KTHHYD","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:28:40Z","url":"https://arxiv.org/pdf/2011.12100.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Niemeyer and Geiger - 2021 - GIRAFFE Representing Scenes as Compositional Gene.pdf","md5":"eb5cde6f36bd755be728d2c56d50ff23","mtime":1624206520000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:28:40Z","dateModified":"2021-06-20T16:28:40Z"}},{"key":"WGSCZPRG","version":69,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WGSCZPRG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WGSCZPRG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GXUECHN2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WGSCZPRG","version":69,"parentItem":"GXUECHN2","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:28:21Z","url":"https://arxiv.org/abs/2010.07492","note":"","contentType":"text/html","charset":"utf-8","filename":"2010.html","md5":"91aaf10969852f83ac35b9508536f62d","mtime":1624206501000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:28:21Z","dateModified":"2021-06-20T16:28:21Z"}},{"key":"9NK9V4FM","version":69,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9NK9V4FM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9NK9V4FM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GXUECHN2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9NK9V4FM","version":69,"parentItem":"GXUECHN2","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:28:18Z","url":"https://arxiv.org/pdf/2010.07492.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhang et al. - 2020 - NeRF++ Analyzing and Improving Neural Radiance Fi.pdf","md5":"ed4b0002c220b279212235a27fa9d514","mtime":1624206498000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:28:18Z","dateModified":"2021-06-20T16:28:18Z"}},{"key":"ZP4GB2LQ","version":67,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZP4GB2LQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZP4GB2LQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GXUECHN2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZP4GB2LQ","version":67,"parentItem":"GXUECHN2","itemType":"attachment","linkMode":"imported_url","title":"Full Text","accessDate":"2021-06-20T16:27:46Z","url":"https://arxiv.org/pdf/2010.07492.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhang et al. - 2020 - NeRF++ Analyzing and Improving Neural Radiance Fi.pdf","md5":"ed4b0002c220b279212235a27fa9d514","mtime":1624206466000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:27:46Z","dateModified":"2021-06-20T16:27:46Z"}},{"key":"VQ2WVHEK","version":67,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VQ2WVHEK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VQ2WVHEK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/7LPDVVES","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VQ2WVHEK","version":67,"parentItem":"7LPDVVES","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:27:42Z","url":"https://arxiv.org/abs/2010.13938","note":"","contentType":"text/html","charset":"utf-8","filename":"2010.html","md5":"6cce1538da32bcae8f3e7bf5ab721446","mtime":1624206462000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:27:42Z","dateModified":"2021-06-20T16:27:42Z"}},{"key":"FESL9WN8","version":67,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FESL9WN8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FESL9WN8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/F86LTWEM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FESL9WN8","version":67,"parentItem":"F86LTWEM","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:27:38Z","url":"https://arxiv.org/abs/2008.06534","note":"","contentType":"text/html","charset":"utf-8","filename":"2008.html","md5":"330d00295883a9eea12288ed2bc36264","mtime":1624206458000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:27:38Z","dateModified":"2021-06-20T16:27:38Z"}},{"key":"UNCADFI9","version":67,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UNCADFI9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UNCADFI9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/7LPDVVES","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UNCADFI9","version":67,"parentItem":"7LPDVVES","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:27:38Z","url":"https://arxiv.org/pdf/2010.13938.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chibane et al. - 2020 - Neural Unsigned Distance Fields for Implicit Funct.pdf","md5":"c9cb86376009c76e2334d62b0d50112a","mtime":1624206458000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:27:38Z","dateModified":"2021-06-20T16:27:38Z"}},{"key":"6T75DCMG","version":67,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6T75DCMG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6T75DCMG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/F86LTWEM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6T75DCMG","version":67,"parentItem":"F86LTWEM","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:27:33Z","url":"https://arxiv.org/pdf/2008.06534.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Attal et al. - 2020 - MatryODShka Real-time 6DoF Video View Synthesis u.pdf","md5":"666c765e83ac29d290b5987b22c7a1d6","mtime":1624206453000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:27:33Z","dateModified":"2021-06-20T16:27:33Z"}},{"key":"MM5XRJKA","version":67,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MM5XRJKA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MM5XRJKA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VR6DWTNT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MM5XRJKA","version":67,"parentItem":"VR6DWTNT","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:27:20Z","url":"https://arxiv.org/abs/2008.02268","note":"","contentType":"text/html","charset":"utf-8","filename":"2008.html","md5":"ba2475b23a81239fb56cb1943977e040","mtime":1624206440000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:27:20Z","dateModified":"2021-06-20T16:27:20Z"}},{"key":"GSDX6EA9","version":67,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GSDX6EA9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GSDX6EA9","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VR6DWTNT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GSDX6EA9","version":67,"parentItem":"VR6DWTNT","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:27:15Z","url":"https://arxiv.org/pdf/2008.02268.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Martin-Brualla et al. - 2021 - NeRF in the Wild Neural Radiance Fields for Uncon.pdf","md5":"22f9fd8b04ebabe2d892c1ff3711502c","mtime":1624206435000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:27:15Z","dateModified":"2021-06-20T16:27:15Z"}},{"key":"LDNSNC59","version":649,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LDNSNC59","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LDNSNC59","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/5H9I5CSF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LDNSNC59","version":649,"parentItem":"5H9I5CSF","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:26:20Z","url":"https://arxiv.org/abs/2006.11239","note":"","contentType":"text/html","charset":"utf-8","filename":"2006.html","md5":"4d3c41db05eeade9f915ddb4314f574b","mtime":1624206380000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/5WT6MPRH","http://zotero.org/groups/4458581/items/GL6DKCQT"]},"dateAdded":"2021-06-20T16:26:20Z","dateModified":"2021-06-20T16:26:20Z"}},{"key":"A96VX9MJ","version":359,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/A96VX9MJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/A96VX9MJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PP3NI8CA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"A96VX9MJ","version":359,"parentItem":"PP3NI8CA","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:26:17Z","url":"https://arxiv.org/abs/2006.10739","note":"","contentType":"text/html","charset":"utf-8","filename":"2006.html","md5":"f0706fbdc5596a75686a7f9d811cff47","mtime":1624206377000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/NKWL9E6R"},"dateAdded":"2021-06-20T16:26:17Z","dateModified":"2021-06-20T16:26:17Z"}},{"key":"KJ672B4G","version":359,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KJ672B4G","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KJ672B4G","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PP3NI8CA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KJ672B4G","version":359,"parentItem":"PP3NI8CA","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:26:13Z","url":"https://arxiv.org/pdf/2006.10739.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tancik et al. - 2020 - Fourier Features Let Networks Learn High Frequency.pdf","md5":"484216db4d5573cbcc9852a4ed313e8f","mtime":1624206373000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/77CZ2ZND"},"dateAdded":"2021-06-20T16:26:13Z","dateModified":"2021-06-20T16:26:13Z"}},{"key":"LFAUS283","version":64,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LFAUS283","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LFAUS283","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UPA65XTD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LFAUS283","version":64,"parentItem":"UPA65XTD","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:26:11Z","url":"https://arxiv.org/abs/2006.09662","note":"","contentType":"text/html","charset":"utf-8","filename":"2006.html","md5":"e6996cd32c4f848abd5d4ee1f2f7f476","mtime":1624206371000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:26:11Z","dateModified":"2021-06-20T16:26:11Z"}},{"key":"YLV65RTQ","version":64,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YLV65RTQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YLV65RTQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VYS6ZLXT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YLV65RTQ","version":64,"parentItem":"VYS6ZLXT","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:26:10Z","url":"https://arxiv.org/abs/2003.08934","note":"","contentType":"text/html","charset":"utf-8","filename":"2003.html","md5":"c7937151a4b643410381abbd501df080","mtime":1624206370000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:26:10Z","dateModified":"2021-06-20T16:26:10Z"}},{"key":"I756D5CA","version":64,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/I756D5CA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/I756D5CA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UPA65XTD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"I756D5CA","version":64,"parentItem":"UPA65XTD","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:26:07Z","url":"https://arxiv.org/pdf/2006.09662.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Sitzmann et al. - 2020 - MetaSDF Meta-learning Signed Distance Functions.pdf","md5":"a642e30c328019bda91af4517e7add82","mtime":1624206367000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:26:07Z","dateModified":"2021-06-20T16:26:07Z"}},{"key":"SLRP3HIB","version":64,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SLRP3HIB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SLRP3HIB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VYS6ZLXT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SLRP3HIB","version":64,"parentItem":"VYS6ZLXT","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:26:03Z","url":"https://arxiv.org/pdf/2003.08934.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf","md5":"ed4df06e919cae0e638015fa78d935eb","mtime":1624206363000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:26:03Z","dateModified":"2021-06-20T16:26:03Z"}},{"key":"NWYZGQLH","version":362,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NWYZGQLH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NWYZGQLH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WSXPLJSX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NWYZGQLH","version":362,"parentItem":"WSXPLJSX","itemType":"note","note":"Comment: CVPR 2021","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/H4TLHTKP"},"dateAdded":"2021-06-20T16:03:55Z","dateModified":"2021-06-20T16:24:59Z"}},{"key":"WSXPLJSX","version":362,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WSXPLJSX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WSXPLJSX","type":"text/html"},"attachment":{"href":"https://api.zotero.org/users/7902311/items/F9DCKB7B","type":"application/json","attachmentType":"application/pdf","attachmentSize":6493563}},"meta":{"creatorSummary":"Yu et al.","parsedDate":"2020-12-03","numChildren":3},"data":{"key":"WSXPLJSX","version":362,"itemType":"journalArticle","title":"pixelNeRF: Neural Radiance Fields from One or Few Images","creators":[{"creatorType":"author","firstName":"Alex","lastName":"Yu"},{"creatorType":"author","firstName":"Vickie","lastName":"Ye"},{"creatorType":"author","firstName":"Matthew","lastName":"Tancik"},{"creatorType":"author","firstName":"Angjoo","lastName":"Kanazawa"}],"abstractNote":"We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf","publicationTitle":"arXiv:2012.02190 [cs]","volume":"","issue":"","pages":"","date":"2020-12-03","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"pixelNeRF","url":"http://arxiv.org/abs/2012.02190","accessDate":"2021-04-26T04:31:14Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.02190","tags":[],"collections":["CPYKW3PF"],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/PBCRUD8V","owl:sameAs":"http://zotero.org/groups/4320173/items/EGRHI42D"},"dateAdded":"2021-04-26T04:31:14Z","dateModified":"2021-06-20T16:24:59Z"}},{"key":"KSTKXVLX","version":77,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KSTKXVLX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KSTKXVLX","type":"text/html"},"attachment":{"href":"https://api.zotero.org/users/7902311/items/2JA6F8X6","type":"application/json","attachmentType":"application/pdf","attachmentSize":24114521}},"meta":{"creatorSummary":"Trevithick and Yang","parsedDate":"2020-11-29","numChildren":4},"data":{"key":"KSTKXVLX","version":77,"itemType":"journalArticle","title":"GRF: Learning a General Radiance Field for 3D Scene Representation and Rendering","creators":[{"creatorType":"author","firstName":"Alex","lastName":"Trevithick"},{"creatorType":"author","firstName":"Bo","lastName":"Yang"}],"abstractNote":"We present a simple yet powerful implicit neural function that can represent and render arbitrarily complex 3D scenes in a single network only from 2D observations. The function models 3D scenes as a general radiance field, which takes a set of posed 2D images with camera poses and intrinsics as input, constructs an internal representation for each 3D point of the scene, and renders the corresponding appearance and geometry of any 3D point viewing from an arbitrary angle. The key to our approach is to explicitly integrate the principle of multi-view geometry to obtain the internal representations from observed 2D views, such that the learned implicit representations empirically remain multi-view consistent. In addition, we introduce an effective neural module to learn general features for each pixel in 2D images, allowing the constructed internal 3D representations to be general as well. Extensive experiments demonstrate the superiority of our approach.","publicationTitle":"arXiv:2010.04595 [cs]","volume":"","issue":"","pages":"","date":"2020-11-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"GRF","url":"http://arxiv.org/abs/2010.04595","accessDate":"2021-06-07T09:23:49Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2010.04595","tags":[],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/Q8H2Z875"},"dateAdded":"2021-06-07T09:23:49Z","dateModified":"2021-06-20T16:24:58Z"}},{"key":"JW2UB4TN","version":56,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JW2UB4TN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JW2UB4TN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KSTKXVLX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JW2UB4TN","version":56,"parentItem":"KSTKXVLX","itemType":"note","note":"Comment: Code and data are available at: https://github.com/alextrevithick/GRF","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:47Z","dateModified":"2021-06-20T16:24:58Z"}},{"key":"ZN4Q97I6","version":94,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZN4Q97I6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZN4Q97I6","type":"text/html"}},"meta":{"creatorSummary":"Sohl-Dickstein et al.","parsedDate":"2015-11-18","numChildren":1},"data":{"key":"ZN4Q97I6","version":94,"itemType":"journalArticle","title":"Deep Unsupervised Learning using Nonequilibrium Thermodynamics","creators":[{"creatorType":"author","firstName":"Jascha","lastName":"Sohl-Dickstein"},{"creatorType":"author","firstName":"Eric A.","lastName":"Weiss"},{"creatorType":"author","firstName":"Niru","lastName":"Maheswaranathan"},{"creatorType":"author","firstName":"Surya","lastName":"Ganguli"}],"abstractNote":"A central problem in machine learning involves modeling complex data-sets using highly ﬂexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both ﬂexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly ﬂexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.","publicationTitle":"arXiv:1503.03585 [cond-mat, q-bio, stat]","volume":"","issue":"","pages":"","date":"2015-11-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1503.03585","accessDate":"2021-06-20T14:32:45Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1503.03585","tags":[],"collections":["TNWL7M5C"],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/UBPKFFDI"},"dateAdded":"2021-06-20T14:32:45Z","dateModified":"2021-06-20T16:24:57Z"}},{"key":"4Y6N7I5E","version":64,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4Y6N7I5E","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4Y6N7I5E","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/865FJ5C2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4Y6N7I5E","version":64,"parentItem":"865FJ5C2","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:24:29Z","url":"https://arxiv.org/abs/2006.09661","note":"","contentType":"text/html","charset":"utf-8","filename":"2006.html","md5":"0a1598ce31d9dbf71abd928b94988c1c","mtime":1624206269000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:24:29Z","dateModified":"2021-06-20T16:24:29Z"}},{"key":"T5R4WXKK","version":64,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/T5R4WXKK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/T5R4WXKK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/865FJ5C2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"T5R4WXKK","version":64,"parentItem":"865FJ5C2","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-20T16:24:18Z","url":"https://arxiv.org/pdf/2006.09661.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Acti.pdf","md5":"2f23581f133c3af23338a4fcae7c7691","mtime":1624206258000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:24:18Z","dateModified":"2021-06-20T16:24:18Z"}},{"key":"K2X9V4A7","version":62,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/K2X9V4A7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/K2X9V4A7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S8D7ZN3X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"K2X9V4A7","version":62,"parentItem":"S8D7ZN3X","itemType":"note","note":"<p>Comment: 10 pages, 8 figures</p>","tags":[],"relations":{},"dateAdded":"2021-06-20T16:22:50Z","dateModified":"2021-06-20T16:22:58Z"}},{"key":"S8D7ZN3X","version":96,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/S8D7ZN3X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/S8D7ZN3X","type":"text/html"}},"meta":{"creatorSummary":"Sasaki et al.","parsedDate":"2021-04-12","numChildren":3},"data":{"key":"S8D7ZN3X","version":96,"itemType":"journalArticle","title":"UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models","creators":[{"creatorType":"author","firstName":"Hiroshi","lastName":"Sasaki"},{"creatorType":"author","firstName":"Chris G.","lastName":"Willcocks"},{"creatorType":"author","firstName":"Toby P.","lastName":"Breckon"}],"abstractNote":"We propose a novel unpaired image-to-image translation method that uses denoising diffusion probabilistic models without requiring adversarial training. Our method, UNpaired Image Translation with Denoising Diffusion Probabilistic Models (UNIT-DDPM), trains a generative model to infer the joint distribution of images over both domains as a Markov chain by minimising a denoising score matching objective conditioned on the other domain. In particular, we update both domain translation models simultaneously, and we generate target domain images by a denoising Markov Chain Monte Carlo approach that is conditioned on the input source domain images, based on Langevin dynamics. Our approach provides stable model training for image-to-image translation and generates high-quality image outputs. This enables state-of-the-art Fr\\'echet Inception Distance (FID) performance on several public datasets, including both colour and multispectral imagery, significantly outperforming the contemporary adversarial image-to-image translation methods.","publicationTitle":"arXiv:2104.05358 [cs, eess]","volume":"","issue":"","pages":"","date":"2021-04-12","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"UNIT-DDPM","url":"http://arxiv.org/abs/2104.05358","accessDate":"2021-06-20T16:22:50Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.05358","tags":[],"collections":["TNWL7M5C"],"relations":{},"dateAdded":"2021-06-20T16:22:50Z","dateModified":"2021-06-20T16:22:50Z"}},{"key":"2EQX4IT2","version":59,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2EQX4IT2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2EQX4IT2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GXUECHN2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2EQX4IT2","version":59,"parentItem":"GXUECHN2","itemType":"note","note":"<p>Comment: Code is available at https://github.com/Kai-46/nerfplusplus; fix a minor formatting issue in Fig. 4</p>","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:46Z","dateModified":"2021-06-20T16:19:51Z"}},{"key":"GWA4L4XB","version":64,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GWA4L4XB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GWA4L4XB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Z9RMBP5X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GWA4L4XB","version":64,"parentItem":"Z9RMBP5X","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-20T16:18:44Z","url":"https://arxiv.org/abs/2003.09852","note":"","contentType":"text/html","charset":"utf-8","filename":"2003.html","md5":"f203f4b65ab8d57361803f7ec85d398f","mtime":1624205924000,"tags":[],"relations":{},"dateAdded":"2021-06-20T16:18:44Z","dateModified":"2021-06-20T16:18:44Z"}},{"key":"CXXN7MIC","version":60,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CXXN7MIC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CXXN7MIC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/RXML26ZL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CXXN7MIC","version":60,"parentItem":"RXML26ZL","itemType":"note","note":"<p>Comment: Project website: https://video-nerf.github.io/</p>","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:51Z","dateModified":"2021-06-20T16:05:19Z"}},{"key":"EPINPR2Q","version":60,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EPINPR2Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EPINPR2Q","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/EQVWR6WG","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EPINPR2Q","version":60,"parentItem":"EQVWR6WG","itemType":"note","note":"<p>Comment: paper with appendix, project page: https://nbei.github.io/gan-pos-encoding.html</p>","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:55Z","dateModified":"2021-06-20T16:05:14Z"}},{"key":"KPCRXSAV","version":60,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KPCRXSAV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KPCRXSAV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4BWLNEMH","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KPCRXSAV","version":60,"parentItem":"4BWLNEMH","itemType":"note","note":"<p>Comment: Website: http://yenchenlin.me/inerf/</p>","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:55Z","dateModified":"2021-06-20T16:05:11Z"}},{"key":"BITURSRB","version":649,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BITURSRB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BITURSRB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/SEIXRRIR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BITURSRB","version":649,"parentItem":"SEIXRRIR","itemType":"note","note":"Comment: Added compute requirements, ImageNet 256$\\times$256 upsampling FID and samples, DDIM guided sampler, fixed typos","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/TLXM5843"},"dateAdded":"2021-06-20T16:04:05Z","dateModified":"2021-06-20T16:04:05Z"}},{"key":"SEIXRRIR","version":648,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SEIXRRIR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SEIXRRIR","type":"text/html"}},"meta":{"creatorSummary":"Dhariwal and Nichol","parsedDate":"2021-06-01","numChildren":3},"data":{"key":"SEIXRRIR","version":648,"itemType":"journalArticle","title":"Diffusion Models Beat GANs on Image Synthesis","creators":[{"creatorType":"author","firstName":"Prafulla","lastName":"Dhariwal"},{"creatorType":"author","firstName":"Alex","lastName":"Nichol"}],"abstractNote":"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion","publicationTitle":"arXiv:2105.05233 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-06-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2105.05233","accessDate":"2021-06-20T16:04:05Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2105.05233","tags":[],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":"http://zotero.org/groups/4458581/items/GAI6KKPA"},"dateAdded":"2021-06-20T16:04:05Z","dateModified":"2021-06-20T16:04:05Z"}},{"key":"HMU9Q54J","version":361,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HMU9Q54J","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HMU9Q54J","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/74A95424","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HMU9Q54J","version":361,"parentItem":"74A95424","itemType":"note","note":"Comment: Project page: https://nerf.live","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/QQTN3DCI"},"dateAdded":"2021-06-20T16:04:03Z","dateModified":"2021-06-20T16:04:03Z"}},{"key":"74A95424","version":360,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/74A95424","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/74A95424","type":"text/html"}},"meta":{"creatorSummary":"Hedman et al.","parsedDate":"2021-03-26","numChildren":3},"data":{"key":"74A95424","version":360,"itemType":"journalArticle","title":"Baking Neural Radiance Fields for Real-Time View Synthesis","creators":[{"creatorType":"author","firstName":"Peter","lastName":"Hedman"},{"creatorType":"author","firstName":"Pratul P.","lastName":"Srinivasan"},{"creatorType":"author","firstName":"Ben","lastName":"Mildenhall"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Paul","lastName":"Debevec"}],"abstractNote":"Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.","publicationTitle":"arXiv:2103.14645 [cs]","volume":"","issue":"","pages":"","date":"2021-03-26","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2103.14645","accessDate":"2021-06-20T16:04:03Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.14645","tags":[],"collections":["8CLBFGR2","CPYKW3PF"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/8YL49DGT"},"dateAdded":"2021-06-20T16:04:03Z","dateModified":"2021-06-20T16:04:03Z"}},{"key":"MCKXIXQG","version":198,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MCKXIXQG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MCKXIXQG","type":"text/html"}},"meta":{"creatorSummary":"DeVries et al.","parsedDate":"2021-04-01","numChildren":2},"data":{"key":"MCKXIXQG","version":198,"itemType":"journalArticle","title":"Unconstrained Scene Generation with Locally Conditioned Radiance Fields","creators":[{"creatorType":"author","firstName":"Terrance","lastName":"DeVries"},{"creatorType":"author","firstName":"Miguel Angel","lastName":"Bautista"},{"creatorType":"author","firstName":"Nitish","lastName":"Srivastava"},{"creatorType":"author","firstName":"Graham W.","lastName":"Taylor"},{"creatorType":"author","firstName":"Joshua M.","lastName":"Susskind"}],"abstractNote":"We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.","publicationTitle":"arXiv:2104.00670 [cs]","volume":"","issue":"","pages":"","date":"2021-04-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2104.00670","accessDate":"2021-06-20T16:04:03Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.00670","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:04:03Z","dateModified":"2021-06-20T16:04:03Z"}},{"key":"EDN42UE6","version":96,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EDN42UE6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EDN42UE6","type":"text/html"}},"meta":{"creatorSummary":"Chen et al.","parsedDate":"2021-03-29","numChildren":2},"data":{"key":"EDN42UE6","version":96,"itemType":"journalArticle","title":"MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo","creators":[{"creatorType":"author","firstName":"Anpei","lastName":"Chen"},{"creatorType":"author","firstName":"Zexiang","lastName":"Xu"},{"creatorType":"author","firstName":"Fuqiang","lastName":"Zhao"},{"creatorType":"author","firstName":"Xiaoshuai","lastName":"Zhang"},{"creatorType":"author","firstName":"Fanbo","lastName":"Xiang"},{"creatorType":"author","firstName":"Jingyi","lastName":"Yu"},{"creatorType":"author","firstName":"Hao","lastName":"Su"}],"abstractNote":"We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.","publicationTitle":"arXiv:2103.15595 [cs]","volume":"","issue":"","pages":"","date":"2021-03-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"MVSNeRF","url":"http://arxiv.org/abs/2103.15595","accessDate":"2021-06-20T16:04:03Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.15595","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:04:03Z","dateModified":"2021-06-20T16:04:03Z"}},{"key":"BKNP8W5V","version":82,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BKNP8W5V","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BKNP8W5V","type":"text/html"}},"meta":{"creatorSummary":"Sucar et al.","parsedDate":"2021-03-23","numChildren":2},"data":{"key":"BKNP8W5V","version":82,"itemType":"journalArticle","title":"iMAP: Implicit Mapping and Positioning in Real-Time","creators":[{"creatorType":"author","firstName":"Edgar","lastName":"Sucar"},{"creatorType":"author","firstName":"Shikun","lastName":"Liu"},{"creatorType":"author","firstName":"Joseph","lastName":"Ortiz"},{"creatorType":"author","firstName":"Andrew J.","lastName":"Davison"}],"abstractNote":"We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking. Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.","publicationTitle":"arXiv:2103.12352 [cs]","volume":"","issue":"","pages":"","date":"2021-03-23","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"iMAP","url":"http://arxiv.org/abs/2103.12352","accessDate":"2021-06-20T16:04:01Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.12352","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:04:02Z","dateModified":"2021-06-20T16:04:02Z"}},{"key":"YABERY8I","version":360,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YABERY8I","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YABERY8I","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/J9YJJ2H5","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YABERY8I","version":360,"parentItem":"J9YJJ2H5","itemType":"note","note":"Comment: main paper: 10 pages, 6 figures; supplementary: 10 pages, 17 figures","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/X6U3WULV"},"dateAdded":"2021-06-20T16:04:01Z","dateModified":"2021-06-20T16:04:01Z"}},{"key":"J9YJJ2H5","version":360,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/J9YJJ2H5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/J9YJJ2H5","type":"text/html"}},"meta":{"creatorSummary":"Garbin et al.","parsedDate":"2021-04-15","numChildren":3},"data":{"key":"J9YJJ2H5","version":360,"itemType":"journalArticle","title":"FastNeRF: High-Fidelity Neural Rendering at 200FPS","creators":[{"creatorType":"author","firstName":"Stephan J.","lastName":"Garbin"},{"creatorType":"author","firstName":"Marek","lastName":"Kowalski"},{"creatorType":"author","firstName":"Matthew","lastName":"Johnson"},{"creatorType":"author","firstName":"Jamie","lastName":"Shotton"},{"creatorType":"author","firstName":"Julien","lastName":"Valentin"}],"abstractNote":"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.","publicationTitle":"arXiv:2103.10380 [cs]","volume":"","issue":"","pages":"","date":"2021-04-15","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"FastNeRF","url":"http://arxiv.org/abs/2103.10380","accessDate":"2021-06-20T16:04:01Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.10380","tags":[],"collections":["8CLBFGR2","CPYKW3PF"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/XLVM7R6C"},"dateAdded":"2021-06-20T16:04:01Z","dateModified":"2021-06-20T16:04:01Z"}},{"key":"Q3DXFQ7X","version":96,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Q3DXFQ7X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Q3DXFQ7X","type":"text/html"}},"meta":{"creatorSummary":"Neff et al.","parsedDate":"2021-05-11","numChildren":3},"data":{"key":"Q3DXFQ7X","version":96,"itemType":"journalArticle","title":"DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks","creators":[{"creatorType":"author","firstName":"Thomas","lastName":"Neff"},{"creatorType":"author","firstName":"Pascal","lastName":"Stadlbauer"},{"creatorType":"author","firstName":"Mathias","lastName":"Parger"},{"creatorType":"author","firstName":"Andreas","lastName":"Kurz"},{"creatorType":"author","firstName":"Joerg H.","lastName":"Mueller"},{"creatorType":"author","firstName":"Chakravarty R. Alla","lastName":"Chaitanya"},{"creatorType":"author","firstName":"Anton","lastName":"Kaplanyan"},{"creatorType":"author","firstName":"Markus","lastName":"Steinberger"}],"abstractNote":"The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in neural networks. However, one major limitation preventing the use of NeRF in interactive and real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS when aiming for real-time rendering on consumer hardware. In this work, we take a step towards bringing neural representations closer to practical rendering of synthetic content in interactive and real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when local samples are placed around surfaces in the scene. To this end, we propose a depth oracle network, which predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, a dual network design with a depth oracle network as a first step and a locally sampled shading network for ray accumulation. With our design, we reduce the inference costs by up to 48x compared to NeRF. Using an off-the-shelf inference API in combination with simple compute kernels, we are the first to render raymarching-based neural representations at interactive frame rates (15 frames per second at 800x800) on a single GPU. At the same time, since we focus on the important parts of the scene around surfaces, we achieve equal or better quality compared to NeRF to enable interactive high-quality rendering.","publicationTitle":"arXiv:2103.03231 [cs]","volume":"","issue":"","pages":"","date":"2021-05-11","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"DONeRF","url":"http://arxiv.org/abs/2103.03231","accessDate":"2021-06-20T16:04:01Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.03231","tags":[],"collections":["8CLBFGR2","CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:04:01Z","dateModified":"2021-06-20T16:04:01Z"}},{"key":"SHWZ65JX","version":62,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SHWZ65JX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SHWZ65JX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/Q3DXFQ7X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SHWZ65JX","version":62,"parentItem":"Q3DXFQ7X","itemType":"note","note":"Comment: Project website: https://depthoraclenerf.github.io/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:04:01Z","dateModified":"2021-06-20T16:04:01Z"}},{"key":"PDM4ZUQK","version":358,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PDM4ZUQK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PDM4ZUQK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DK93ZEF2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PDM4ZUQK","version":358,"parentItem":"DK93ZEF2","itemType":"note","note":"Comment: CVPR 2021 (Oral)","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/CVDUFNNZ"},"dateAdded":"2021-06-20T16:04:00Z","dateModified":"2021-06-20T16:04:00Z"}},{"key":"QNXL2EIJ","version":82,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QNXL2EIJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QNXL2EIJ","type":"text/html"}},"meta":{"creatorSummary":"Lombardi et al.","parsedDate":"2021-05-06","numChildren":3},"data":{"key":"QNXL2EIJ","version":82,"itemType":"journalArticle","title":"Mixture of Volumetric Primitives for Efficient Neural Rendering","creators":[{"creatorType":"author","firstName":"Stephen","lastName":"Lombardi"},{"creatorType":"author","firstName":"Tomas","lastName":"Simon"},{"creatorType":"author","firstName":"Gabriel","lastName":"Schwartz"},{"creatorType":"author","firstName":"Michael","lastName":"Zollhoefer"},{"creatorType":"author","firstName":"Yaser","lastName":"Sheikh"},{"creatorType":"author","firstName":"Jason","lastName":"Saragih"}],"abstractNote":"Real-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a deconvolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance.","publicationTitle":"arXiv:2103.01954 [cs]","volume":"","issue":"","pages":"","date":"2021-05-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2103.01954","accessDate":"2021-06-20T16:04:00Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.01954","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:04:00Z","dateModified":"2021-06-20T16:04:00Z"}},{"key":"N9TACYGJ","version":82,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/N9TACYGJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/N9TACYGJ","type":"text/html"}},"meta":{"creatorSummary":"Xiang et al.","parsedDate":"2021-03-01","numChildren":2},"data":{"key":"N9TACYGJ","version":82,"itemType":"journalArticle","title":"NeuTex: Neural Texture Mapping for Volumetric Neural Rendering","creators":[{"creatorType":"author","firstName":"Fanbo","lastName":"Xiang"},{"creatorType":"author","firstName":"Zexiang","lastName":"Xu"},{"creatorType":"author","firstName":"Miloš","lastName":"Hašan"},{"creatorType":"author","firstName":"Yannick","lastName":"Hold-Geoffroy"},{"creatorType":"author","firstName":"Kalyan","lastName":"Sunkavalli"},{"creatorType":"author","firstName":"Hao","lastName":"Su"}],"abstractNote":"Recent work has demonstrated that volumetric scene representations combined with differentiable volume rendering can enable photo-realistic rendering for challenging scenes that mesh reconstruction fails on. However, these methods entangle geometry and appearance in a \"black-box\" volume that cannot be edited. Instead, we present an approach that explicitly disentangles geometry--represented as a continuous 3D volume--from appearance--represented as a continuous 2D texture map. We achieve this by introducing a 3D-to-2D texture mapping (or surface parameterization) network into volumetric representations. We constrain this texture mapping network using an additional 2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D surface points map to 2D texture points that map back to the original 3D points. We demonstrate that this representation can be reconstructed using only multi-view image supervision and generates high-quality rendering results. More importantly, by separating geometry and texture, we allow users to edit appearance by simply editing 2D texture maps.","publicationTitle":"arXiv:2103.00762 [cs]","volume":"","issue":"","pages":"","date":"2021-03-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"NeuTex","url":"http://arxiv.org/abs/2103.00762","accessDate":"2021-06-20T16:04:00Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.00762","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:04:00Z","dateModified":"2021-06-20T16:04:00Z"}},{"key":"BE56GYH3","version":82,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BE56GYH3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BE56GYH3","type":"text/html"}},"meta":{"creatorSummary":"Wang et al.","parsedDate":"2021-04-06","numChildren":3},"data":{"key":"BE56GYH3","version":82,"itemType":"journalArticle","title":"IBRNet: Learning Multi-View Image-Based Rendering","creators":[{"creatorType":"author","firstName":"Qianqian","lastName":"Wang"},{"creatorType":"author","firstName":"Zhicheng","lastName":"Wang"},{"creatorType":"author","firstName":"Kyle","lastName":"Genova"},{"creatorType":"author","firstName":"Pratul","lastName":"Srinivasan"},{"creatorType":"author","firstName":"Howard","lastName":"Zhou"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Ricardo","lastName":"Martin-Brualla"},{"creatorType":"author","firstName":"Noah","lastName":"Snavely"},{"creatorType":"author","firstName":"Thomas","lastName":"Funkhouser"}],"abstractNote":"We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods. Project page: https://ibrnet.github.io/","publicationTitle":"arXiv:2102.13090 [cs]","volume":"","issue":"","pages":"","date":"2021-04-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"IBRNet","url":"http://arxiv.org/abs/2102.13090","accessDate":"2021-06-20T16:03:59Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2102.13090","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:04:00Z","dateModified":"2021-06-20T16:04:00Z"}},{"key":"RBLYDM6P","version":61,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RBLYDM6P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RBLYDM6P","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QNXL2EIJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RBLYDM6P","version":61,"parentItem":"QNXL2EIJ","itemType":"note","note":"Comment: 13 pages; SIGGRAPH 2021","tags":[],"relations":{},"dateAdded":"2021-06-20T16:04:00Z","dateModified":"2021-06-20T16:04:00Z"}},{"key":"4BBHY7ZG","version":61,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4BBHY7ZG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4BBHY7ZG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BE56GYH3","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4BBHY7ZG","version":61,"parentItem":"BE56GYH3","itemType":"note","note":"Comment: CVPR 2021. Project page: https://ibrnet.github.io/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:04:00Z","dateModified":"2021-06-20T16:04:00Z"}},{"key":"VDKL49H5","version":81,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VDKL49H5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VDKL49H5","type":"text/html"}},"meta":{"creatorSummary":"Takikawa et al.","parsedDate":"2021-01-26","numChildren":2},"data":{"key":"VDKL49H5","version":81,"itemType":"journalArticle","title":"Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes","creators":[{"creatorType":"author","firstName":"Towaki","lastName":"Takikawa"},{"creatorType":"author","firstName":"Joey","lastName":"Litalien"},{"creatorType":"author","firstName":"Kangxue","lastName":"Yin"},{"creatorType":"author","firstName":"Karsten","lastName":"Kreis"},{"creatorType":"author","firstName":"Charles","lastName":"Loop"},{"creatorType":"author","firstName":"Derek","lastName":"Nowrouzezahrai"},{"creatorType":"author","firstName":"Alec","lastName":"Jacobson"},{"creatorType":"author","firstName":"Morgan","lastName":"McGuire"},{"creatorType":"author","firstName":"Sanja","lastName":"Fidler"}],"abstractNote":"Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.","publicationTitle":"arXiv:2101.10994 [cs]","volume":"","issue":"","pages":"","date":"2021-01-26","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Neural Geometric Level of Detail","url":"http://arxiv.org/abs/2101.10994","accessDate":"2021-06-20T16:03:58Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2101.10994","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:59Z","dateModified":"2021-06-20T16:03:59Z"}},{"key":"UQPB3TH8","version":81,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UQPB3TH8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UQPB3TH8","type":"text/html"}},"meta":{"creatorSummary":"Rematas et al.","parsedDate":"2021-02-17","numChildren":3},"data":{"key":"UQPB3TH8","version":81,"itemType":"journalArticle","title":"ShaRF: Shape-conditioned Radiance Fields from a Single View","creators":[{"creatorType":"author","firstName":"Konstantinos","lastName":"Rematas"},{"creatorType":"author","firstName":"Ricardo","lastName":"Martin-Brualla"},{"creatorType":"author","firstName":"Vittorio","lastName":"Ferrari"}],"abstractNote":"We present a method for estimating neural scenes representations of objects given only a single image. The core of our method is the estimation of a geometric scaffold for the object and its use as a guide for the reconstruction of the underlying radiance field. Our formulation is based on a generative process that first maps a latent code to a voxelized shape, and then renders it to an image, with the object appearance being controlled by a second latent code. During inference, we optimize both the latent codes and the networks to fit a test image of a new object. The explicit disentanglement of shape and appearance allows our model to be fine-tuned given a single image. We can then render new views in a geometrically consistent manner and they represent faithfully the input object. Additionally, our method is able to generalize to images outside of the training domain (more realistic renderings and even real photographs). Finally, the inferred geometric scaffold is itself an accurate estimate of the object's 3D shape. We demonstrate in several experiments the effectiveness of our approach in both synthetic and real images.","publicationTitle":"arXiv:2102.08860 [cs]","volume":"","issue":"","pages":"","date":"2021-02-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"ShaRF","url":"http://arxiv.org/abs/2102.08860","accessDate":"2021-06-20T16:03:59Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2102.08860","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:59Z","dateModified":"2021-06-20T16:03:59Z"}},{"key":"B5JJGQCT","version":61,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/B5JJGQCT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/B5JJGQCT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UQPB3TH8","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"B5JJGQCT","version":61,"parentItem":"UQPB3TH8","itemType":"note","note":"Comment: Project page: http://www.krematas.com/sharf/index.html","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:59Z","dateModified":"2021-06-20T16:03:59Z"}},{"key":"WMXXCCEA","version":171,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WMXXCCEA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WMXXCCEA","type":"text/html"}},"meta":{"creatorSummary":"Tretschk et al.","parsedDate":"2021-02-26","numChildren":3},"data":{"key":"WMXXCCEA","version":171,"itemType":"journalArticle","title":"Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video","creators":[{"creatorType":"author","firstName":"Edgar","lastName":"Tretschk"},{"creatorType":"author","firstName":"Ayush","lastName":"Tewari"},{"creatorType":"author","firstName":"Vladislav","lastName":"Golyanik"},{"creatorType":"author","firstName":"Michael","lastName":"Zollhöfer"},{"creatorType":"author","firstName":"Christoph","lastName":"Lassner"},{"creatorType":"author","firstName":"Christian","lastName":"Theobalt"}],"abstractNote":"We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input, e.g., from a monocular video recording, and creates a high-quality space-time geometry and appearance representation. In particular, we show that even a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, for example a `bullet-time' video effect. Our method disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly to represent scene motion. We also propose a novel rigidity regression network that enables us to better constrain rigid regions of the scene, which leads to more stable results. The ray bending and rigidity network are trained without any explicit supervision. In addition to novel view synthesis, our formulation enables dense correspondence estimation across views and time, as well as compelling video editing applications such as motion exaggeration. We demonstrate the effectiveness of our method using extensive evaluations, including ablation studies and comparisons to the state of the art. We urge the reader to watch the supplemental video for qualitative results. Our code will be open sourced.","publicationTitle":"arXiv:2012.12247 [cs]","volume":"","issue":"","pages":"","date":"2021-02-26","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Non-Rigid Neural Radiance Fields","url":"http://arxiv.org/abs/2012.12247","accessDate":"2021-06-20T16:03:58Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.12247","tags":[],"collections":["CPYKW3PF","I7H95BKE"],"relations":{},"dateAdded":"2021-06-20T16:03:58Z","dateModified":"2021-06-20T16:03:58Z"}},{"key":"IWDM4ZF5","version":95,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IWDM4ZF5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IWDM4ZF5","type":"text/html"}},"meta":{"creatorSummary":"Wang et al.","parsedDate":"2020-12-17","numChildren":2},"data":{"key":"IWDM4ZF5","version":95,"itemType":"journalArticle","title":"Learning Compositional Radiance Fields of Dynamic Human Heads","creators":[{"creatorType":"author","firstName":"Ziyan","lastName":"Wang"},{"creatorType":"author","firstName":"Timur","lastName":"Bagautdinov"},{"creatorType":"author","firstName":"Stephen","lastName":"Lombardi"},{"creatorType":"author","firstName":"Tomas","lastName":"Simon"},{"creatorType":"author","firstName":"Jason","lastName":"Saragih"},{"creatorType":"author","firstName":"Jessica","lastName":"Hodgins"},{"creatorType":"author","firstName":"Michael","lastName":"Zollhöfer"}],"abstractNote":"Photorealistic rendering of dynamic humans is an important ability for telepresence systems, virtual shopping, synthetic data generation, and more. Recently, neural rendering methods, which combine techniques from computer graphics and machine learning, have created high-fidelity models of humans and objects. Some of these methods do not produce results with high-enough fidelity for driveable human models (Neural Volumes) whereas others have extremely long rendering times (NeRF). We propose a novel compositional 3D representation that combines the best of previous methods to produce both higher-resolution and faster results. Our representation bridges the gap between discrete and continuous volumetric representations by combining a coarse 3D-structure-aware grid of animation codes with a continuous learned scene function that maps every position and its corresponding local animation code to its view-dependent emitted radiance and local volume density. Differentiable volume rendering is employed to compute photo-realistic novel views of the human head and upper body as well as to train our novel representation end-to-end using only 2D supervision. In addition, we show that the learned dynamic radiance field can be used to synthesize novel unseen expressions based on a global animation code. Our approach achieves state-of-the-art results for synthesizing novel views of dynamic human heads and the upper body.","publicationTitle":"arXiv:2012.09955 [cs]","volume":"","issue":"","pages":"","date":"2020-12-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2012.09955","accessDate":"2021-06-20T16:03:58Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.09955","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:58Z","dateModified":"2021-06-20T16:03:58Z"}},{"key":"75JSFXK4","version":81,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/75JSFXK4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/75JSFXK4","type":"text/html"}},"meta":{"creatorSummary":"Peng et al.","parsedDate":"2021-03-29","numChildren":3},"data":{"key":"75JSFXK4","version":81,"itemType":"journalArticle","title":"Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans","creators":[{"creatorType":"author","firstName":"Sida","lastName":"Peng"},{"creatorType":"author","firstName":"Yuanqing","lastName":"Zhang"},{"creatorType":"author","firstName":"Yinghao","lastName":"Xu"},{"creatorType":"author","firstName":"Qianqian","lastName":"Wang"},{"creatorType":"author","firstName":"Qing","lastName":"Shuai"},{"creatorType":"author","firstName":"Hujun","lastName":"Bao"},{"creatorType":"author","firstName":"Xiaowei","lastName":"Zhou"}],"abstractNote":"This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at https://zju3dv.github.io/neuralbody/.","publicationTitle":"arXiv:2012.15838 [cs]","volume":"","issue":"","pages":"","date":"2021-03-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Neural Body","url":"http://arxiv.org/abs/2012.15838","accessDate":"2021-06-20T16:03:58Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.15838","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:58Z","dateModified":"2021-06-20T16:03:58Z"}},{"key":"R64JJF2B","version":61,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/R64JJF2B","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/R64JJF2B","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S9RRBVBF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"R64JJF2B","version":61,"parentItem":"S9RRBVBF","itemType":"note","note":"Comment: project page see nerfmm.active.vision","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:58Z","dateModified":"2021-06-20T16:03:58Z"}},{"key":"3DEGGJ55","version":61,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3DEGGJ55","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3DEGGJ55","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WMXXCCEA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3DEGGJ55","version":61,"parentItem":"WMXXCCEA","itemType":"note","note":"Comment: Project page (incl. supplemental videos and code): https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:58Z","dateModified":"2021-06-20T16:03:58Z"}},{"key":"RV3V46EK","version":61,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RV3V46EK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RV3V46EK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/75JSFXK4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RV3V46EK","version":61,"parentItem":"75JSFXK4","itemType":"note","note":"Comment: CVPR 2021. Project page: https://zju3dv.github.io/neuralbody/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:58Z","dateModified":"2021-06-20T16:03:58Z"}},{"key":"9YFDKDRF","version":199,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9YFDKDRF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9YFDKDRF","type":"text/html"}},"meta":{"creatorSummary":"Hu et al.","parsedDate":"2021-04-16","numChildren":3},"data":{"key":"9YFDKDRF","version":199,"itemType":"journalArticle","title":"Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image","creators":[{"creatorType":"author","firstName":"Ronghang","lastName":"Hu"},{"creatorType":"author","firstName":"Nikhila","lastName":"Ravi"},{"creatorType":"author","firstName":"Alex","lastName":"Berg"},{"creatorType":"author","firstName":"Deepak","lastName":"Pathak"}],"abstractNote":"We present Worldsheet, a method for novel view synthesis using just a single RGB image as input. The main insight is that simply shrink-wrapping a planar mesh sheet onto the input image, consistent with the learned intermediate depth, captures underlying geometry sufficient to generate photorealistic unseen views with large viewpoint changes. To operationalize this, we propose a novel differentiable texture sampler that allows our wrapped mesh sheet to be textured and rendered differentiably into an image from a target viewpoint. Our approach is category-agnostic, end-to-end trainable without using any 3D supervision, and requires a single image at test time. We also explore a simple extension by stacking multiple layers of Worldsheets to better handle occlusions. Worldsheet consistently outperforms prior state-of-the-art methods on single-image view synthesis across several datasets. Furthermore, this simple idea captures novel views surprisingly well on a wide range of high-resolution in-the-wild images, converting them into navigable 3D pop-ups. Video results and code at https://worldsheet.github.io.","publicationTitle":"arXiv:2012.09854 [cs, stat]","volume":"","issue":"","pages":"","date":"2021-04-16","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Worldsheet","url":"http://arxiv.org/abs/2012.09854","accessDate":"2021-06-20T16:03:56Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.09854","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:57Z","dateModified":"2021-06-20T16:03:57Z"}},{"key":"VYTC7KJ6","version":81,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VYTC7KJ6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VYTC7KJ6","type":"text/html"}},"meta":{"creatorSummary":"Liu et al.","parsedDate":"2020-12-18","numChildren":3},"data":{"key":"VYTC7KJ6","version":81,"itemType":"journalArticle","title":"Infinite Nature: Perpetual View Generation of Natural Scenes from a Single Image","creators":[{"creatorType":"author","firstName":"Andrew","lastName":"Liu"},{"creatorType":"author","firstName":"Richard","lastName":"Tucker"},{"creatorType":"author","firstName":"Varun","lastName":"Jampani"},{"creatorType":"author","firstName":"Ameesh","lastName":"Makadia"},{"creatorType":"author","firstName":"Noah","lastName":"Snavely"},{"creatorType":"author","firstName":"Angjoo","lastName":"Kanazawa"}],"abstractNote":"We introduce the problem of perpetual view generation -- long-range generation of novel views corresponding to an arbitrarily long camera trajectory given a single image. This is a challenging problem that goes far beyond the capabilities of current view synthesis methods, which work for a limited range of viewpoints and quickly degenerate when presented with a large camera motion. Methods designed for video generation also have limited ability to produce long video sequences and are often agnostic to scene geometry. We take a hybrid approach that integrates both geometry and image synthesis in an iterative render, refine, and repeat framework, allowing for long-range generation that cover large distances after hundreds of frames. Our approach can be trained from a set of monocular video sequences without any manual annotation. We propose a dataset of aerial footage of natural coastal scenes, and compare our method with recent view synthesis and conditional video generation baselines, showing that it can generate plausible scenes for much longer time horizons over large camera trajectories compared to existing methods. Please visit our project page at https://infinite-nature.github.io/.","publicationTitle":"arXiv:2012.09855 [cs]","volume":"","issue":"","pages":"","date":"2020-12-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Infinite Nature","url":"http://arxiv.org/abs/2012.09855","accessDate":"2021-06-20T16:03:57Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.09855","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:57Z","dateModified":"2021-06-20T16:03:57Z"}},{"key":"6XEFNZBH","version":61,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6XEFNZBH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6XEFNZBH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VYTC7KJ6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6XEFNZBH","version":61,"parentItem":"VYTC7KJ6","itemType":"note","note":"Comment: Project page at https://infinite-nature.github.io/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:57Z","dateModified":"2021-06-20T16:03:57Z"}},{"key":"5Z47VFVG","version":61,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5Z47VFVG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5Z47VFVG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9YFDKDRF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5Z47VFVG","version":61,"parentItem":"9YFDKDRF","itemType":"note","note":"Comment: v2 diff: Added occlusion handling via layered Wordsheets. Webpage at https://worldsheet.github.io","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:57Z","dateModified":"2021-06-20T16:03:57Z"}},{"key":"6L8GNRIL","version":174,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6L8GNRIL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6L8GNRIL","type":"text/html"}},"meta":{"creatorSummary":"Gao et al.","parsedDate":"2021-04-16","numChildren":3},"data":{"key":"6L8GNRIL","version":174,"itemType":"journalArticle","title":"Portrait Neural Radiance Fields from a Single Image","creators":[{"creatorType":"author","firstName":"Chen","lastName":"Gao"},{"creatorType":"author","firstName":"Yichang","lastName":"Shih"},{"creatorType":"author","firstName":"Wei-Sheng","lastName":"Lai"},{"creatorType":"author","firstName":"Chia-Kai","lastName":"Liang"},{"creatorType":"author","firstName":"Jia-Bin","lastName":"Huang"}],"abstractNote":"We present a method for estimating Neural Radiance Fields (NeRF) from a single headshot portrait. While NeRF has demonstrated high-quality view synthesis, it requires multiple images of static scenes and thus impractical for casual captures and moving subjects. In this work, we propose to pretrain the weights of a multilayer perceptron (MLP), which implicitly models the volumetric density and colors, with a meta-learning framework using a light stage portrait dataset. To improve the generalization to unseen faces, we train the MLP in the canonical coordinate space approximated by 3D face morphable models. We quantitatively evaluate the method using controlled captures and demonstrate the generalization to real portrait images, showing favorable results against state-of-the-arts.","publicationTitle":"arXiv:2012.05903 [cs]","volume":"","issue":"","pages":"","date":"2021-04-16","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2012.05903","accessDate":"2021-06-20T16:03:56Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.05903","tags":[],"collections":["CPYKW3PF","KFHEDKK5"],"relations":{},"dateAdded":"2021-06-20T16:03:56Z","dateModified":"2021-06-20T16:03:56Z"}},{"key":"HTAUNQ6X","version":95,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HTAUNQ6X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HTAUNQ6X","type":"text/html"}},"meta":{"creatorSummary":"Guo et al.","parsedDate":"2020-12-15","numChildren":3},"data":{"key":"HTAUNQ6X","version":95,"itemType":"journalArticle","title":"Object-Centric Neural Scene Rendering","creators":[{"creatorType":"author","firstName":"Michelle","lastName":"Guo"},{"creatorType":"author","firstName":"Alireza","lastName":"Fathi"},{"creatorType":"author","firstName":"Jiajun","lastName":"Wu"},{"creatorType":"author","firstName":"Thomas","lastName":"Funkhouser"}],"abstractNote":"We present a method for composing photorealistic scenes from captured images of objects. Our work builds upon neural radiance fields (NeRFs), which implicitly model the volumetric density and directionally-emitted radiance of a scene. While NeRFs synthesize realistic pictures, they only model static scenes and are closely tied to specific imaging conditions. This property makes NeRFs hard to generalize to new scenarios, including new lighting or new arrangements of objects. Instead of learning a scene radiance field as a NeRF does, we propose to learn object-centric neural scattering functions (OSFs), a representation that models per-object light transport implicitly using a lighting- and view-dependent neural network. This enables rendering scenes even when objects or lights move, without retraining. Combined with a volumetric path tracing procedure, our framework is capable of rendering both intra- and inter-object light transport effects including occlusions, specularities, shadows, and indirect illumination. We evaluate our approach on scene composition and show that it generalizes to novel illumination conditions, producing photorealistic, physically accurate renderings of multi-object scenes.","publicationTitle":"arXiv:2012.08503 [cs]","volume":"","issue":"","pages":"","date":"2020-12-15","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2012.08503","accessDate":"2021-06-20T16:03:56Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.08503","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:56Z","dateModified":"2021-06-20T16:03:56Z"}},{"key":"ZT43H7LK","version":81,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZT43H7LK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZT43H7LK","type":"text/html"}},"meta":{"creatorSummary":"Gafni et al.","parsedDate":"2020-12-05","numChildren":3},"data":{"key":"ZT43H7LK","version":81,"itemType":"journalArticle","title":"Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction","creators":[{"creatorType":"author","firstName":"Guy","lastName":"Gafni"},{"creatorType":"author","firstName":"Justus","lastName":"Thies"},{"creatorType":"author","firstName":"Michael","lastName":"Zollhöfer"},{"creatorType":"author","firstName":"Matthias","lastName":"Nießner"}],"abstractNote":"We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.","publicationTitle":"arXiv:2012.03065 [cs]","volume":"","issue":"","pages":"","date":"2020-12-05","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2012.03065","accessDate":"2021-06-20T16:03:56Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.03065","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:56Z","dateModified":"2021-06-20T16:03:56Z"}},{"key":"KWK58QX5","version":61,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KWK58QX5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KWK58QX5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HTAUNQ6X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KWK58QX5","version":61,"parentItem":"HTAUNQ6X","itemType":"note","note":"Comment: Summary Video: https://youtu.be/NtR7xgxSL1U Project Webpage: https://shellguo.com/osf","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:56Z","dateModified":"2021-06-20T16:03:56Z"}},{"key":"Y7TB4Y7M","version":60,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y7TB4Y7M","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y7TB4Y7M","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZT43H7LK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Y7TB4Y7M","version":60,"parentItem":"ZT43H7LK","itemType":"note","note":"Comment: Video: https://youtu.be/m7oROLdQnjk | Project page: https://gafniguy.github.io/4D-Facial-Avatars/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:56Z","dateModified":"2021-06-20T16:03:56Z"}},{"key":"U6R8PFHN","version":60,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U6R8PFHN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U6R8PFHN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6L8GNRIL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"U6R8PFHN","version":60,"parentItem":"6L8GNRIL","itemType":"note","note":"Comment: Project webpage: https://portrait-nerf.github.io/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:56Z","dateModified":"2021-06-20T16:03:56Z"}},{"key":"4BWLNEMH","version":95,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4BWLNEMH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4BWLNEMH","type":"text/html"}},"meta":{"creatorSummary":"Yen-Chen et al.","parsedDate":"2021-04-01","numChildren":3},"data":{"key":"4BWLNEMH","version":95,"itemType":"journalArticle","title":"iNeRF: Inverting Neural Radiance Fields for Pose Estimation","creators":[{"creatorType":"author","firstName":"Lin","lastName":"Yen-Chen"},{"creatorType":"author","firstName":"Pete","lastName":"Florence"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Alberto","lastName":"Rodriguez"},{"creatorType":"author","firstName":"Phillip","lastName":"Isola"},{"creatorType":"author","firstName":"Tsung-Yi","lastName":"Lin"}],"abstractNote":"We present iNeRF, a framework that performs mesh-free pose estimation by \"inverting\" a Neural RadianceField (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis - synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation - given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.","publicationTitle":"arXiv:2012.05877 [cs]","volume":"","issue":"","pages":"","date":"2021-04-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"iNeRF","url":"http://arxiv.org/abs/2012.05877","accessDate":"2021-06-20T16:03:55Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.05877","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:55Z","dateModified":"2021-06-20T16:03:55Z"}},{"key":"VMJXU2HQ","version":80,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VMJXU2HQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VMJXU2HQ","type":"text/html"}},"meta":{"creatorSummary":"Srinivasan et al.","parsedDate":"2020-12-07","numChildren":3},"data":{"key":"VMJXU2HQ","version":80,"itemType":"journalArticle","title":"NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis","creators":[{"creatorType":"author","firstName":"Pratul P.","lastName":"Srinivasan"},{"creatorType":"author","firstName":"Boyang","lastName":"Deng"},{"creatorType":"author","firstName":"Xiuming","lastName":"Zhang"},{"creatorType":"author","firstName":"Matthew","lastName":"Tancik"},{"creatorType":"author","firstName":"Ben","lastName":"Mildenhall"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"}],"abstractNote":"We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.","publicationTitle":"arXiv:2012.03927 [cs]","volume":"","issue":"","pages":"","date":"2020-12-07","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"NeRV","url":"http://arxiv.org/abs/2012.03927","accessDate":"2021-06-20T16:03:55Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.03927","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:55Z","dateModified":"2021-06-20T16:03:55Z"}},{"key":"EQVWR6WG","version":80,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EQVWR6WG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EQVWR6WG","type":"text/html"}},"meta":{"creatorSummary":"Xu et al.","parsedDate":"2020-12-09","numChildren":3},"data":{"key":"EQVWR6WG","version":80,"itemType":"journalArticle","title":"Positional Encoding as Spatial Inductive Bias in GANs","creators":[{"creatorType":"author","firstName":"Rui","lastName":"Xu"},{"creatorType":"author","firstName":"Xintao","lastName":"Wang"},{"creatorType":"author","firstName":"Kai","lastName":"Chen"},{"creatorType":"author","firstName":"Bolei","lastName":"Zhou"},{"creatorType":"author","firstName":"Chen Change","lastName":"Loy"}],"abstractNote":"SinGAN shows impressive capability in learning internal patch distribution despite its limited effective receptive field. We are interested in knowing how such a translation-invariant convolutional generator could capture the global structure with just a spatially i.i.d. input. In this work, taking SinGAN and StyleGAN2 as examples, we show that such capability, to a large extent, is brought by the implicit positional encoding when using zero padding in the generators. Such positional encoding is indispensable for generating images with high fidelity. The same phenomenon is observed in other generative architectures such as DCGAN and PGGAN. We further show that zero padding leads to an unbalanced spatial bias with a vague relation between locations. To offer a better spatial inductive bias, we investigate alternative positional encodings and analyze their effects. Based on a more flexible positional encoding explicitly, we propose a new multi-scale training strategy and demonstrate its effectiveness in the state-of-the-art unconditional generator StyleGAN2. Besides, the explicit spatial inductive bias substantially improve SinGAN for more versatile image manipulation.","publicationTitle":"arXiv:2012.05217 [cs]","volume":"","issue":"","pages":"","date":"2020-12-09","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2012.05217","accessDate":"2021-06-20T16:03:55Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.05217","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:55Z","dateModified":"2021-06-20T16:03:55Z"}},{"key":"LE2KW39R","version":60,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LE2KW39R","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LE2KW39R","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VMJXU2HQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"LE2KW39R","version":60,"parentItem":"VMJXU2HQ","itemType":"note","note":"Comment: Project page: https://people.eecs.berkeley.edu/~pratul/nerv","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:55Z","dateModified":"2021-06-20T16:03:55Z"}},{"key":"5W2FTFLY","version":80,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5W2FTFLY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5W2FTFLY","type":"text/html"}},"meta":{"creatorSummary":"Boss et al.","parsedDate":"2021-05-19","numChildren":2},"data":{"key":"5W2FTFLY","version":80,"itemType":"journalArticle","title":"NeRD: Neural Reflectance Decomposition from Image Collections","creators":[{"creatorType":"author","firstName":"Mark","lastName":"Boss"},{"creatorType":"author","firstName":"Raphael","lastName":"Braun"},{"creatorType":"author","firstName":"Varun","lastName":"Jampani"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Ce","lastName":"Liu"},{"creatorType":"author","firstName":"Hendrik P. A.","lastName":"Lensch"}],"abstractNote":"Decomposing a scene into its shape, reflectance, and illumination is a challenging but essential problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. By decomposing a scene into explicit representations, any rendering framework can be leveraged to generate novel views under any illumination in real-time. NeRD is a method that achieves this decomposition by introducing physically-based rendering to neural radiance fields. Even challenging non-Lambertian reflectances, complex geometry, and unknown illumination can be decomposed into high-quality models. The datasets and code is available on the project page: https://markboss.me/publication/2021-nerd/","publicationTitle":"arXiv:2012.03918 [cs]","volume":"","issue":"","pages":"","date":"2021-05-19","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"NeRD","url":"http://arxiv.org/abs/2012.03918","accessDate":"2021-06-20T16:03:54Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.03918","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:54Z","dateModified":"2021-06-20T16:03:54Z"}},{"key":"WHVCIMNZ","version":165,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WHVCIMNZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WHVCIMNZ","type":"text/html"}},"meta":{"creatorSummary":"Lindell et al.","parsedDate":"2021-05-22","numChildren":3},"data":{"key":"WHVCIMNZ","version":165,"itemType":"journalArticle","title":"AutoInt: Automatic Integration for Fast Neural Volume Rendering","creators":[{"creatorType":"author","firstName":"David B.","lastName":"Lindell"},{"creatorType":"author","firstName":"Julien N. P.","lastName":"Martel"},{"creatorType":"author","firstName":"Gordon","lastName":"Wetzstein"}],"abstractNote":"Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10 times with a tradeoff of slightly reduced image quality.","publicationTitle":"arXiv:2012.01714 [cs]","volume":"","issue":"","pages":"","date":"2021-05-22","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"AutoInt","url":"http://arxiv.org/abs/2012.01714","accessDate":"2021-06-20T16:03:53Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.01714","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:53Z","dateModified":"2021-06-20T16:03:53Z"}},{"key":"43G9UVNV","version":121,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/43G9UVNV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/43G9UVNV","type":"text/html"}},"meta":{"creatorSummary":"Tancik et al.","parsedDate":"2021-03-23","numChildren":3},"data":{"key":"43G9UVNV","version":121,"itemType":"journalArticle","title":"Learned Initializations for Optimizing Coordinate-Based Neural Representations","creators":[{"creatorType":"author","firstName":"Matthew","lastName":"Tancik"},{"creatorType":"author","firstName":"Ben","lastName":"Mildenhall"},{"creatorType":"author","firstName":"Terrance","lastName":"Wang"},{"creatorType":"author","firstName":"Divi","lastName":"Schmidt"},{"creatorType":"author","firstName":"Pratul P.","lastName":"Srinivasan"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Ren","lastName":"Ng"}],"abstractNote":"Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.","publicationTitle":"arXiv:2012.02189 [cs]","volume":"","issue":"","pages":"","date":"2021-03-23","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2012.02189","accessDate":"2021-06-20T16:03:53Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.02189","tags":[],"collections":["VDV7CTDX"],"relations":{},"dateAdded":"2021-06-20T16:03:53Z","dateModified":"2021-06-20T16:03:53Z"}},{"key":"B5TC93EQ","version":95,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/B5TC93EQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/B5TC93EQ","type":"text/html"}},"meta":{"creatorSummary":"Pumarola et al.","parsedDate":"2020-11-27","numChildren":2},"data":{"key":"B5TC93EQ","version":95,"itemType":"journalArticle","title":"D-NeRF: Neural Radiance Fields for Dynamic Scenes","creators":[{"creatorType":"author","firstName":"Albert","lastName":"Pumarola"},{"creatorType":"author","firstName":"Enric","lastName":"Corona"},{"creatorType":"author","firstName":"Gerard","lastName":"Pons-Moll"},{"creatorType":"author","firstName":"Francesc","lastName":"Moreno-Noguer"}],"abstractNote":"Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \\emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.","publicationTitle":"arXiv:2011.13961 [cs]","volume":"","issue":"","pages":"","date":"2020-11-27","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"D-NeRF","url":"http://arxiv.org/abs/2011.13961","accessDate":"2021-06-20T16:03:53Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.13961","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:53Z","dateModified":"2021-06-20T16:03:53Z"}},{"key":"MDMF8SRF","version":60,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/MDMF8SRF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/MDMF8SRF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/43G9UVNV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"MDMF8SRF","version":60,"parentItem":"43G9UVNV","itemType":"note","note":"Comment: Project page: https://www.matthewtancik.com/learnit","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:53Z","dateModified":"2021-06-20T16:03:53Z"}},{"key":"DQHUP86T","version":80,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DQHUP86T","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DQHUP86T","type":"text/html"}},"meta":{"creatorSummary":"Chan et al.","parsedDate":"2021-04-05","numChildren":3},"data":{"key":"DQHUP86T","version":80,"itemType":"journalArticle","title":"pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis","creators":[{"creatorType":"author","firstName":"Eric R.","lastName":"Chan"},{"creatorType":"author","firstName":"Marco","lastName":"Monteiro"},{"creatorType":"author","firstName":"Petr","lastName":"Kellnhofer"},{"creatorType":"author","firstName":"Jiajun","lastName":"Wu"},{"creatorType":"author","firstName":"Gordon","lastName":"Wetzstein"}],"abstractNote":"We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($\\pi$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $\\pi$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.","publicationTitle":"arXiv:2012.00926 [cs]","volume":"","issue":"","pages":"","date":"2021-04-05","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"pi-GAN","url":"http://arxiv.org/abs/2012.00926","accessDate":"2021-06-20T16:03:52Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2012.00926","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:52Z","dateModified":"2021-06-20T16:03:52Z"}},{"key":"93SRFTTC","version":79,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/93SRFTTC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/93SRFTTC","type":"text/html"}},"meta":{"creatorSummary":"Li et al.","parsedDate":"2021-04-20","numChildren":3},"data":{"key":"93SRFTTC","version":79,"itemType":"journalArticle","title":"Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes","creators":[{"creatorType":"author","firstName":"Zhengqi","lastName":"Li"},{"creatorType":"author","firstName":"Simon","lastName":"Niklaus"},{"creatorType":"author","firstName":"Noah","lastName":"Snavely"},{"creatorType":"author","firstName":"Oliver","lastName":"Wang"}],"abstractNote":"We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.","publicationTitle":"arXiv:2011.13084 [cs]","volume":"","issue":"","pages":"","date":"2021-04-20","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2011.13084","accessDate":"2021-06-20T16:03:51Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.13084","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:52Z","dateModified":"2021-06-20T16:03:52Z"}},{"key":"ZVKEBYDF","version":60,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZVKEBYDF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZVKEBYDF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/93SRFTTC","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZVKEBYDF","version":60,"parentItem":"93SRFTTC","itemType":"note","note":"Comment: CVPR 2021, Project Website: http://www.cs.cornell.edu/~zl548/NSFF/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:52Z","dateModified":"2021-06-20T16:03:52Z"}},{"key":"RXML26ZL","version":180,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RXML26ZL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RXML26ZL","type":"text/html"}},"meta":{"creatorSummary":"Xian et al.","parsedDate":"2020-11-25","numChildren":3},"data":{"key":"RXML26ZL","version":180,"itemType":"journalArticle","title":"Space-time Neural Irradiance Fields for Free-Viewpoint Video","creators":[{"creatorType":"author","firstName":"Wenqi","lastName":"Xian"},{"creatorType":"author","firstName":"Jia-Bin","lastName":"Huang"},{"creatorType":"author","firstName":"Johannes","lastName":"Kopf"},{"creatorType":"author","firstName":"Changil","lastName":"Kim"}],"abstractNote":"We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results.","publicationTitle":"arXiv:2011.12950 [cs]","volume":"","issue":"","pages":"","date":"2020-11-25","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2011.12950","accessDate":"2021-06-20T16:03:51Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.12950","tags":[],"collections":["CPYKW3PF","FJNUUAVD"],"relations":{},"dateAdded":"2021-06-20T16:03:51Z","dateModified":"2021-06-20T16:03:51Z"}},{"key":"VSEJ2GUV","version":174,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VSEJ2GUV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VSEJ2GUV","type":"text/html"}},"meta":{"creatorSummary":"Park et al.","parsedDate":"2021-05-13","numChildren":3},"data":{"key":"VSEJ2GUV","version":174,"itemType":"journalArticle","title":"Nerfies: Deformable Neural Radiance Fields","creators":[{"creatorType":"author","firstName":"Keunhong","lastName":"Park"},{"creatorType":"author","firstName":"Utkarsh","lastName":"Sinha"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Sofien","lastName":"Bouaziz"},{"creatorType":"author","firstName":"Dan B.","lastName":"Goldman"},{"creatorType":"author","firstName":"Steven M.","lastName":"Seitz"},{"creatorType":"author","firstName":"Ricardo","lastName":"Martin-Brualla"}],"abstractNote":"We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.","publicationTitle":"arXiv:2011.12948 [cs]","volume":"","issue":"","pages":"","date":"2021-05-13","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Nerfies","url":"http://arxiv.org/abs/2011.12948","accessDate":"2021-06-20T16:03:51Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.12948","tags":[],"collections":["CPYKW3PF","I7H95BKE","KFHEDKK5"],"relations":{},"dateAdded":"2021-06-20T16:03:51Z","dateModified":"2021-06-20T16:03:51Z"}},{"key":"WFPLC6QT","version":60,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WFPLC6QT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WFPLC6QT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VSEJ2GUV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WFPLC6QT","version":60,"parentItem":"VSEJ2GUV","itemType":"note","note":"Comment: Project page with videos: https://nerfies.github.io/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:51Z","dateModified":"2021-06-20T16:03:51Z"}},{"key":"V5KTHHYD","version":177,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/V5KTHHYD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/V5KTHHYD","type":"text/html"}},"meta":{"creatorSummary":"Niemeyer and Geiger","parsedDate":"2021-04-29","numChildren":3},"data":{"key":"V5KTHHYD","version":177,"itemType":"journalArticle","title":"GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields","creators":[{"creatorType":"author","firstName":"Michael","lastName":"Niemeyer"},{"creatorType":"author","firstName":"Andreas","lastName":"Geiger"}],"abstractNote":"Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.","publicationTitle":"arXiv:2011.12100 [cs]","volume":"","issue":"","pages":"","date":"2021-04-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"GIRAFFE","url":"http://arxiv.org/abs/2011.12100","accessDate":"2021-06-20T16:03:50Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.12100","tags":[{"tag":"nerf"}],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:50Z","dateModified":"2021-06-20T16:03:50Z"}},{"key":"ZNDAS9TD","version":94,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZNDAS9TD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZNDAS9TD","type":"text/html"}},"meta":{"creatorSummary":"Rebain et al.","parsedDate":"2020-11-24","numChildren":2},"data":{"key":"ZNDAS9TD","version":94,"itemType":"journalArticle","title":"DeRF: Decomposed Radiance Fields","creators":[{"creatorType":"author","firstName":"Daniel","lastName":"Rebain"},{"creatorType":"author","firstName":"Wei","lastName":"Jiang"},{"creatorType":"author","firstName":"Soroosh","lastName":"Yazdani"},{"creatorType":"author","firstName":"Ke","lastName":"Li"},{"creatorType":"author","firstName":"Kwang Moo","lastName":"Yi"},{"creatorType":"author","firstName":"Andrea","lastName":"Tagliasacchi"}],"abstractNote":"With the advent of Neural Radiance Fields (NeRF), neural networks can now render novel views of a 3D scene with quality that fools the human eye. Yet, generating these images is very computationally intensive, limiting their applicability in practical scenarios. In this paper, we propose a technique based on spatial decomposition capable of mitigating this issue. Our key observation is that there are diminishing returns in employing larger (deeper and/or wider) networks. Hence, we propose to spatially decompose a scene and dedicate smaller networks for each decomposed part. When working together, these networks can render the whole scene. This allows us near-constant inference time regardless of the number of decomposed parts. Moreover, we show that a Voronoi spatial decomposition is preferable for this purpose, as it is provably compatible with the Painter's Algorithm for efficient and GPU-friendly rendering. Our experiments show that for real-world scenes, our method provides up to 3x more efficient inference than NeRF (with the same rendering quality), or an improvement of up to 1.0~dB in PSNR (for the same inference cost).","publicationTitle":"arXiv:2011.12490 [cs]","volume":"","issue":"","pages":"","date":"2020-11-24","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"DeRF","url":"http://arxiv.org/abs/2011.12490","accessDate":"2021-06-20T16:03:50Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.12490","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:50Z","dateModified":"2021-06-20T16:03:50Z"}},{"key":"XCPDD5N9","version":79,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XCPDD5N9","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XCPDD5N9","type":"text/html"}},"meta":{"creatorSummary":"Li et al.","parsedDate":"2020-11-22","numChildren":3},"data":{"key":"XCPDD5N9","version":79,"itemType":"journalArticle","title":"Multi-Plane Program Induction with 3D Box Priors","creators":[{"creatorType":"author","firstName":"Yikai","lastName":"Li"},{"creatorType":"author","firstName":"Jiayuan","lastName":"Mao"},{"creatorType":"author","firstName":"Xiuming","lastName":"Zhang"},{"creatorType":"author","firstName":"William T.","lastName":"Freeman"},{"creatorType":"author","firstName":"Joshua B.","lastName":"Tenenbaum"},{"creatorType":"author","firstName":"Noah","lastName":"Snavely"},{"creatorType":"author","firstName":"Jiajun","lastName":"Wu"}],"abstractNote":"We consider two important aspects in understanding and editing images: modeling regular, program-like texture or patterns in 2D planes, and 3D posing of these planes in the scene. Unlike prior work on image-based program synthesis, which assumes the image contains a single visible 2D plane, we present Box Program Induction (BPI), which infers a program-like scene representation that simultaneously models repeated structure on multiple 2D planes, the 3D position and orientation of the planes, and camera parameters, all from a single image. Our model assumes a box prior, i.e., that the image captures either an inner view or an outer view of a box in 3D. It uses neural networks to infer visual cues such as vanishing points, wireframe lines to guide a search-based algorithm to find the program that best explains the image. Such a holistic, structured scene representation enables 3D-aware interactive image editing operations such as inpainting missing pixels, changing camera parameters, and extrapolate the image contents.","publicationTitle":"arXiv:2011.10007 [cs, stat]","volume":"","issue":"","pages":"","date":"2020-11-22","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2011.10007","accessDate":"2021-06-20T16:03:50Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.10007","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:50Z","dateModified":"2021-06-20T16:03:50Z"}},{"key":"T2HTFEC5","version":60,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/T2HTFEC5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/T2HTFEC5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XCPDD5N9","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"T2HTFEC5","version":60,"parentItem":"XCPDD5N9","itemType":"note","note":"Comment: NeurIPS 2020. First two authors contributed equally. Project page: http://bpi.csail.mit.edu","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:50Z","dateModified":"2021-06-20T16:03:50Z"}},{"key":"CVAGJJRT","version":59,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CVAGJJRT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CVAGJJRT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/V5KTHHYD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"CVAGJJRT","version":59,"parentItem":"V5KTHHYD","itemType":"note","note":"Comment: Accepted to CVPR 2021 (oral). Project page: http://bit.ly/giraffe-project","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:50Z","dateModified":"2021-06-20T16:03:50Z"}},{"key":"TQX66PWD","version":59,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TQX66PWD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TQX66PWD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HSN56RWV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TQX66PWD","version":59,"parentItem":"HSN56RWV","itemType":"note","note":"Comment: 19 pages, 17 figures","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:49Z","dateModified":"2021-06-20T16:03:49Z"}},{"key":"6CH93RLT","version":362,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6CH93RLT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6CH93RLT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/86AX9V9R","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6CH93RLT","version":362,"parentItem":"86AX9V9R","itemType":"note","note":"Comment: 20 pages, in progress","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/LC8DV9RA"},"dateAdded":"2021-06-20T16:03:48Z","dateModified":"2021-06-20T16:03:48Z"}},{"key":"86AX9V9R","version":362,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/86AX9V9R","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/86AX9V9R","type":"text/html"}},"meta":{"creatorSummary":"Liu et al.","parsedDate":"2021-01-06","numChildren":3},"data":{"key":"86AX9V9R","version":362,"itemType":"journalArticle","title":"Neural Sparse Voxel Fields","creators":[{"creatorType":"author","firstName":"Lingjie","lastName":"Liu"},{"creatorType":"author","firstName":"Jiatao","lastName":"Gu"},{"creatorType":"author","firstName":"Kyaw Zaw","lastName":"Lin"},{"creatorType":"author","firstName":"Tat-Seng","lastName":"Chua"},{"creatorType":"author","firstName":"Christian","lastName":"Theobalt"}],"abstractNote":"Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: https://github.com/facebookresearch/NSVF.","publicationTitle":"arXiv:2007.11571 [cs]","volume":"","issue":"","pages":"","date":"2021-01-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2007.11571","accessDate":"2021-06-20T16:03:48Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2007.11571","tags":[],"collections":["CPYKW3PF"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/UCFBLXW4"},"dateAdded":"2021-06-20T16:03:48Z","dateModified":"2021-06-20T16:03:48Z"}},{"key":"U962VCXI","version":183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/U962VCXI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/U962VCXI","type":"text/html"}},"meta":{"creatorSummary":"Lin et al.","parsedDate":"2020-10-20","numChildren":3},"data":{"key":"U962VCXI","version":183,"itemType":"journalArticle","title":"SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images","creators":[{"creatorType":"author","firstName":"Chen-Hsuan","lastName":"Lin"},{"creatorType":"author","firstName":"Chaoyang","lastName":"Wang"},{"creatorType":"author","firstName":"Simon","lastName":"Lucey"}],"abstractNote":"Dense 3D object reconstruction from a single image has recently witnessed remarkable advances, but supervising neural networks with ground-truth 3D shapes is impractical due to the laborious process of creating paired image-shape datasets. Recent efforts have turned to learning 3D reconstruction without 3D supervision from RGB images with annotated 2D silhouettes, dramatically reducing the cost and effort of annotation. These techniques, however, remain impractical as they still require multi-view annotations of the same object instance during training. As a result, most experimental efforts to date have been limited to synthetic datasets. In this paper, we address this issue and propose SDF-SRN, an approach that requires only a single view of objects at training time, offering greater utility for real-world scenarios. SDF-SRN learns implicit 3D shape representations to handle arbitrary shape topologies that may exist in the datasets. To this end, we derive a novel differentiable rendering formulation for learning signed distance functions (SDF) from 2D silhouettes. Our method outperforms the state of the art under challenging single-view supervision settings on both synthetic and real-world datasets.","publicationTitle":"arXiv:2010.10505 [cs]","volume":"","issue":"","pages":"","date":"2020-10-20","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"SDF-SRN","url":"http://arxiv.org/abs/2010.10505","accessDate":"2021-06-20T16:03:48Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2010.10505","tags":[],"collections":["5XFEXSAE"],"relations":{},"dateAdded":"2021-06-20T16:03:48Z","dateModified":"2021-06-20T16:03:48Z"}},{"key":"BVV9955T","version":167,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BVV9955T","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BVV9955T","type":"text/html"}},"meta":{"creatorSummary":"Riegler and Koltun","parsedDate":"2021-05-02","numChildren":3},"data":{"key":"BVV9955T","version":167,"itemType":"journalArticle","title":"Stable View Synthesis","creators":[{"creatorType":"author","firstName":"Gernot","lastName":"Riegler"},{"creatorType":"author","firstName":"Vladlen","lastName":"Koltun"}],"abstractNote":"We present Stable View Synthesis (SVS). Given a set of source images depicting a scene from freely distributed viewpoints, SVS synthesizes new views of the scene. The method operates on a geometric scaffold computed via structure-from-motion and multi-view stereo. Each point on this 3D scaffold is associated with view rays and corresponding feature vectors that encode the appearance of this point in the input images. The core of SVS is view-dependent on-surface feature aggregation, in which directional feature vectors at each 3D point are processed to produce a new feature vector for a ray that maps this point into the new target view. The target view is then rendered by a convolutional network from a tensor of features synthesized in this way for all pixels. The method is composed of differentiable modules and is trained end-to-end. It supports spatially-varying view-dependent importance weighting and feature transformation of source images at each point; spatial and temporal stability due to the smooth dependence of on-surface feature aggregation on the target view; and synthesis of view-dependent effects such as specular reflection. Experimental results demonstrate that SVS outperforms state-of-the-art view synthesis methods both quantitatively and qualitatively on three diverse real-world datasets, achieving unprecedented levels of realism in free-viewpoint video of challenging large-scale scenes. Code is available at https://github.com/intel-isl/StableViewSynthesis","publicationTitle":"arXiv:2011.07233 [cs]","volume":"","issue":"","pages":"","date":"2021-05-02","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2011.07233","accessDate":"2021-06-20T16:03:48Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2011.07233","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:48Z","dateModified":"2021-06-20T16:03:48Z"}},{"key":"7RWRT9N4","version":59,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7RWRT9N4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7RWRT9N4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/U962VCXI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7RWRT9N4","version":59,"parentItem":"U962VCXI","itemType":"note","note":"Comment: Accepted to NeurIPS 2020. Project page & code: https://chenhsuanlin.bitbucket.io/signed-distance-SRN/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:48Z","dateModified":"2021-06-20T16:03:48Z"}},{"key":"AUZ9DPRH","version":59,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AUZ9DPRH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AUZ9DPRH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BVV9955T","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"AUZ9DPRH","version":59,"parentItem":"BVV9955T","itemType":"note","note":"Comment: Published at CVPR 2021, https://youtu.be/gqgXIY09htI","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:48Z","dateModified":"2021-06-20T16:03:48Z"}},{"key":"7LPDVVES","version":184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7LPDVVES","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7LPDVVES","type":"text/html"}},"meta":{"creatorSummary":"Chibane et al.","parsedDate":"2020-10-26","numChildren":3},"data":{"key":"7LPDVVES","version":184,"itemType":"journalArticle","title":"Neural Unsigned Distance Fields for Implicit Function Learning","creators":[{"creatorType":"author","firstName":"Julian","lastName":"Chibane"},{"creatorType":"author","firstName":"Aymen","lastName":"Mir"},{"creatorType":"author","firstName":"Gerard","lastName":"Pons-Moll"}],"abstractNote":"In this work we target a learnable output representation that allows continuous, high resolution outputs of arbitrary shape. Recent works represent 3D surfaces implicitly with a Neural Network, thereby breaking previous barriers in resolution, and ability to represent diverse topologies. However, neural implicit representations are limited to closed surfaces, which divide the space into inside and outside. Many real world objects such as walls of a scene scanned by a sensor, clothing, or a car with inner structures are not closed. This constitutes a significant barrier, in terms of data pre-processing (objects need to be artificially closed creating artifacts), and the ability to output open surfaces. In this work, we propose Neural Distance Fields (NDF), a neural network based model which predicts the unsigned distance field for arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high resolutions as prior implicit models, but do not require closed surface data, and significantly broaden the class of representable shapes in the output. NDF allow to extract the surface as very dense point clouds and as meshes. We also show that NDF allow for surface normal calculation and can be rendered using a slight modification of sphere tracing. We find NDF can be used for multi-target regression (multiple outputs for one input) with techniques that have been exclusively used for rendering in graphics. Experiments on ShapeNet show that NDF, while simple, is the state-of-the art, and allows to reconstruct shapes with inner structures, such as the chairs inside a bus. Notably, we show that NDF are not restricted to 3D shapes, and can approximate more general open surfaces such as curves, manifolds, and functions. Code is available for research at https://virtualhumans.mpi-inf.mpg.de/ndf/.","publicationTitle":"arXiv:2010.13938 [cs]","volume":"","issue":"","pages":"","date":"2020-10-26","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2010.13938","accessDate":"2021-06-20T16:03:47Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2010.13938","tags":[],"collections":["5XFEXSAE"],"relations":{},"dateAdded":"2021-06-20T16:03:47Z","dateModified":"2021-06-20T16:03:47Z"}},{"key":"3SI7FM4P","version":59,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3SI7FM4P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3SI7FM4P","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/7LPDVVES","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3SI7FM4P","version":59,"parentItem":"7LPDVVES","itemType":"note","note":"Comment: Neural Information Processing Systems (NeurIPS) 2020","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:47Z","dateModified":"2021-06-20T16:03:47Z"}},{"key":"VR6DWTNT","version":94,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VR6DWTNT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VR6DWTNT","type":"text/html"}},"meta":{"creatorSummary":"Martin-Brualla et al.","parsedDate":"2021-01-06","numChildren":3},"data":{"key":"VR6DWTNT","version":94,"itemType":"journalArticle","title":"NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections","creators":[{"creatorType":"author","firstName":"Ricardo","lastName":"Martin-Brualla"},{"creatorType":"author","firstName":"Noha","lastName":"Radwan"},{"creatorType":"author","firstName":"Mehdi S. M.","lastName":"Sajjadi"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Alexey","lastName":"Dosovitskiy"},{"creatorType":"author","firstName":"Daniel","lastName":"Duckworth"}],"abstractNote":"We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.","publicationTitle":"arXiv:2008.02268 [cs]","volume":"","issue":"","pages":"","date":"2021-01-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"NeRF in the Wild","url":"http://arxiv.org/abs/2008.02268","accessDate":"2021-06-20T16:03:45Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2008.02268","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:45Z","dateModified":"2021-06-20T16:03:45Z"}},{"key":"F86LTWEM","version":78,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/F86LTWEM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/F86LTWEM","type":"text/html"}},"meta":{"creatorSummary":"Attal et al.","parsedDate":"2020-08-14","numChildren":3},"data":{"key":"F86LTWEM","version":78,"itemType":"journalArticle","title":"MatryODShka: Real-time 6DoF Video View Synthesis using Multi-Sphere Images","creators":[{"creatorType":"author","firstName":"Benjamin","lastName":"Attal"},{"creatorType":"author","firstName":"Selena","lastName":"Ling"},{"creatorType":"author","firstName":"Aaron","lastName":"Gokaslan"},{"creatorType":"author","firstName":"Christian","lastName":"Richardt"},{"creatorType":"author","firstName":"James","lastName":"Tompkin"}],"abstractNote":"We introduce a method to convert stereo 360{\\deg} (omnidirectional stereo) imagery into a layered, multi-sphere image representation for six degree-of-freedom (6DoF) rendering. Stereo 360{\\deg} imagery can be captured from multi-camera systems for virtual reality (VR), but lacks motion parallax and correct-in-all-directions disparity cues. Together, these can quickly lead to VR sickness when viewing content. One solution is to try and generate a format suitable for 6DoF rendering, such as by estimating depth. However, this raises questions as to how to handle disoccluded regions in dynamic scenes. Our approach is to simultaneously learn depth and disocclusions via a multi-sphere image representation, which can be rendered with correct 6DoF disparity and motion parallax in VR. This significantly improves comfort for the viewer, and can be inferred and rendered in real time on modern GPU hardware. Together, these move towards making VR video a more comfortable immersive medium.","publicationTitle":"arXiv:2008.06534 [cs]","volume":"","issue":"","pages":"","date":"2020-08-14","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"MatryODShka","url":"http://arxiv.org/abs/2008.06534","accessDate":"2021-06-20T16:03:45Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2008.06534","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:45Z","dateModified":"2021-06-20T16:03:45Z"}},{"key":"QBWKQDGW","version":59,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QBWKQDGW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QBWKQDGW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/F86LTWEM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"QBWKQDGW","version":59,"parentItem":"F86LTWEM","itemType":"note","note":"Comment: 25 pages, 13 figures, Published at European Conference on Computer Vision (ECCV 2020), Project Page: http://visual.cs.brown.edu/matryodshka","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:45Z","dateModified":"2021-06-20T16:03:45Z"}},{"key":"X8EPCZV3","version":59,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/X8EPCZV3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/X8EPCZV3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VR6DWTNT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"X8EPCZV3","version":59,"parentItem":"VR6DWTNT","itemType":"note","note":"Comment: Project website: https://nerf-w.github.io. Ricardo Martin-Brualla, Noha Radwan, and Mehdi S. M. Sajjadi contributed equally to this work. Updated with results for three additional scenes","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:45Z","dateModified":"2021-06-20T16:03:45Z"}},{"key":"9GEQIU6C","version":359,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9GEQIU6C","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9GEQIU6C","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PP3NI8CA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9GEQIU6C","version":359,"parentItem":"PP3NI8CA","itemType":"note","note":"Comment: Project page: https://people.eecs.berkeley.edu/~bmild/fourfeat/","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/U7T3P8JZ"},"dateAdded":"2021-06-20T16:03:44Z","dateModified":"2021-06-20T16:03:44Z"}},{"key":"PP3NI8CA","version":359,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PP3NI8CA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PP3NI8CA","type":"text/html"}},"meta":{"creatorSummary":"Tancik et al.","parsedDate":"2020-06-18","numChildren":3},"data":{"key":"PP3NI8CA","version":359,"itemType":"journalArticle","title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains","creators":[{"creatorType":"author","firstName":"Matthew","lastName":"Tancik"},{"creatorType":"author","firstName":"Pratul P.","lastName":"Srinivasan"},{"creatorType":"author","firstName":"Ben","lastName":"Mildenhall"},{"creatorType":"author","firstName":"Sara","lastName":"Fridovich-Keil"},{"creatorType":"author","firstName":"Nithin","lastName":"Raghavan"},{"creatorType":"author","firstName":"Utkarsh","lastName":"Singhal"},{"creatorType":"author","firstName":"Ravi","lastName":"Ramamoorthi"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Ren","lastName":"Ng"}],"abstractNote":"We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.","publicationTitle":"arXiv:2006.10739 [cs]","volume":"","issue":"","pages":"","date":"2020-06-18","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2006.10739","accessDate":"2021-06-20T16:03:44Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2006.10739","tags":[],"collections":["CPYKW3PF","J4345UQS"],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/N64THQEK"},"dateAdded":"2021-06-20T16:03:44Z","dateModified":"2021-06-20T16:03:44Z"}},{"key":"VYS6ZLXT","version":94,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VYS6ZLXT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VYS6ZLXT","type":"text/html"}},"meta":{"creatorSummary":"Mildenhall et al.","parsedDate":"2020-08-03","numChildren":3},"data":{"key":"VYS6ZLXT","version":94,"itemType":"journalArticle","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","creators":[{"creatorType":"author","firstName":"Ben","lastName":"Mildenhall"},{"creatorType":"author","firstName":"Pratul P.","lastName":"Srinivasan"},{"creatorType":"author","firstName":"Matthew","lastName":"Tancik"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Ravi","lastName":"Ramamoorthi"},{"creatorType":"author","firstName":"Ren","lastName":"Ng"}],"abstractNote":"We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.","publicationTitle":"arXiv:2003.08934 [cs]","volume":"","issue":"","pages":"","date":"2020-08-03","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"NeRF","url":"http://arxiv.org/abs/2003.08934","accessDate":"2021-06-20T16:03:43Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2003.08934","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-06-20T16:03:43Z","dateModified":"2021-06-20T16:03:43Z"}},{"key":"865FJ5C2","version":77,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/865FJ5C2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/865FJ5C2","type":"text/html"}},"meta":{"creatorSummary":"Sitzmann et al.","parsedDate":"2020-06-17","numChildren":3},"data":{"key":"865FJ5C2","version":77,"itemType":"journalArticle","title":"Implicit Neural Representations with Periodic Activation Functions","creators":[{"creatorType":"author","firstName":"Vincent","lastName":"Sitzmann"},{"creatorType":"author","firstName":"Julien N. P.","lastName":"Martel"},{"creatorType":"author","firstName":"Alexander W.","lastName":"Bergman"},{"creatorType":"author","firstName":"David B.","lastName":"Lindell"},{"creatorType":"author","firstName":"Gordon","lastName":"Wetzstein"}],"abstractNote":"Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.","publicationTitle":"arXiv:2006.09661 [cs, eess]","volume":"","issue":"","pages":"","date":"2020-06-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2006.09661","accessDate":"2021-06-20T16:03:43Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2006.09661","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:43Z","dateModified":"2021-06-20T16:03:43Z"}},{"key":"Z9RMBP5X","version":77,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Z9RMBP5X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Z9RMBP5X","type":"text/html"}},"meta":{"creatorSummary":"Yariv et al.","parsedDate":"2020-10-25","numChildren":2},"data":{"key":"Z9RMBP5X","version":77,"itemType":"journalArticle","title":"Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance","creators":[{"creatorType":"author","firstName":"Lior","lastName":"Yariv"},{"creatorType":"author","firstName":"Yoni","lastName":"Kasten"},{"creatorType":"author","firstName":"Dror","lastName":"Moran"},{"creatorType":"author","firstName":"Meirav","lastName":"Galun"},{"creatorType":"author","firstName":"Matan","lastName":"Atzmon"},{"creatorType":"author","firstName":"Ronen","lastName":"Basri"},{"creatorType":"author","firstName":"Yaron","lastName":"Lipman"}],"abstractNote":"In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.","publicationTitle":"arXiv:2003.09852 [cs]","volume":"","issue":"","pages":"","date":"2020-10-25","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2003.09852","accessDate":"2021-06-20T16:03:43Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2003.09852","tags":[],"collections":[],"relations":{},"dateAdded":"2021-06-20T16:03:43Z","dateModified":"2021-06-20T16:03:43Z"}},{"key":"ZTAX693L","version":58,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZTAX693L","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZTAX693L","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VYS6ZLXT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZTAX693L","version":58,"parentItem":"VYS6ZLXT","itemType":"note","note":"Comment: ECCV 2020 (oral). Project page with videos and code: http://tancik.com/nerf","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:43Z","dateModified":"2021-06-20T16:03:43Z"}},{"key":"FNEA2VDY","version":58,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FNEA2VDY","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FNEA2VDY","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/865FJ5C2","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FNEA2VDY","version":58,"parentItem":"865FJ5C2","itemType":"note","note":"Comment: Project website: https://vsitzmann.github.io/siren/ Project video: https://youtu.be/Q2fLWGBeaiI","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:43Z","dateModified":"2021-06-20T16:03:43Z"}},{"key":"5H9I5CSF","version":648,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5H9I5CSF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5H9I5CSF","type":"text/html"}},"meta":{"creatorSummary":"Ho et al.","parsedDate":"2020-12-16","numChildren":2},"data":{"key":"5H9I5CSF","version":648,"itemType":"journalArticle","title":"Denoising Diffusion Probabilistic Models","creators":[{"creatorType":"author","firstName":"Jonathan","lastName":"Ho"},{"creatorType":"author","firstName":"Ajay","lastName":"Jain"},{"creatorType":"author","firstName":"Pieter","lastName":"Abbeel"}],"abstractNote":"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion","publicationTitle":"arXiv:2006.11239 [cs, stat]","volume":"","issue":"","pages":"","date":"2020-12-16","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2006.11239","accessDate":"2021-06-20T16:03:42Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2006.11239","tags":[],"collections":["TNWL7M5C"],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/RQT67FRB","http://zotero.org/groups/4458581/items/L7RVLRQW"]},"dateAdded":"2021-06-20T16:03:42Z","dateModified":"2021-06-20T16:03:42Z"}},{"key":"UPA65XTD","version":182,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UPA65XTD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UPA65XTD","type":"text/html"}},"meta":{"creatorSummary":"Sitzmann et al.","parsedDate":"2020-06-17","numChildren":3},"data":{"key":"UPA65XTD","version":182,"itemType":"journalArticle","title":"MetaSDF: Meta-learning Signed Distance Functions","creators":[{"creatorType":"author","firstName":"Vincent","lastName":"Sitzmann"},{"creatorType":"author","firstName":"Eric R.","lastName":"Chan"},{"creatorType":"author","firstName":"Richard","lastName":"Tucker"},{"creatorType":"author","firstName":"Noah","lastName":"Snavely"},{"creatorType":"author","firstName":"Gordon","lastName":"Wetzstein"}],"abstractNote":"Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.","publicationTitle":"arXiv:2006.09662 [cs]","volume":"","issue":"","pages":"","date":"2020-06-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"MetaSDF","url":"http://arxiv.org/abs/2006.09662","accessDate":"2021-06-20T16:03:40Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2006.09662","tags":[],"collections":["5XFEXSAE"],"relations":{},"dateAdded":"2021-06-20T16:03:40Z","dateModified":"2021-06-20T16:03:40Z"}},{"key":"WHCX2LY5","version":57,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WHCX2LY5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WHCX2LY5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/UPA65XTD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WHCX2LY5","version":57,"parentItem":"UPA65XTD","itemType":"note","note":"Comment: Project website: https://vsitzmann.github.io/metasdf/","tags":[],"relations":{},"dateAdded":"2021-06-20T16:03:40Z","dateModified":"2021-06-20T16:03:40Z"}},{"key":"HT896E7F","version":57,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HT896E7F","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HT896E7F","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZN4Q97I6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HT896E7F","version":57,"parentItem":"ZN4Q97I6","itemType":"attachment","linkMode":"imported_file","title":"Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf","accessDate":"","url":"","note":"","contentType":"application/pdf","charset":"","filename":"Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf","md5":"b1390adda57bbb669fa958add52328c3","mtime":1624199565000,"tags":[],"relations":{},"dateAdded":"2021-06-20T14:32:37Z","dateModified":"2021-06-20T14:32:45Z"}},{"key":"EFRX7579","version":649,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EFRX7579","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EFRX7579","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QWRQKTJA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EFRX7579","version":649,"parentItem":"QWRQKTJA","itemType":"note","note":"<p>Test note</p>","tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/A4TC5D3C","http://zotero.org/groups/4458581/items/BWISMBHL"]},"dateAdded":"2021-06-20T14:07:40Z","dateModified":"2021-06-20T14:07:44Z"}},{"key":"4SKTVI4Y","version":649,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4SKTVI4Y","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4SKTVI4Y","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QWRQKTJA","type":"application/json"}},"meta":{},"data":{"key":"4SKTVI4Y","version":649,"parentItem":"QWRQKTJA","itemType":"attachment","linkMode":"linked_url","title":"https://cascaded-diffusion.github.io/","accessDate":"2021-06-20T14:07:31Z","url":"https://cascaded-diffusion.github.io/","note":"","contentType":"","charset":"","tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/69NQY94S","http://zotero.org/groups/4458581/items/EMMI2ZBZ"]},"dateAdded":"2021-06-20T14:07:31Z","dateModified":"2021-06-20T14:07:31Z"}},{"key":"HGQ9WVQ7","version":649,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HGQ9WVQ7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HGQ9WVQ7","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QWRQKTJA","type":"application/json"},"enclosure":{"type":"application/pdf","href":"https://api.zotero.org/users/7902311/items/HGQ9WVQ7/file/view","title":"Ho et al. - Cascaded Diﬀusion Models for High Fidelity Image G.pdf","length":27985746}},"meta":{"numChildren":0},"data":{"key":"HGQ9WVQ7","version":649,"parentItem":"QWRQKTJA","itemType":"attachment","linkMode":"imported_url","title":"Ho et al. - Cascaded Diﬀusion Models for High Fidelity Image G.pdf","accessDate":"2021-06-20T07:01:38Z","url":"https://cascaded-diffusion.github.io/assets/cascaded_diffusion.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Ho et al. - Cascaded Diﬀusion Models for High Fidelity Image G.pdf","md5":"5c7f9b255a25e3039269467474b591a6","mtime":1624172501000,"tags":[],"relations":{"owl:sameAs":["http://zotero.org/groups/4320173/items/CK7LKVAP","http://zotero.org/groups/4458581/items/I77T94N3"]},"dateAdded":"2021-06-20T07:01:38Z","dateModified":"2021-06-20T07:01:41Z"}},{"key":"H7MHDYWE","version":39,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/H7MHDYWE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/H7MHDYWE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KSTKXVLX","type":"application/json"},"enclosure":{"type":"text/html","href":"https://api.zotero.org/users/7902311/items/H7MHDYWE/file/view"}},"meta":{"numChildren":0},"data":{"key":"H7MHDYWE","version":39,"parentItem":"KSTKXVLX","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-06-07T09:26:06Z","url":"https://arxiv.org/abs/2010.04595","note":"","contentType":"text/html","charset":"utf-8","filename":"2010.html","md5":"f0cd4532ba3d792ca81288036a5640e1","mtime":1623057966000,"tags":[],"relations":{},"dateAdded":"2021-06-07T09:26:06Z","dateModified":"2021-06-07T09:26:06Z"}},{"key":"2JA6F8X6","version":35,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2JA6F8X6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2JA6F8X6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KSTKXVLX","type":"application/json"},"enclosure":{"type":"application/pdf","href":"https://api.zotero.org/users/7902311/items/2JA6F8X6/file/view","title":"Trevithick and Yang - 2020 - GRF Learning a General Radiance Field for 3D Scen.pdf","length":24114521}},"meta":{"numChildren":0},"data":{"key":"2JA6F8X6","version":35,"parentItem":"KSTKXVLX","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-06-07T09:26:01Z","url":"https://arxiv.org/pdf/2010.04595.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Trevithick and Yang - 2020 - GRF Learning a General Radiance Field for 3D Scen.pdf","md5":"775f53af9fedb8aa732457fb4610a400","mtime":1623057961000,"tags":[],"relations":{},"dateAdded":"2021-06-07T09:26:01Z","dateModified":"2021-06-07T09:26:01Z"}},{"key":"39SNY222","version":23,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/39SNY222","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/39SNY222","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KSTKXVLX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"39SNY222","version":23,"parentItem":"KSTKXVLX","itemType":"note","note":"Comment: Code and data are available at: https://github.com/alextrevithick/GRF","tags":[],"relations":{},"dateAdded":"2021-06-07T09:23:49Z","dateModified":"2021-06-07T09:23:49Z"}},{"key":"2WG37TIK","version":122,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2WG37TIK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2WG37TIK","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KHSD5WHK","type":"application/json"},"enclosure":{"type":"text/html","href":"https://api.zotero.org/users/7902311/items/2WG37TIK/file/view"}},"meta":{"numChildren":0},"data":{"key":"2WG37TIK","version":122,"parentItem":"KHSD5WHK","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-04-26T04:46:14Z","url":"https://arxiv.org/abs/2104.00677","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"c27e67968d251e1d9f071977caac66c3","mtime":1619412374441,"tags":[],"relations":{},"dateAdded":"2021-04-26T04:46:14Z","dateModified":"2021-04-26T04:46:14Z"}},{"key":"C952EH2V","version":122,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/C952EH2V","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/C952EH2V","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KHSD5WHK","type":"application/json"},"enclosure":{"type":"application/pdf","href":"https://api.zotero.org/users/7902311/items/C952EH2V/file/view","title":"Jain et al. - 2021 - Putting NeRF on a Diet Semantically Consistent Fe.pdf","length":48381864}},"meta":{"numChildren":0},"data":{"key":"C952EH2V","version":122,"parentItem":"KHSD5WHK","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-04-26T04:46:07Z","url":"https://arxiv.org/pdf/2104.00677.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Jain et al. - 2021 - Putting NeRF on a Diet Semantically Consistent Fe.pdf","md5":"170929df9864fb46c1097a205ee2bb3a","mtime":1619412367439,"tags":[],"relations":{},"dateAdded":"2021-04-26T04:46:07Z","dateModified":"2021-04-26T04:46:07Z"}},{"key":"KHSD5WHK","version":94,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KHSD5WHK","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KHSD5WHK","type":"text/html"},"attachment":{"href":"https://api.zotero.org/users/7902311/items/C952EH2V","type":"application/json","attachmentType":"application/pdf","attachmentSize":48381864}},"meta":{"creatorSummary":"Jain et al.","parsedDate":"2021-04-01","numChildren":3},"data":{"key":"KHSD5WHK","version":94,"itemType":"journalArticle","title":"Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis","creators":[{"creatorType":"author","firstName":"Ajay","lastName":"Jain"},{"creatorType":"author","firstName":"Matthew","lastName":"Tancik"},{"creatorType":"author","firstName":"Pieter","lastName":"Abbeel"}],"abstractNote":"We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360{\\deg} scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.","publicationTitle":"arXiv:2104.00677 [cs]","volume":"","issue":"","pages":"","date":"2021-04-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"Putting NeRF on a Diet","url":"http://arxiv.org/abs/2104.00677","accessDate":"2021-04-26T04:42:06Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.00677","tags":[],"collections":["CPYKW3PF"],"relations":{},"dateAdded":"2021-04-26T04:42:06Z","dateModified":"2021-04-26T04:42:06Z"}},{"key":"WCX8EDFW","version":11,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WCX8EDFW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WCX8EDFW","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KHSD5WHK","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WCX8EDFW","version":11,"parentItem":"KHSD5WHK","itemType":"note","note":"<p>Comment: Project website: https://www.ajayj.com/dietnerf</p>","tags":[],"relations":{},"dateAdded":"2021-04-26T04:42:06Z","dateModified":"2021-04-26T04:41:34Z"}},{"key":"GVJSXIF3","version":362,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GVJSXIF3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GVJSXIF3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WSXPLJSX","type":"application/json"},"enclosure":{"type":"text/html","href":"https://api.zotero.org/users/7902311/items/GVJSXIF3/file/view"}},"meta":{"numChildren":0},"data":{"key":"GVJSXIF3","version":362,"parentItem":"WSXPLJSX","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-04-26T04:31:51Z","url":"https://arxiv.org/abs/2012.02190","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"9aca628bac5ee958dbc2f50c2ccd49e6","mtime":1619411511000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/ENI87CE3"},"dateAdded":"2021-04-26T04:31:51Z","dateModified":"2021-04-26T04:31:51Z"}},{"key":"F9DCKB7B","version":362,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/F9DCKB7B","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/F9DCKB7B","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WSXPLJSX","type":"application/json"},"enclosure":{"type":"application/pdf","href":"https://api.zotero.org/users/7902311/items/F9DCKB7B/file/view","title":"Yu et al. - 2020 - pixelNeRF Neural Radiance Fields from One or Few .pdf","length":6493563}},"meta":{"numChildren":0},"data":{"key":"F9DCKB7B","version":362,"parentItem":"WSXPLJSX","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-04-26T04:31:46Z","url":"https://arxiv.org/pdf/2012.02190.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Yu et al. - 2020 - pixelNeRF Neural Radiance Fields from One or Few .pdf","md5":"1af265f3cbb93541b01337852c3b23d9","mtime":1619411506000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/CCMHFP7N"},"dateAdded":"2021-04-26T04:31:46Z","dateModified":"2021-04-26T04:31:46Z"}},{"key":"HRWHKQT3","version":122,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HRWHKQT3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HRWHKQT3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HNYWQ5PZ","type":"application/json"},"enclosure":{"type":"text/html","href":"https://api.zotero.org/users/7902311/items/HRWHKQT3/file/view"}},"meta":{"numChildren":0},"data":{"key":"HRWHKQT3","version":122,"parentItem":"HNYWQ5PZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2021-04-26T04:31:17Z","url":"https://arxiv.org/abs/2007.02442","note":"","contentType":"text/html","charset":"utf-8","filename":"2007.html","md5":"dff6634e12a11c73dd53b784ca69dd4e","mtime":1619411477543,"tags":[],"relations":{},"dateAdded":"2021-04-26T04:31:17Z","dateModified":"2021-04-26T04:31:17Z"}},{"key":"TEECTE59","version":122,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TEECTE59","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TEECTE59","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/HNYWQ5PZ","type":"application/json"},"enclosure":{"type":"application/pdf","href":"https://api.zotero.org/users/7902311/items/TEECTE59/file/view","title":"Schwarz et al. - 2021 - GRAF Generative Radiance Fields for 3D-Aware Imag.pdf","length":5592753}},"meta":{"numChildren":0},"data":{"key":"TEECTE59","version":122,"parentItem":"HNYWQ5PZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2021-04-26T04:31:12Z","url":"https://arxiv.org/pdf/2007.02442.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Schwarz et al. - 2021 - GRAF Generative Radiance Fields for 3D-Aware Imag.pdf","md5":"3c10edc444c32574bda1b0d20a7f0bb0","mtime":1619411472543,"tags":[],"relations":{},"dateAdded":"2021-04-26T04:31:12Z","dateModified":"2021-04-26T04:31:12Z"}},{"key":"HNYWQ5PZ","version":77,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HNYWQ5PZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HNYWQ5PZ","type":"text/html"},"attachment":{"href":"https://api.zotero.org/users/7902311/items/TEECTE59","type":"application/json","attachmentType":"application/pdf","attachmentSize":5592753}},"meta":{"creatorSummary":"Schwarz et al.","parsedDate":"2021-03-30","numChildren":2},"data":{"key":"HNYWQ5PZ","version":77,"itemType":"journalArticle","title":"GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis","creators":[{"creatorType":"author","firstName":"Katja","lastName":"Schwarz"},{"creatorType":"author","firstName":"Yiyi","lastName":"Liao"},{"creatorType":"author","firstName":"Michael","lastName":"Niemeyer"},{"creatorType":"author","firstName":"Andreas","lastName":"Geiger"}],"abstractNote":"While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.","publicationTitle":"arXiv:2007.02442 [cs]","volume":"","issue":"","pages":"","date":"2021-03-30","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"GRAF","url":"http://arxiv.org/abs/2007.02442","accessDate":"2021-04-26T04:30:42Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2007.02442","tags":[],"collections":[],"relations":{},"dateAdded":"2021-04-26T04:30:42Z","dateModified":"2021-04-26T04:30:42Z"}},{"key":"E7H6JBJ3","version":1078,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/E7H6JBJ3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/E7H6JBJ3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WNN2PALS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"E7H6JBJ3","version":1078,"parentItem":"WNN2PALS","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-03-18T06:55:03Z","url":"https://arxiv.org/abs/2202.00512","note":"","contentType":"text/html","charset":"utf-8","filename":"2202.html","md5":"b35120eea4fd4ffa30dc21aa755738b2","mtime":1647586503000,"tags":[],"relations":{},"dateAdded":"2022-03-18T06:55:03Z","dateModified":"2022-03-18T06:55:03Z"}},{"key":"ICCZGRIR","version":1078,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ICCZGRIR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ICCZGRIR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WNN2PALS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ICCZGRIR","version":1078,"parentItem":"WNN2PALS","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-03-18T06:54:56Z","url":"https://arxiv.org/pdf/2202.00512.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Salimans and Ho - 2022 - Progressive Distillation for Fast Sampling of Diff.pdf","md5":"a15f544012c748869c385463840292b4","mtime":1647586496000,"tags":[],"relations":{},"dateAdded":"2022-03-18T06:54:56Z","dateModified":"2022-03-18T06:54:56Z"}},{"key":"WNN2PALS","version":1076,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WNN2PALS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WNN2PALS","type":"text/html"}},"meta":{"creatorSummary":"Salimans and Ho","parsedDate":"2022-02-01","numChildren":3},"data":{"key":"WNN2PALS","version":1076,"itemType":"journalArticle","title":"Progressive Distillation for Fast Sampling of Diffusion Models","creators":[{"creatorType":"author","firstName":"Tim","lastName":"Salimans"},{"creatorType":"author","firstName":"Jonathan","lastName":"Ho"}],"abstractNote":"Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers taking as many as 8192 steps, and are able to distill down to models taking as few as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.","publicationTitle":"arXiv:2202.00512 [cs, stat]","volume":"","issue":"","pages":"","date":"2022-02-01","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2202.00512","accessDate":"2022-03-18T06:54:36Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2202.00512","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-03-18T06:54:36Z","dateModified":"2022-03-18T06:54:36Z"}},{"key":"GBEYG2Q5","version":1076,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GBEYG2Q5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GBEYG2Q5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WNN2PALS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GBEYG2Q5","version":1076,"parentItem":"WNN2PALS","itemType":"note","note":"Comment: Published as a conference paper at ICLR 2022","tags":[],"relations":{},"dateAdded":"2022-03-18T06:54:36Z","dateModified":"2022-03-18T06:54:36Z"}},{"key":"ZJQARVFC","version":1074,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZJQARVFC","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZJQARVFC","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/7DDX978P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZJQARVFC","version":1074,"parentItem":"7DDX978P","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-03-12T17:37:16Z","url":"https://arxiv.org/abs/2111.15640","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"60cd6dedd33a26825ce69b370309ff68","mtime":1647106636000,"tags":[],"relations":{},"dateAdded":"2022-03-12T17:37:16Z","dateModified":"2022-03-12T17:37:16Z"}},{"key":"F7KKFN2N","version":1074,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/F7KKFN2N","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/F7KKFN2N","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/7DDX978P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"F7KKFN2N","version":1074,"parentItem":"7DDX978P","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-03-12T17:37:08Z","url":"https://arxiv.org/pdf/2111.15640.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Preechakul et al. - 2022 - Diffusion Autoencoders Toward a Meaningful and De.pdf","md5":"9965c5558098f4ce5bdf2119780164aa","mtime":1647106628000,"tags":[],"relations":{},"dateAdded":"2022-03-12T17:37:08Z","dateModified":"2022-03-12T17:37:08Z"}},{"key":"P22TZT62","version":1071,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/P22TZT62","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/P22TZT62","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/7DDX978P","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"P22TZT62","version":1071,"parentItem":"7DDX978P","itemType":"note","note":"Comment: Please visit our project page: https://Diff-AE.github.io/","tags":[],"relations":{},"dateAdded":"2022-03-12T17:34:16Z","dateModified":"2022-03-12T17:34:16Z"}},{"key":"IMMDADKD","version":1185,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IMMDADKD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IMMDADKD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/G3EK5DHW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"IMMDADKD","version":1185,"parentItem":"G3EK5DHW","itemType":"note","note":"Comment: Code available at https://www.github.com/wpeebles/gangealing . Project page and videos available at https://www.wpeebles.com/gangealing","tags":[],"relations":{},"dateAdded":"2022-03-30T08:19:06Z","dateModified":"2022-06-13T16:19:43Z"}},{"key":"S4AZE83Q","version":1079,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/S4AZE83Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/S4AZE83Q","type":"text/html"}},"meta":{"creatorSummary":"Peebles et al.","parsedDate":"2021-12-09","numChildren":1},"data":{"key":"S4AZE83Q","version":1079,"itemType":"journalArticle","title":"GAN-Supervised Dense Visual Alignment","creators":[{"creatorType":"author","firstName":"William","lastName":"Peebles"},{"creatorType":"author","firstName":"Jun-Yan","lastName":"Zhu"},{"creatorType":"author","firstName":"Richard","lastName":"Zhang"},{"creatorType":"author","firstName":"Antonio","lastName":"Torralba"},{"creatorType":"author","firstName":"Alexei","lastName":"Efros"},{"creatorType":"author","firstName":"Eli","lastName":"Shechtman"}],"abstractNote":"We propose GAN-Supervised Learning, a framework for learning discriminative models and their GAN-generated training data jointly end-to-end. We apply our framework to the dense visual alignment problem. Inspired by the classic Congealing method, our GANgealing algorithm trains a Spatial Transformer to map random samples from a GAN trained on unaligned data to a common, jointly-learned target mode. We show results on eight datasets, all of which demonstrate our method successfully aligns complex data and discovers dense correspondences. GANgealing significantly outperforms past self-supervised correspondence algorithms and performs on-par with (and sometimes exceeds) state-of-the-art supervised correspondence algorithms on several datasets -- without making use of any correspondence supervision or data augmentation and despite being trained exclusively on GAN-generated data. For precise correspondence, we improve upon state-of-the-art supervised methods by as much as $3\\times$. We show applications of our method for augmented reality, image editing and automated pre-processing of image datasets for downstream GAN training.","publicationTitle":"arXiv:2112.05143 [cs]","volume":"","issue":"","pages":"","date":"2021-12-09","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2112.05143","accessDate":"2022-03-30T08:19:06Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2112.05143","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-03-30T08:19:06Z","dateModified":"2022-03-30T08:19:06Z"}},{"key":"6P2DRGYH","version":1089,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6P2DRGYH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6P2DRGYH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IUD2CXDQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6P2DRGYH","version":1089,"parentItem":"IUD2CXDQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-03-30T09:49:27Z","url":"https://arxiv.org/abs/2203.09517","note":"","contentType":"text/html","charset":"utf-8","filename":"2203.html","md5":"b17e452fc5750fe6c6d13407ffed4a25","mtime":1648633767000,"tags":[],"relations":{},"dateAdded":"2022-03-30T09:49:27Z","dateModified":"2022-03-30T09:49:27Z"}},{"key":"NEFPLDBX","version":1089,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NEFPLDBX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NEFPLDBX","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IUD2CXDQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NEFPLDBX","version":1089,"parentItem":"IUD2CXDQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-03-30T09:49:12Z","url":"https://arxiv.org/pdf/2203.09517.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chen et al. - 2022 - TensoRF Tensorial Radiance Fields.pdf","md5":"a0a1d44559d1ebab48764d8f575b6fc4","mtime":1648633752000,"tags":[],"relations":{},"dateAdded":"2022-03-30T09:49:12Z","dateModified":"2022-03-30T09:49:12Z"}},{"key":"3LMFRNNE","version":1086,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3LMFRNNE","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3LMFRNNE","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IUD2CXDQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3LMFRNNE","version":1086,"parentItem":"IUD2CXDQ","itemType":"note","note":"Comment: Project Page: https://apchenstu.github.io/TensoRF/","tags":[],"relations":{},"dateAdded":"2022-03-30T09:46:54Z","dateModified":"2022-03-30T09:46:54Z"}},{"key":"IUD2CXDQ","version":1086,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IUD2CXDQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IUD2CXDQ","type":"text/html"}},"meta":{"creatorSummary":"Chen et al.","parsedDate":"2022-03-17","numChildren":3},"data":{"key":"IUD2CXDQ","version":1086,"itemType":"journalArticle","title":"TensoRF: Tensorial Radiance Fields","creators":[{"creatorType":"author","firstName":"Anpei","lastName":"Chen"},{"creatorType":"author","firstName":"Zexiang","lastName":"Xu"},{"creatorType":"author","firstName":"Andreas","lastName":"Geiger"},{"creatorType":"author","firstName":"Jingyi","lastName":"Yu"},{"creatorType":"author","firstName":"Hao","lastName":"Su"}],"abstractNote":"We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).","publicationTitle":"arXiv:2203.09517 [cs]","volume":"","issue":"","pages":"","date":"2022-03-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"TensoRF","url":"http://arxiv.org/abs/2203.09517","accessDate":"2022-03-30T09:46:54Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2203.09517","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-03-30T09:46:54Z","dateModified":"2022-03-30T09:46:54Z"}},{"key":"SEWFH344","version":1082,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SEWFH344","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SEWFH344","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S4AZE83Q","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SEWFH344","version":1082,"parentItem":"S4AZE83Q","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-03-30T08:23:56Z","url":"https://arxiv.org/abs/2112.05143","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"652d42a84a59e425bc60c912c235365f","mtime":1648628635000,"tags":[],"relations":{},"dateAdded":"2022-03-30T08:23:56Z","dateModified":"2022-03-30T08:23:56Z"}},{"key":"PYEHP8HT","version":1082,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PYEHP8HT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PYEHP8HT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/S4AZE83Q","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"PYEHP8HT","version":1082,"parentItem":"S4AZE83Q","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-03-30T08:23:48Z","url":"https://arxiv.org/pdf/2112.05143.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Peebles et al. - 2021 - GAN-Supervised Dense Visual Alignment.pdf","md5":"18fc54d508c326bd134cf1cbe42dfe9f","mtime":1648628628000,"tags":[],"relations":{},"dateAdded":"2022-03-30T08:23:48Z","dateModified":"2022-03-30T08:23:48Z"}},{"key":"472SR27Q","version":1099,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/472SR27Q","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/472SR27Q","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/SGPYRTWW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"472SR27Q","version":1099,"parentItem":"SGPYRTWW","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-03-30T13:46:05Z","url":"https://arxiv.org/abs/2010.16402","note":"","contentType":"text/html","charset":"utf-8","filename":"2010.html","md5":"a166a081d017fcf60af69c858bad4b0c","mtime":1648647965000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/6BQ5VJ2S"},"dateAdded":"2022-03-30T13:46:05Z","dateModified":"2022-03-30T13:46:05Z"}},{"key":"8X694Z7P","version":1099,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8X694Z7P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8X694Z7P","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/SGPYRTWW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8X694Z7P","version":1099,"parentItem":"SGPYRTWW","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-03-30T13:45:59Z","url":"https://arxiv.org/pdf/2010.16402.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kornblith et al. - 2021 - Why Do Better Loss Functions Lead to Less Transfer.pdf","md5":"b0d61809a523dca4e3d66630513a43ea","mtime":1648647959000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/HGQVVV6X"},"dateAdded":"2022-03-30T13:45:59Z","dateModified":"2022-03-30T13:45:59Z"}},{"key":"77SLFMRH","version":1099,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/77SLFMRH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/77SLFMRH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/SGPYRTWW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"77SLFMRH","version":1099,"parentItem":"SGPYRTWW","itemType":"note","note":"Comment: NeurIPS 2021","tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/YD4FQM8N"},"dateAdded":"2022-03-30T13:45:43Z","dateModified":"2022-03-30T13:45:43Z"}},{"key":"SGPYRTWW","version":1099,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SGPYRTWW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SGPYRTWW","type":"text/html"}},"meta":{"creatorSummary":"Kornblith et al.","parsedDate":"2021-11-03","numChildren":3},"data":{"key":"SGPYRTWW","version":1099,"itemType":"journalArticle","title":"Why Do Better Loss Functions Lead to Less Transferable Features?","creators":[{"creatorType":"author","firstName":"Simon","lastName":"Kornblith"},{"creatorType":"author","firstName":"Ting","lastName":"Chen"},{"creatorType":"author","firstName":"Honglak","lastName":"Lee"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"}],"abstractNote":"Previous work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks.","publicationTitle":"arXiv:2010.16402 [cs]","volume":"","issue":"","pages":"","date":"2021-11-03","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2010.16402","accessDate":"2022-03-30T13:45:43Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2010.16402","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/CB3Y5S8R"},"dateAdded":"2022-03-30T13:45:43Z","dateModified":"2022-03-30T13:45:43Z"}},{"key":"JWC2N8JS","version":1097,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JWC2N8JS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JWC2N8JS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZEFKYLKZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JWC2N8JS","version":1097,"parentItem":"ZEFKYLKZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-03-30T14:26:00Z","url":"https://arxiv.org/abs/2202.01197","note":"","contentType":"text/html","charset":"utf-8","filename":"2202.html","md5":"67877bfd3631dd8e9badd898d554abd1","mtime":1648650360000,"tags":[],"relations":{},"dateAdded":"2022-03-30T14:26:00Z","dateModified":"2022-03-30T14:26:00Z"}},{"key":"KSKWLKXU","version":1097,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KSKWLKXU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KSKWLKXU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZEFKYLKZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KSKWLKXU","version":1097,"parentItem":"ZEFKYLKZ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-03-30T14:25:49Z","url":"https://arxiv.org/pdf/2202.01197.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Du et al. - 2022 - VOS Learning What You Don't Know by Virtual Outli.pdf","md5":"9444398e42a7c8fcaff78d97efc5db8f","mtime":1648650349000,"tags":[],"relations":{},"dateAdded":"2022-03-30T14:25:49Z","dateModified":"2022-03-30T14:25:49Z"}},{"key":"XW86MDTH","version":1095,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XW86MDTH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XW86MDTH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ZEFKYLKZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XW86MDTH","version":1095,"parentItem":"ZEFKYLKZ","itemType":"note","note":"Comment: ICLR 2022","tags":[],"relations":{},"dateAdded":"2022-03-30T14:24:32Z","dateModified":"2022-03-30T14:24:32Z"}},{"key":"ZEFKYLKZ","version":1095,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZEFKYLKZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZEFKYLKZ","type":"text/html"}},"meta":{"creatorSummary":"Du et al.","parsedDate":"2022-02-04","numChildren":3},"data":{"key":"ZEFKYLKZ","version":1095,"itemType":"journalArticle","title":"VOS: Learning What You Don't Know by Virtual Outlier Synthesis","creators":[{"creatorType":"author","firstName":"Xuefeng","lastName":"Du"},{"creatorType":"author","firstName":"Zhaoning","lastName":"Wang"},{"creatorType":"author","firstName":"Mu","lastName":"Cai"},{"creatorType":"author","firstName":"Yixuan","lastName":"Li"}],"abstractNote":"Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at https://github.com/deeplearning-wisc/vos.","publicationTitle":"arXiv:2202.01197 [cs]","volume":"","issue":"","pages":"","date":"2022-02-04","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"VOS","url":"http://arxiv.org/abs/2202.01197","accessDate":"2022-03-30T14:24:32Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2202.01197","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-03-30T14:24:32Z","dateModified":"2022-03-30T14:24:32Z"}},{"key":"57VCFF6V","version":1101,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/57VCFF6V","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/57VCFF6V","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IB7W7QST","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"57VCFF6V","version":1101,"parentItem":"IB7W7QST","itemType":"attachment","linkMode":"imported_url","title":"Ramesh et al. - Hierarchical Text-Conditional Image Generation wit.pdf","accessDate":"2022-04-07T14:16:45Z","url":"https://cdn.openai.com/papers/dall-e-2.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Ramesh et al. - Hierarchical Text-Conditional Image Generation wit.pdf","md5":"0a817e8354506b2a441c089d46c0a9ae","mtime":1649346400000,"tags":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/ZHSX4SVG"},"dateAdded":"2022-04-07T15:46:40Z","dateModified":"2022-04-07T15:46:40Z"}},{"key":"IB7W7QST","version":1100,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IB7W7QST","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IB7W7QST","type":"text/html"}},"meta":{"creatorSummary":"Ramesh et al.","numChildren":1},"data":{"key":"IB7W7QST","version":1100,"itemType":"journalArticle","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents","creators":[{"creatorType":"author","firstName":"Aditya","lastName":"Ramesh"},{"creatorType":"author","firstName":"Prafulla","lastName":"Dhariwal"},{"creatorType":"author","firstName":"Alex","lastName":"Nichol"},{"creatorType":"author","firstName":"Casey","lastName":"Chu"},{"creatorType":"author","firstName":"Mark","lastName":"Chen"}],"abstractNote":"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.","publicationTitle":"","volume":"","issue":"","pages":"24","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{"owl:sameAs":"http://zotero.org/groups/4320173/items/VYKGSVRN"},"dateAdded":"2022-04-07T15:46:40Z","dateModified":"2022-04-07T15:46:40Z"}},{"key":"VDPXTFRA","version":1104,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VDPXTFRA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VDPXTFRA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BK54G5H8","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VDPXTFRA","version":1104,"parentItem":"BK54G5H8","itemType":"attachment","linkMode":"imported_url","title":"Suwajanakorn et al. - Discovery of Latent 3D Keypoints via End-to-end Ge.pdf","accessDate":"2022-04-22T05:00:25Z","url":"https://keypointnet.github.io/keypointnet_neurips.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Suwajanakorn et al. - Discovery of Latent 3D Keypoints via End-to-end Ge.pdf","md5":"12e5d77ffa5bc30c7f0852ea286b05bc","mtime":1650603636000,"tags":[],"relations":{},"dateAdded":"2022-04-22T05:00:25Z","dateModified":"2022-04-22T05:00:37Z"}},{"key":"BK54G5H8","version":1103,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BK54G5H8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BK54G5H8","type":"text/html"}},"meta":{"creatorSummary":"Suwajanakorn et al.","numChildren":1},"data":{"key":"BK54G5H8","version":1103,"itemType":"journalArticle","title":"Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning","creators":[{"creatorType":"author","firstName":"Supasorn","lastName":"Suwajanakorn"},{"creatorType":"author","firstName":"Noah","lastName":"Snavely"},{"creatorType":"author","firstName":"Jonathan J","lastName":"Tompson"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"}],"abstractNote":"This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-speciﬁc 3D keypoints, along with their detectors. Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task. We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category. Importantly, we ﬁnd that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation. The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet [6] are visualized at keypointnet.github.io.","publicationTitle":"","volume":"","issue":"","pages":"12","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2022-04-22T05:00:36Z","dateModified":"2022-04-22T05:00:36Z"}},{"key":"9WDHCWDA","version":1138,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9WDHCWDA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9WDHCWDA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CULFMLZ4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9WDHCWDA","version":1138,"parentItem":"CULFMLZ4","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-05-04T11:01:23Z","url":"https://arxiv.org/abs/2204.13902","note":"","contentType":"text/html","charset":"utf-8","filename":"2204.html","md5":"3584dede2cd402c7d88771beb3a25c86","mtime":1651662083000,"tags":[],"relations":{},"dateAdded":"2022-05-04T11:01:23Z","dateModified":"2022-05-04T11:01:23Z"}},{"key":"NHICVIXI","version":1137,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NHICVIXI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NHICVIXI","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CULFMLZ4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NHICVIXI","version":1137,"parentItem":"CULFMLZ4","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-05-04T11:01:17Z","url":"https://arxiv.org/pdf/2204.13902.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhang and Chen - 2022 - Fast Sampling of Diffusion Models with Exponential.pdf","md5":"47e7ccb2fe37c6fd1bcbb4a0252f649f","mtime":1651662077000,"tags":[],"relations":{},"dateAdded":"2022-05-04T11:01:17Z","dateModified":"2022-05-04T11:01:17Z"}},{"key":"5LQV5C3V","version":1191,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5LQV5C3V","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5LQV5C3V","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XQX5CAYV","type":"application/json"}},"meta":{},"data":{"key":"5LQV5C3V","version":1191,"parentItem":"XQX5CAYV","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-05-04T11:00:45Z","url":"https://arxiv.org/abs/2106.01357","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"5c38734ab6e0491234eb4c157339fc76","mtime":1651662045000,"tags":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/HR3FYC7K"},"dateAdded":"2022-05-04T11:00:45Z","dateModified":"2022-06-13T16:20:30Z"}},{"key":"C4R5K3GM","version":1191,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/C4R5K3GM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/C4R5K3GM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XQX5CAYV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"C4R5K3GM","version":1191,"parentItem":"XQX5CAYV","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-05-04T11:00:37Z","url":"https://arxiv.org/pdf/2106.01357.pdf","note":"","contentType":"application/pdf","charset":"","filename":"De Bortoli et al. - 2021 - Diffusion Schrodinger Bridge with Applications t.pdf","md5":"db0aa26c5ce476ca729b35f09fbdbee3","mtime":1651662037000,"tags":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/5GW2VJIJ"},"dateAdded":"2022-05-04T11:00:37Z","dateModified":"2022-06-13T16:20:30Z"}},{"key":"CULFMLZ4","version":1132,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CULFMLZ4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CULFMLZ4","type":"text/html"}},"meta":{"creatorSummary":"Zhang and Chen","parsedDate":"2022-04-29","numChildren":3},"data":{"key":"CULFMLZ4","version":1132,"itemType":"journalArticle","title":"Fast Sampling of Diffusion Models with Exponential Integrator","creators":[{"creatorType":"author","firstName":"Qinsheng","lastName":"Zhang"},{"creatorType":"author","firstName":"Yongxin","lastName":"Chen"}],"abstractNote":"The past few years have witnessed the great success of Diffusion models~(DMs) in generating high-fidelity samples in generative modeling tasks. A major limitation of the DM is its notoriously slow sampling procedure which normally requires hundreds to thousands of time discretization steps of the learned diffusion process to reach the desired accuracy. Our goal is to develop a fast sampling method for DMs with much less number of steps while retaining high sample quality. To this end, we systematically analyze the sampling procedure in DMs and identify key factors that affect the sample quality, among which the method of discretization is most crucial. By carefully examining the learned diffusion process, we propose Diffusion Exponential Integrator Sampler~(DEIS). It is based on the Exponential Integrator designed for discretizing ordinary differential equations (ODEs) and leverages a semilinear structure of the learned diffusion process to reduce the discretization error. The proposed method can be applied to any DMs and can generate high-fidelity samples in as few as 10 steps. In our experiments, it takes about 3 minutes on one A6000 GPU to generate $50k$ images from CIFAR10. Moreover, by directly using pre-trained DMs, we achieve the state-of-art sampling performance when the number of score function evaluation~(NFE) is limited, e.g., 3.37 FID and 9.74 Inception score with only 15 NFEs on CIFAR10.","publicationTitle":"arXiv:2204.13902 [cs]","volume":"","issue":"","pages":"","date":"2022-04-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2204.13902","accessDate":"2022-05-04T10:59:58Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2204.13902","tags":[{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-05-04T10:59:59Z","dateModified":"2022-05-04T10:59:59Z"}},{"key":"BMBP4EG6","version":1132,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BMBP4EG6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BMBP4EG6","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CULFMLZ4","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"BMBP4EG6","version":1132,"parentItem":"CULFMLZ4","itemType":"note","note":"Comment: 22 pages,16 figures","tags":[],"relations":{},"dateAdded":"2022-05-04T10:59:59Z","dateModified":"2022-05-04T10:59:59Z"}},{"key":"3A9V2F8S","version":1132,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3A9V2F8S","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3A9V2F8S","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XQX5CAYV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3A9V2F8S","version":1132,"parentItem":"XQX5CAYV","itemType":"note","note":"Comment: 58 pages, 18 figures (correction of Proposition 5)","tags":[],"relations":{},"dateAdded":"2022-05-04T10:59:55Z","dateModified":"2022-05-04T10:59:55Z"}},{"key":"XQX5CAYV","version":1191,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XQX5CAYV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XQX5CAYV","type":"text/html"}},"meta":{"creatorSummary":"De Bortoli et al.","parsedDate":"2021-12-06","numChildren":4},"data":{"key":"XQX5CAYV","version":1191,"itemType":"journalArticle","title":"Diffusion Schr\\\"odinger Bridge with Applications to Score-Based Generative Modeling","creators":[{"creatorType":"author","firstName":"Valentin","lastName":"De Bortoli"},{"creatorType":"author","firstName":"James","lastName":"Thornton"},{"creatorType":"author","firstName":"Jeremy","lastName":"Heng"},{"creatorType":"author","firstName":"Arnaud","lastName":"Doucet"}],"abstractNote":"Progressively applying Gaussian noise transforms complex data distributions to approximately Gaussian. Reversing this dynamic defines a generative model. When the forward noising process is given by a Stochastic Differential Equation (SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the associated reverse-time SDE may be estimated using score-matching. A limitation of this approach is that the forward-time SDE must be run for a sufficiently long time for the final distribution to be approximately Gaussian. In contrast, solving the Schr\\\"odinger Bridge problem (SB), i.e. an entropy-regularized optimal transport problem on path spaces, yields diffusions which generate samples from the data distribution in finite time. We present Diffusion SB (DSB), an original approximation of the Iterative Proportional Fitting (IPF) procedure to solve the SB problem, and provide theoretical analysis along with generative modeling experiments. The first DSB iteration recovers the methodology proposed by Song et al. (2021), with the flexibility of using shorter time intervals, as subsequent DSB iterations reduce the discrepancy between the final-time marginal of the forward (resp. backward) SDE with respect to the prior (resp. data) distribution. Beyond generative modeling, DSB offers a widely applicable computational optimal transport tool as the continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi, 2013).","publicationTitle":"arXiv:2106.01357 [cs, math, stat]","volume":"","issue":"","pages":"","date":"2021-12-06","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2106.01357","accessDate":"2022-05-04T10:59:55Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.01357","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Mathematics - Probability","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{"dc:replaces":"http://zotero.org/users/7902311/items/P5DLYJAB"},"dateAdded":"2022-05-04T10:59:55Z","dateModified":"2022-06-13T16:20:30Z"}},{"key":"S3SYZPML","version":1130,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/S3SYZPML","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/S3SYZPML","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/L46M3FDT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"S3SYZPML","version":1130,"parentItem":"L46M3FDT","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-04-27T08:35:54Z","url":"https://arxiv.org/abs/2103.12340","note":"","contentType":"text/html","charset":"utf-8","filename":"2103.html","md5":"02950b7b33a5aaa87f8fd514e9485ff3","mtime":1651048554000,"tags":[],"relations":{},"dateAdded":"2022-04-27T08:35:54Z","dateModified":"2022-04-27T08:35:54Z"}},{"key":"4Y3Z3LA4","version":1130,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4Y3Z3LA4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4Y3Z3LA4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/L46M3FDT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"4Y3Z3LA4","version":1130,"parentItem":"L46M3FDT","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-04-27T08:35:48Z","url":"https://arxiv.org/pdf/2103.12340.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Ke et al. - 2021 - Deep Occlusion-Aware Instance Segmentation with Ov.pdf","md5":"cf575f6a9451a7db02e85728a7ffce30","mtime":1651048548000,"tags":[],"relations":{},"dateAdded":"2022-04-27T08:35:48Z","dateModified":"2022-04-27T08:35:48Z"}},{"key":"5Q9PN4TH","version":1126,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5Q9PN4TH","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5Q9PN4TH","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KJIQZEGZ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5Q9PN4TH","version":1126,"parentItem":"KJIQZEGZ","itemType":"attachment","linkMode":"imported_url","title":"Zhan et al. - 2020 - Self-Supervised Scene De-Occlusion.pdf","accessDate":"2022-04-27T08:33:18Z","url":"https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhan_Self-Supervised_Scene_De-Occlusion_CVPR_2020_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhan et al. - 2020 - Self-Supervised Scene De-Occlusion.pdf","md5":"e066f8d3b29b6c7c845e6192eda23c5f","mtime":1651048478000,"tags":[],"relations":{},"dateAdded":"2022-04-27T08:33:18Z","dateModified":"2022-04-27T08:34:39Z"}},{"key":"KJIQZEGZ","version":1124,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KJIQZEGZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KJIQZEGZ","type":"text/html"}},"meta":{"creatorSummary":"Zhan et al.","parsedDate":"2020","numChildren":1},"data":{"key":"KJIQZEGZ","version":1124,"itemType":"conferencePaper","title":"Self-Supervised Scene De-Occlusion","creators":[{"creatorType":"author","firstName":"Xiaohang","lastName":"Zhan"},{"creatorType":"author","firstName":"Xingang","lastName":"Pan"},{"creatorType":"author","firstName":"Bo","lastName":"Dai"},{"creatorType":"author","firstName":"Ziwei","lastName":"Liu"},{"creatorType":"author","firstName":"Dahua","lastName":"Lin"},{"creatorType":"author","firstName":"Chen Change","lastName":"Loy"}],"abstractNote":"","date":"6/2020","proceedingsTitle":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","place":"Seattle, WA, USA","publisher":"IEEE","volume":"","pages":"3783-3791","series":"","language":"en","DOI":"10.1109/CVPR42600.2020.00384","ISBN":"978-1-72817-168-5","shortTitle":"","url":"https://ieeexplore.ieee.org/document/9156608/","accessDate":"2022-04-27T08:34:34Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2022-04-27T08:34:34Z","dateModified":"2022-04-27T08:34:35Z"}},{"key":"FCRDKK2W","version":1125,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FCRDKK2W","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FCRDKK2W","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/L46M3FDT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FCRDKK2W","version":1125,"parentItem":"L46M3FDT","itemType":"note","note":"Comment: Accepted by CVPR2021. BCNet Code: https://github.com/lkeab/BCNet","tags":[],"relations":{},"dateAdded":"2022-04-27T08:34:30Z","dateModified":"2022-04-27T08:34:30Z"}},{"key":"L46M3FDT","version":1125,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/L46M3FDT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/L46M3FDT","type":"text/html"}},"meta":{"creatorSummary":"Ke et al.","parsedDate":"2021-03-23","numChildren":3},"data":{"key":"L46M3FDT","version":1125,"itemType":"journalArticle","title":"Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers","creators":[{"creatorType":"author","firstName":"Lei","lastName":"Ke"},{"creatorType":"author","firstName":"Yu-Wing","lastName":"Tai"},{"creatorType":"author","firstName":"Chi-Keung","lastName":"Tang"}],"abstractNote":"Segmenting highly-overlapping objects is challenging, because typically no distinction is made between real object contours and occlusion boundaries. Unlike previous two-stage instance segmentation methods, we model image formation as composition of two overlapping layers, and propose Bilayer Convolutional Network (BCNet), where the top GCN layer detects the occluding objects (occluder) and the bottom GCN layer infers partially occluded instance (occludee). The explicit modeling of occlusion relationship with bilayer structure naturally decouples the boundaries of both the occluding and occluded instances, and considers the interaction between them during mask regression. We validate the efficacy of bilayer decoupling on both one-stage and two-stage object detectors with different backbones and network layer choices. Despite its simplicity, extensive experiments on COCO and KINS show that our occlusion-aware BCNet achieves large and consistent performance gain especially for heavy occlusion cases. Code is available at https://github.com/lkeab/BCNet.","publicationTitle":"arXiv:2103.12340 [cs]","volume":"","issue":"","pages":"","date":"2021-03-23","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2103.12340","accessDate":"2022-04-27T08:34:30Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2103.12340","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-04-27T08:34:30Z","dateModified":"2022-04-27T08:34:30Z"}},{"key":"T8D8MX9K","version":1126,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/T8D8MX9K","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/T8D8MX9K","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GQFBJKW8","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"T8D8MX9K","version":1126,"parentItem":"GQFBJKW8","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-04-27T08:34:21Z","url":"https://arxiv.org/abs/2203.15361","note":"","contentType":"text/html","charset":"utf-8","filename":"2203.html","md5":"f52fa14942a992c4728d26a132a6729e","mtime":1651048458000,"tags":[],"relations":{},"dateAdded":"2022-04-27T08:34:21Z","dateModified":"2022-04-27T08:34:21Z"}},{"key":"DB32YD6F","version":1126,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DB32YD6F","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DB32YD6F","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GQFBJKW8","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DB32YD6F","version":1126,"parentItem":"GQFBJKW8","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-04-27T08:34:09Z","url":"https://arxiv.org/pdf/2203.15361.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chen et al. - 2022 - Self-Supervised Image Representation Learning with.pdf","md5":"8bbed4bced1c1d772dd93c19ff8fef96","mtime":1651048448000,"tags":[],"relations":{},"dateAdded":"2022-04-27T08:34:09Z","dateModified":"2022-04-27T08:34:09Z"}},{"key":"3CTGJTA4","version":1126,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3CTGJTA4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3CTGJTA4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DEP7T4R6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"3CTGJTA4","version":1126,"parentItem":"DEP7T4R6","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-04-27T08:33:59Z","url":"https://arxiv.org/abs/2108.09897","note":"","contentType":"text/html","charset":"utf-8","filename":"2108.html","md5":"5c0921781ef5d4fba9c1d6b18bbe4b79","mtime":1651048439000,"tags":[],"relations":{},"dateAdded":"2022-04-27T08:33:59Z","dateModified":"2022-04-27T08:33:59Z"}},{"key":"2LSTHE7X","version":1124,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/2LSTHE7X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/2LSTHE7X","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DEP7T4R6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"2LSTHE7X","version":1124,"parentItem":"DEP7T4R6","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-04-27T08:33:47Z","url":"https://arxiv.org/pdf/2108.09897.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Nguyen and Todorovic - 2021 - A Weakly Supervised Amodal Segmenter with Boundary.pdf","md5":"76e0ea52045f1f34c36c9e29de458817","mtime":1651048427000,"tags":[],"relations":{},"dateAdded":"2022-04-27T08:33:47Z","dateModified":"2022-04-27T08:33:47Z"}},{"key":"F3H5N9VJ","version":1124,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/F3H5N9VJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/F3H5N9VJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GQFBJKW8","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"F3H5N9VJ","version":1124,"parentItem":"GQFBJKW8","itemType":"note","note":"Comment: Accepted by CVPR 2022","tags":[],"relations":{},"dateAdded":"2022-04-27T08:33:23Z","dateModified":"2022-04-27T08:33:23Z"}},{"key":"GQFBJKW8","version":1124,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GQFBJKW8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GQFBJKW8","type":"text/html"}},"meta":{"creatorSummary":"Chen et al.","parsedDate":"2022-03-29","numChildren":3},"data":{"key":"GQFBJKW8","version":1124,"itemType":"journalArticle","title":"Self-Supervised Image Representation Learning with Geometric Set Consistency","creators":[{"creatorType":"author","firstName":"Nenglun","lastName":"Chen"},{"creatorType":"author","firstName":"Lei","lastName":"Chu"},{"creatorType":"author","firstName":"Hao","lastName":"Pan"},{"creatorType":"author","firstName":"Yan","lastName":"Lu"},{"creatorType":"author","firstName":"Wenping","lastName":"Wang"}],"abstractNote":"We propose a method for self-supervised image representation learning under the guidance of 3D geometric consistency. Our intuition is that 3D geometric consistency priors such as smooth regions and surface discontinuities may imply consistent semantics or object boundaries, and can act as strong cues to guide the learning of 2D image representations without semantic labels. Specifically, we introduce 3D geometric consistency into a contrastive learning framework to enforce the feature consistency within image views. We propose to use geometric consistency sets as constraints and adapt the InfoNCE loss accordingly. We show that our learned image representations are general. By fine-tuning our pre-trained representations for various 2D image-based downstream tasks, including semantic segmentation, object detection, and instance segmentation on real-world indoor scene datasets, we achieve superior performance compared with state-of-the-art methods.","publicationTitle":"arXiv:2203.15361 [cs]","volume":"","issue":"","pages":"","date":"2022-03-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2203.15361","accessDate":"2022-04-27T08:33:23Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2203.15361","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-04-27T08:33:23Z","dateModified":"2022-04-27T08:33:23Z"}},{"key":"DEP7T4R6","version":1124,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DEP7T4R6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DEP7T4R6","type":"text/html"}},"meta":{"creatorSummary":"Nguyen and Todorovic","parsedDate":"2021-08-29","numChildren":3},"data":{"key":"DEP7T4R6","version":1124,"itemType":"journalArticle","title":"A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation","creators":[{"creatorType":"author","firstName":"Khoi","lastName":"Nguyen"},{"creatorType":"author","firstName":"Sinisa","lastName":"Todorovic"}],"abstractNote":"This paper addresses weakly supervised amodal instance segmentation, where the goal is to segment both visible and occluded (amodal) object parts, while training provides only ground-truth visible (modal) segmentations. Following prior work, we use data manipulation to generate occlusions in training images and thus train a segmenter to predict amodal segmentations of the manipulated data. The resulting predictions on training images are taken as the pseudo-ground truth for the standard training of Mask-RCNN, which we use for amodal instance segmentation of test images. For generating the pseudo-ground truth, we specify a new Amodal Segmenter based on Boundary Uncertainty estimation (ASBU) and make two contributions. First, while prior work uses the occluder's mask, our ASBU uses the occlusion boundary as input. Second, ASBU estimates an uncertainty map of the prediction. The estimated uncertainty regularizes learning such that lower segmentation loss is incurred on regions with high uncertainty. ASBU achieves significant performance improvement relative to the state of the art on the COCOA and KINS datasets in three tasks: amodal instance segmentation, amodal completion, and ordering recovery.","publicationTitle":"arXiv:2108.09897 [cs]","volume":"","issue":"","pages":"","date":"2021-08-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2108.09897","accessDate":"2022-04-27T08:33:08Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2108.09897","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-04-27T08:33:08Z","dateModified":"2022-04-27T08:33:08Z"}},{"key":"FKNJ8PMF","version":1124,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FKNJ8PMF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FKNJ8PMF","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DEP7T4R6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FKNJ8PMF","version":1124,"parentItem":"DEP7T4R6","itemType":"note","note":"Comment: Accepted to ICCV 2021","tags":[],"relations":{},"dateAdded":"2022-04-27T08:33:08Z","dateModified":"2022-04-27T08:33:08Z"}},{"key":"NCMFZLXM","version":1122,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NCMFZLXM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NCMFZLXM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/KD64JT86","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NCMFZLXM","version":1122,"parentItem":"KD64JT86","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-04-27T05:03:27Z","url":"https://arxiv.org/abs/2106.13228","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"c9bd8d6468f9f021dc992c07c32e549d","mtime":1651035807000,"tags":[],"relations":{},"dateAdded":"2022-04-27T05:03:27Z","dateModified":"2022-04-27T05:03:27Z"}},{"key":"YBENC8AS","version":1183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YBENC8AS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YBENC8AS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4EKYAFYR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YBENC8AS","version":1183,"parentItem":"4EKYAFYR","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-04-27T05:03:07Z","url":"https://arxiv.org/pdf/2106.13228.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Park et al. - 2021 - HyperNeRF A Higher-Dimensional Representation for.pdf","md5":"a354c3179e907cbe933cf0f25ac9b7bb","mtime":1651035787000,"tags":[],"relations":{},"dateAdded":"2022-04-27T05:03:07Z","dateModified":"2022-06-13T16:19:40Z"}},{"key":"KD64JT86","version":1119,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KD64JT86","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KD64JT86","type":"text/html"}},"meta":{"creatorSummary":"Park et al.","parsedDate":"2021-09-10","numChildren":3},"data":{"key":"KD64JT86","version":1119,"itemType":"journalArticle","title":"HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields","creators":[{"creatorType":"author","firstName":"Keunhong","lastName":"Park"},{"creatorType":"author","firstName":"Utkarsh","lastName":"Sinha"},{"creatorType":"author","firstName":"Peter","lastName":"Hedman"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Sofien","lastName":"Bouaziz"},{"creatorType":"author","firstName":"Dan B.","lastName":"Goldman"},{"creatorType":"author","firstName":"Ricardo","lastName":"Martin-Brualla"},{"creatorType":"author","firstName":"Steven M.","lastName":"Seitz"}],"abstractNote":"Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this \"hyper-space\". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between \"moments\", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at https://hypernerf.github.io.","publicationTitle":"arXiv:2106.13228 [cs]","volume":"","issue":"","pages":"","date":"2021-09-10","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"HyperNeRF","url":"http://arxiv.org/abs/2106.13228","accessDate":"2022-04-27T05:00:08Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2106.13228","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":[],"relations":{},"dateAdded":"2022-04-27T05:00:09Z","dateModified":"2022-04-27T05:00:09Z"}},{"key":"5D9JMGAN","version":1183,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5D9JMGAN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5D9JMGAN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4EKYAFYR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5D9JMGAN","version":1183,"parentItem":"4EKYAFYR","itemType":"note","note":"Comment: SIGGRAPH Asia 2021, Project page: https://hypernerf.github.io/","tags":[],"relations":{},"dateAdded":"2022-04-27T05:00:09Z","dateModified":"2022-06-13T16:19:40Z"}},{"key":"EZEMMVHZ","version":1117,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EZEMMVHZ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EZEMMVHZ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GERPZFUD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EZEMMVHZ","version":1117,"parentItem":"GERPZFUD","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-04-25T09:22:06Z","url":"https://arxiv.org/abs/2102.08663","note":"","contentType":"text/html","charset":"utf-8","filename":"2102.html","md5":"ef1073d67816ba9ba9447a99053c6e77","mtime":1650878526000,"tags":[],"relations":{},"dateAdded":"2022-04-25T09:22:06Z","dateModified":"2022-04-25T09:22:06Z"}},{"key":"RJNHT8V4","version":1117,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RJNHT8V4","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RJNHT8V4","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GERPZFUD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RJNHT8V4","version":1117,"parentItem":"GERPZFUD","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-04-25T09:22:00Z","url":"https://arxiv.org/pdf/2102.08663.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Takida et al. - 2021 - Preventing Posterior Collapse Induced by Oversmoot.pdf","md5":"85f999ed9199e9c661c1461103bece74","mtime":1650878520000,"tags":[],"relations":{},"dateAdded":"2022-04-25T09:22:00Z","dateModified":"2022-04-25T09:22:00Z"}},{"key":"7BMMWSQR","version":1113,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/7BMMWSQR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/7BMMWSQR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/45TYX24R","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"7BMMWSQR","version":1113,"parentItem":"45TYX24R","itemType":"attachment","linkMode":"imported_url","title":"Lucas et al. - 2019 - Understanding Posterior Collapse in Generative Lat.pdf","accessDate":"2022-04-25T09:19:53Z","url":"https://openreview.net/pdf?id=r1xaVLUYuE","note":"","contentType":"application/pdf","charset":"","filename":"Lucas et al. - 2019 - Understanding Posterior Collapse in Generative Lat.pdf","md5":"eae35ef8597070eeeda81245db9c0a48","mtime":1650878395000,"tags":[],"relations":{},"dateAdded":"2022-04-25T09:19:53Z","dateModified":"2022-04-25T09:19:55Z"}},{"key":"45TYX24R","version":1112,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/45TYX24R","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/45TYX24R","type":"text/html"}},"meta":{"creatorSummary":"Lucas et al.","parsedDate":"2019","numChildren":1},"data":{"key":"45TYX24R","version":1112,"itemType":"journalArticle","title":"Understanding Posterior Collapse in Generative Latent Variable Models","creators":[{"creatorType":"author","firstName":"James","lastName":"Lucas"},{"creatorType":"author","firstName":"George","lastName":"Tucker"},{"creatorType":"author","firstName":"Roger","lastName":"Grosse"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"}],"abstractNote":"Posterior collapse in Variational Autoencoders (VAEs) arises when the variational distribution closely matches the uninformative prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We identify how local maxima can emerge from the marginal log-likelihood of pPCA, which yields similar local maxima for the evidence lower bound (ELBO). We show that training a linear VAE with variational inference recovers a uniquely identiﬁable global maximum corresponding to the principal component directions. We provide empirical evidence that the presence of local maxima causes posterior collapse in non-linear VAEs. Our ﬁndings help to explain a wide range of heuristic approaches in the literature that attempt to diminish the eﬀect of the KL term in the ELBO to alleviate posterior collapse.","publicationTitle":"","volume":"","issue":"","pages":"16","date":"2019","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2022-04-25T09:19:55Z","dateModified":"2022-04-25T09:19:55Z"}},{"key":"GERPZFUD","version":1111,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GERPZFUD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GERPZFUD","type":"text/html"}},"meta":{"creatorSummary":"Takida et al.","parsedDate":"2021-02-17","numChildren":3},"data":{"key":"GERPZFUD","version":1111,"itemType":"journalArticle","title":"Preventing Posterior Collapse Induced by Oversmoothing in Gaussian VAE","creators":[{"creatorType":"author","firstName":"Yuhta","lastName":"Takida"},{"creatorType":"author","firstName":"Wei-Hsiang","lastName":"Liao"},{"creatorType":"author","firstName":"Toshimitsu","lastName":"Uesaka"},{"creatorType":"author","firstName":"Shusuke","lastName":"Takahashi"},{"creatorType":"author","firstName":"Yuki","lastName":"Mitsufuji"}],"abstractNote":"Variational autoencoders (VAEs) often suffer from posterior collapse, which is a phenomenon in which the learned latent space becomes uninformative. This is often related to a hyperparameter resembling the data variance. It can be shown that an inappropriate choice of this parameter causes oversmoothness and leads to posterior collapse in the linearly approximated case and can be empirically verified for the general cases. Therefore, we propose AR-ELBO (Adaptively Regularized Evidence Lower BOund), which controls the smoothness of the model by adapting this variance parameter. In addition, we extend VAE with alternative parameterizations on the variance parameter to deal with non-uniform or conditional data variance. The proposed VAE extensions trained with AR-ELBO show improved Fr\\'echet inception distance (FID) on images generated from the MNIST and CelebA datasets.","publicationTitle":"arXiv:2102.08663 [cs]","volume":"","issue":"","pages":"","date":"2021-02-17","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2102.08663","accessDate":"2022-04-25T09:19:38Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2102.08663","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-04-25T09:19:38Z","dateModified":"2022-04-25T09:19:38Z"}},{"key":"RTEPW6ZB","version":1111,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RTEPW6ZB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RTEPW6ZB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/GERPZFUD","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"RTEPW6ZB","version":1111,"parentItem":"GERPZFUD","itemType":"note","note":"Comment: 21 pages with 6 figures","tags":[],"relations":{},"dateAdded":"2022-04-25T09:19:38Z","dateModified":"2022-04-25T09:19:38Z"}},{"key":"D3ZH5RA5","version":1109,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/D3ZH5RA5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/D3ZH5RA5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/NSHIC3AJ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"D3ZH5RA5","version":1109,"parentItem":"NSHIC3AJ","itemType":"attachment","linkMode":"imported_url","title":"Oord et al. - 2018 - Neural Discrete Representation Learning.pdf","accessDate":"2022-04-25T08:58:43Z","url":"https://arxiv.org/pdf/1711.00937.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Oord et al. - 2018 - Neural Discrete Representation Learning.pdf","md5":"d268870c8814367f11c1e2fcbc6c8e41","mtime":1650877143000,"tags":[],"relations":{},"dateAdded":"2022-04-25T08:58:43Z","dateModified":"2022-04-25T08:59:03Z"}},{"key":"NSHIC3AJ","version":1109,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NSHIC3AJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NSHIC3AJ","type":"text/html"}},"meta":{"creatorSummary":"Oord et al.","parsedDate":"2018-05-30","numChildren":1},"data":{"key":"NSHIC3AJ","version":1109,"itemType":"journalArticle","title":"Neural Discrete Representation Learning","creators":[{"creatorType":"author","firstName":"Aaron van den","lastName":"Oord"},{"creatorType":"author","firstName":"Oriol","lastName":"Vinyals"},{"creatorType":"author","firstName":"Koray","lastName":"Kavukcuoglu"}],"abstractNote":"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector QuantisedVariational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of “posterior collapse” -— where the latents are ignored when they are paired with a powerful autoregressive decoder -— typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.","publicationTitle":"arXiv:1711.00937 [cs]","volume":"","issue":"","pages":"","date":"2018-05-30","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1711.00937","accessDate":"2022-04-25T08:59:02Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1711.00937","tags":[{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-04-25T08:59:02Z","dateModified":"2022-04-25T08:59:02Z"}},{"key":"K9NCF657","version":1154,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/K9NCF657","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/K9NCF657","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/VE9BGBUT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"K9NCF657","version":1154,"parentItem":"VE9BGBUT","itemType":"attachment","linkMode":"imported_url","title":"Saharia et al. - 2022 - Photorealistic Text-to-Image Diffusion Models with.pdf","accessDate":"2022-05-26T04:20:19Z","url":"https://arxiv.org/pdf/2205.11487.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Saharia et al. - 2022 - Photorealistic Text-to-Image Diffusion Models with.pdf","md5":"fec4d50a1d35b9d73f5a502124bfd5ea","mtime":1653538824000,"tags":[],"relations":{},"dateAdded":"2022-05-26T04:20:19Z","dateModified":"2022-05-26T04:20:24Z"}},{"key":"VE9BGBUT","version":1153,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VE9BGBUT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VE9BGBUT","type":"text/html"}},"meta":{"creatorSummary":"Saharia et al.","parsedDate":"2022-05-23","numChildren":1},"data":{"key":"VE9BGBUT","version":1153,"itemType":"journalArticle","title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding","creators":[{"creatorType":"author","firstName":"Chitwan","lastName":"Saharia"},{"creatorType":"author","firstName":"William","lastName":"Chan"},{"creatorType":"author","firstName":"Saurabh","lastName":"Saxena"},{"creatorType":"author","firstName":"Lala","lastName":"Li"},{"creatorType":"author","firstName":"Jay","lastName":"Whang"},{"creatorType":"author","firstName":"Emily","lastName":"Denton"},{"creatorType":"author","firstName":"Seyed Kamyar Seyed","lastName":"Ghasemipour"},{"creatorType":"author","firstName":"Burcu Karagol","lastName":"Ayan"},{"creatorType":"author","firstName":"S. Sara","lastName":"Mahdavi"},{"creatorType":"author","firstName":"Rapha Gontijo","lastName":"Lopes"},{"creatorType":"author","firstName":"Tim","lastName":"Salimans"},{"creatorType":"author","firstName":"Jonathan","lastName":"Ho"},{"creatorType":"author","firstName":"David J.","lastName":"Fleet"},{"creatorType":"author","firstName":"Mohammad","lastName":"Norouzi"}],"abstractNote":"We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-ﬁdelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample ﬁdelity and imagetext alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters ﬁnd Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, GLIDE and DALL-E 2, and ﬁnd that human raters prefer Imagen over other models in side-byside comparisons, both in terms of sample quality and image-text alignment. See imagen.research.google for an overview of the results.","publicationTitle":"arXiv:2205.11487 [cs]","volume":"","issue":"","pages":"","date":"2022-05-23","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2205.11487","accessDate":"2022-05-26T04:20:23Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2205.11487","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-05-26T04:20:23Z","dateModified":"2022-05-26T04:20:23Z"}},{"key":"G5EKCHDS","version":1153,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/G5EKCHDS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/G5EKCHDS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/CETWSCVT","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"G5EKCHDS","version":1153,"parentItem":"CETWSCVT","itemType":"attachment","linkMode":"imported_url","title":"Alayrac et al. - 2022 - Flamingo a Visual Language Model for Few-Shot Lea.pdf","accessDate":"2022-05-26T04:19:20Z","url":"https://arxiv.org/pdf/2204.14198.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Alayrac et al. - 2022 - Flamingo a Visual Language Model for Few-Shot Lea.pdf","md5":"f11d9caf4ccbd59afbdf372f445e3a9f","mtime":1653538760000,"tags":[],"relations":{},"dateAdded":"2022-05-26T04:19:20Z","dateModified":"2022-05-26T04:19:27Z"}},{"key":"CETWSCVT","version":1153,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/CETWSCVT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/CETWSCVT","type":"text/html"}},"meta":{"creatorSummary":"Alayrac et al.","parsedDate":"2022-04-29","numChildren":1},"data":{"key":"CETWSCVT","version":1153,"itemType":"journalArticle","title":"Flamingo: a Visual Language Model for Few-Shot Learning","creators":[{"creatorType":"author","firstName":"Jean-Baptiste","lastName":"Alayrac"},{"creatorType":"author","firstName":"Jeff","lastName":"Donahue"},{"creatorType":"author","firstName":"Pauline","lastName":"Luc"},{"creatorType":"author","firstName":"Antoine","lastName":"Miech"},{"creatorType":"author","firstName":"Iain","lastName":"Barr"},{"creatorType":"author","firstName":"Yana","lastName":"Hasson"},{"creatorType":"author","firstName":"Karel","lastName":"Lenc"},{"creatorType":"author","firstName":"Arthur","lastName":"Mensch"},{"creatorType":"author","firstName":"Katie","lastName":"Millican"},{"creatorType":"author","firstName":"Malcolm","lastName":"Reynolds"},{"creatorType":"author","firstName":"Roman","lastName":"Ring"},{"creatorType":"author","firstName":"Eliza","lastName":"Rutherford"},{"creatorType":"author","firstName":"Serkan","lastName":"Cabi"},{"creatorType":"author","firstName":"Tengda","lastName":"Han"},{"creatorType":"author","firstName":"Zhitao","lastName":"Gong"},{"creatorType":"author","firstName":"Sina","lastName":"Samangooei"},{"creatorType":"author","firstName":"Marianne","lastName":"Monteiro"},{"creatorType":"author","firstName":"Jacob","lastName":"Menick"},{"creatorType":"author","firstName":"Sebastian","lastName":"Borgeaud"},{"creatorType":"author","firstName":"Andrew","lastName":"Brock"},{"creatorType":"author","firstName":"Aida","lastName":"Nematzadeh"},{"creatorType":"author","firstName":"Sahand","lastName":"Sharifzadeh"},{"creatorType":"author","firstName":"Mikolaj","lastName":"Binkowski"},{"creatorType":"author","firstName":"Ricardo","lastName":"Barreira"},{"creatorType":"author","firstName":"Oriol","lastName":"Vinyals"},{"creatorType":"author","firstName":"Andrew","lastName":"Zisserman"},{"creatorType":"author","firstName":"Karen","lastName":"Simonyan"}],"abstractNote":"Building models that can be rapidly adapted to numerous tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. Flamingo models include key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of the proposed Flamingo models, exploring and measuring their ability to rapidly adapt to a variety of image and video understanding benchmarks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer, captioning tasks, which evaluate the ability to describe a scene or an event, and close-ended tasks such as multiple choice visual question-answering. For tasks lying anywhere on this spectrum, we demonstrate that a single Flamingo model can achieve a new state of the art for few-shot learning, simply by prompting the model with task-specific examples. On many of these benchmarks, Flamingo actually surpasses the performance of models that are fine-tuned on thousands of times more task-specific data.","publicationTitle":"arXiv:2204.14198 [cs]","volume":"","issue":"","pages":"","date":"2022-04-29","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"Flamingo","url":"http://arxiv.org/abs/2204.14198","accessDate":"2022-05-26T04:19:25Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2204.14198","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-05-26T04:19:25Z","dateModified":"2022-05-26T04:19:26Z"}},{"key":"WN7YIXQB","version":1150,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WN7YIXQB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WN7YIXQB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DP8YDUZL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"WN7YIXQB","version":1150,"parentItem":"DP8YDUZL","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-05-21T05:05:13Z","url":"https://arxiv.org/abs/1703.10593","note":"","contentType":"text/html","charset":"utf-8","filename":"1703.html","md5":"e05bbeb92b06c7663505ec80f6bc5b70","mtime":1653109512000,"tags":[],"relations":{},"dateAdded":"2022-05-21T05:05:13Z","dateModified":"2022-05-21T05:05:13Z"}},{"key":"Y92SJ2F5","version":1150,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/Y92SJ2F5","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/Y92SJ2F5","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DP8YDUZL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"Y92SJ2F5","version":1150,"parentItem":"DP8YDUZL","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-05-21T05:05:02Z","url":"https://arxiv.org/pdf/1703.10593.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Zhu et al. - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf","md5":"1b04b09d8996a680cd7543f2f14cb4cb","mtime":1653109502000,"tags":[],"relations":{},"dateAdded":"2022-05-21T05:05:02Z","dateModified":"2022-05-21T05:05:02Z"}},{"key":"DP8YDUZL","version":1147,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DP8YDUZL","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DP8YDUZL","type":"text/html"}},"meta":{"creatorSummary":"Zhu et al.","parsedDate":"2020-08-24","numChildren":3},"data":{"key":"DP8YDUZL","version":1147,"itemType":"journalArticle","title":"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks","creators":[{"creatorType":"author","firstName":"Jun-Yan","lastName":"Zhu"},{"creatorType":"author","firstName":"Taesung","lastName":"Park"},{"creatorType":"author","firstName":"Phillip","lastName":"Isola"},{"creatorType":"author","firstName":"Alexei A.","lastName":"Efros"}],"abstractNote":"Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.","publicationTitle":"arXiv:1703.10593 [cs]","volume":"","issue":"","pages":"","date":"2020-08-24","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/1703.10593","accessDate":"2022-05-21T05:01:12Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 1703.10593","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-05-21T05:01:12Z","dateModified":"2022-05-21T05:01:12Z"}},{"key":"G29U7Q8U","version":1147,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/G29U7Q8U","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/G29U7Q8U","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/DP8YDUZL","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"G29U7Q8U","version":1147,"parentItem":"DP8YDUZL","itemType":"note","note":"Comment: An extended version of our ICCV 2017 paper, v7 fixed the typos and updated the implementation details. Code and data: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix","tags":[],"relations":{},"dateAdded":"2022-05-21T05:01:12Z","dateModified":"2022-05-21T05:01:12Z"}},{"key":"IJBZFKJ3","version":1145,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IJBZFKJ3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IJBZFKJ3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XKTUXNCQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"IJBZFKJ3","version":1145,"parentItem":"XKTUXNCQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-05-17T09:59:05Z","url":"https://arxiv.org/abs/2104.08381","note":"","contentType":"text/html","charset":"utf-8","filename":"2104.html","md5":"4ec3ac641213a7a842f87ada1c8a8978","mtime":1652781545000,"tags":[],"relations":{},"dateAdded":"2022-05-17T09:59:05Z","dateModified":"2022-05-17T09:59:05Z"}},{"key":"VMJQEA75","version":1145,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/VMJQEA75","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/VMJQEA75","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XKTUXNCQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"VMJQEA75","version":1145,"parentItem":"XKTUXNCQ","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-05-17T09:58:58Z","url":"https://arxiv.org/pdf/2104.08381.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wang et al. - 2021 - Robust Object Detection via Instance-Level Tempora.pdf","md5":"34d906c1b5d9540cd3a324a5688db257","mtime":1652781538000,"tags":[],"relations":{},"dateAdded":"2022-05-17T09:58:58Z","dateModified":"2022-05-17T09:58:58Z"}},{"key":"XKTUXNCQ","version":1142,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XKTUXNCQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XKTUXNCQ","type":"text/html"}},"meta":{"creatorSummary":"Wang et al.","parsedDate":"2021-08-23","numChildren":3},"data":{"key":"XKTUXNCQ","version":1142,"itemType":"journalArticle","title":"Robust Object Detection via Instance-Level Temporal Cycle Confusion","creators":[{"creatorType":"author","firstName":"Xin","lastName":"Wang"},{"creatorType":"author","firstName":"Thomas E.","lastName":"Huang"},{"creatorType":"author","firstName":"Benlin","lastName":"Liu"},{"creatorType":"author","firstName":"Fisher","lastName":"Yu"},{"creatorType":"author","firstName":"Xiaolong","lastName":"Wang"},{"creatorType":"author","firstName":"Joseph E.","lastName":"Gonzalez"},{"creatorType":"author","firstName":"Trevor","lastName":"Darrell"}],"abstractNote":"Building reliable object detectors that are robust to domain shifts, such as various changes in context, viewpoint, and object appearances, is critical for real-world applications. In this work, we study the effectiveness of auxiliary self-supervised tasks to improve the out-of-distribution generalization of object detectors. Inspired by the principle of maximum entropy, we introduce a novel self-supervised task, instance-level temporal cycle confusion (CycConf), which operates on the region features of the object detectors. For each object, the task is to find the most different object proposals in the adjacent frame in a video and then cycle back to itself for self-supervision. CycConf encourages the object detector to explore invariant structures across instances under various motions, which leads to improved model robustness in unseen domains at test time. We observe consistent out-of-domain performance improvements when training object detectors in tandem with self-supervised tasks on large-scale video datasets (BDD100K and Waymo open data). The joint training framework also establishes a new state-of-the-art on standard unsupervised domain adaptative detection benchmarks (Cityscapes, Foggy Cityscapes, and Sim10K). The code and models are available at https://github.com/xinw1012/cycle-confusion.","publicationTitle":"arXiv:2104.08381 [cs]","volume":"","issue":"","pages":"","date":"2021-08-23","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2104.08381","accessDate":"2022-05-17T09:57:41Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2104.08381","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-05-17T09:57:41Z","dateModified":"2022-05-17T09:57:41Z"}},{"key":"UPRGWW5R","version":1142,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UPRGWW5R","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UPRGWW5R","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XKTUXNCQ","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UPRGWW5R","version":1142,"parentItem":"XKTUXNCQ","itemType":"note","note":"Comment: ICCV 2021","tags":[],"relations":{},"dateAdded":"2022-05-17T09:57:41Z","dateModified":"2022-05-17T09:57:41Z"}},{"key":"79P46WKT","version":1209,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/79P46WKT","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/79P46WKT","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/79E56ULA","type":"application/json"}},"meta":{},"data":{"key":"79P46WKT","version":1209,"parentItem":"79E56ULA","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-07-13T08:17:57Z","url":"https://arxiv.org/abs/2207.05501","note":"","contentType":"text/html","charset":"utf-8","filename":"2207.html","md5":"ad57b5e09796ea0766cd19b3166e61bb","mtime":1657700277000,"tags":[],"relations":{},"dateAdded":"2022-07-13T08:17:57Z","dateModified":"2022-07-13T08:17:57Z"}},{"key":"S5PHZRY8","version":1209,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/S5PHZRY8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/S5PHZRY8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/79E56ULA","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"S5PHZRY8","version":1209,"parentItem":"79E56ULA","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-07-13T08:17:50Z","url":"https://arxiv.org/pdf/2207.05501.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Li et al. - 2022 - Next-ViT Next Generation Vision Transformer for E.pdf","md5":"aa93c813cf56cdf3abad384ef4313d62","mtime":1657700270000,"tags":[],"relations":{},"dateAdded":"2022-07-13T08:17:50Z","dateModified":"2022-07-13T08:17:50Z"}},{"key":"79E56ULA","version":1208,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/79E56ULA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/79E56ULA","type":"text/html"}},"meta":{"creatorSummary":"Li et al.","parsedDate":"2022-07-12","numChildren":2},"data":{"key":"79E56ULA","version":1208,"itemType":"preprint","title":"Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios","creators":[{"creatorType":"author","firstName":"Jiashi","lastName":"Li"},{"creatorType":"author","firstName":"Xin","lastName":"Xia"},{"creatorType":"author","firstName":"Wei","lastName":"Li"},{"creatorType":"author","firstName":"Huixia","lastName":"Li"},{"creatorType":"author","firstName":"Xing","lastName":"Wang"},{"creatorType":"author","firstName":"Xuefeng","lastName":"Xiao"},{"creatorType":"author","firstName":"Rui","lastName":"Wang"},{"creatorType":"author","firstName":"Min","lastName":"Zheng"},{"creatorType":"author","firstName":"Xin","lastName":"Pan"}],"abstractNote":"Due to the complex attention mechanisms and model design, most existing vision Transformers (ViTs) can not perform as efficiently as convolutional neural networks (CNNs) in realistic industrial deployment scenarios, e.g. TensorRT and CoreML. This poses a distinct challenge: Can a visual neural network be designed to infer as fast as CNNs and perform as powerful as ViTs? Recent works have tried to design CNN-Transformer hybrid architectures to address this issue, yet the overall performance of these works is far away from satisfactory. To end these, we propose a next generation vision Transformer for efficient deployment in realistic industrial scenarios, namely Next-ViT, which dominates both CNNs and ViTs from the perspective of latency/accuracy trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer Block (NTB) are respectively developed to capture local and global information with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts performance in various downstream tasks. Extensive experiments show that Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer hybrid architectures with respect to the latency/accuracy trade-off across various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.4 mAP (from 40.4 to 45.8) on COCO detection and 8.2% mIoU (from 38.8% to 47.0%) on ADE20K segmentation under similar latency. Meanwhile, it achieves comparable performance with CSWin, while the inference speed is accelerated by 3.6x. On CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on COCO detection and 3.5% mIoU (from 45.2% to 48.7%) on ADE20K segmentation under similar latency. Code will be released recently.","genre":"","repository":"arXiv","archiveID":"arXiv:2207.05501","place":"","date":"2022-07-12","series":"","seriesNumber":"","DOI":"10.48550/arXiv.2207.05501","citationKey":"","url":"http://arxiv.org/abs/2207.05501","accessDate":"2022-07-13T08:17:32Z","archive":"","archiveLocation":"","shortTitle":"Next-ViT","language":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2207.05501 [cs]","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-07-13T08:17:32Z","dateModified":"2022-07-13T08:17:32Z"}},{"key":"EMHE863S","version":1206,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EMHE863S","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EMHE863S","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/6CVJH5ZW","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EMHE863S","version":1206,"parentItem":"6CVJH5ZW","itemType":"attachment","linkMode":"imported_url","title":"Rockwell et al. - 2021 - PixelSynth Generating a 3D-Consistent Experience .pdf","accessDate":"2022-07-10T02:36:08Z","url":"https://openaccess.thecvf.com/content/ICCV2021/papers/Rockwell_PixelSynth_Generating_a_3D-Consistent_Experience_From_a_Single_Image_ICCV_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Rockwell et al. - 2021 - PixelSynth Generating a 3D-Consistent Experience .pdf","md5":"c68adc529f329e400c501b25df7f1ca4","mtime":1657420572000,"tags":[],"relations":{},"dateAdded":"2022-07-10T02:36:08Z","dateModified":"2022-07-10T02:36:12Z"}},{"key":"6CVJH5ZW","version":1205,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6CVJH5ZW","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6CVJH5ZW","type":"text/html"}},"meta":{"creatorSummary":"Rockwell et al.","parsedDate":"2021","numChildren":1},"data":{"key":"6CVJH5ZW","version":1205,"itemType":"conferencePaper","title":"PixelSynth: Generating a 3D-Consistent Experience from a Single Image","creators":[{"creatorType":"author","firstName":"Chris","lastName":"Rockwell"},{"creatorType":"author","firstName":"David F.","lastName":"Fouhey"},{"creatorType":"author","firstName":"Justin","lastName":"Johnson"}],"abstractNote":"Recent advancements in differentiable rendering and 3D reasoning have driven exciting results in novel view synthesis from a single image. Despite realistic results, methods are limited to relatively small view change. In order to synthesize immersive scenes, models must also be able to extrapolate. We present an approach that fuses 3D reasoning with autoregressive modeling to outpaint large view changes in a 3D-consistent manner, enabling scene synthesis. We demonstrate considerable improvement in singleimage large-angle view synthesis results compared to a variety of methods and possible variants across simulated and real datasets. In addition, we show increased 3D consistency compared to alternative accumulation methods.","date":"10/2021","proceedingsTitle":"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","conferenceName":"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","place":"Montreal, QC, Canada","publisher":"IEEE","volume":"","pages":"14084-14093","series":"","language":"en","DOI":"10.1109/ICCV48922.2021.01384","ISBN":"978-1-66542-812-5","shortTitle":"PixelSynth","url":"https://ieeexplore.ieee.org/document/9711341/","accessDate":"2022-07-10T02:36:11Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":["689E8XER"],"relations":{},"dateAdded":"2022-07-10T02:36:11Z","dateModified":"2022-07-10T02:36:11Z"}},{"key":"D7KF3HLM","version":1205,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/D7KF3HLM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/D7KF3HLM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LTHD859E","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"D7KF3HLM","version":1205,"parentItem":"LTHD859E","itemType":"attachment","linkMode":"imported_url","title":"Hu et al. - 2021 - Worldsheet Wrapping the World in a 3D Sheet for V.pdf","accessDate":"2022-07-10T02:35:52Z","url":"https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Worldsheet_Wrapping_the_World_in_a_3D_Sheet_for_View_ICCV_2021_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Hu et al. - 2021 - Worldsheet Wrapping the World in a 3D Sheet for V.pdf","md5":"9a2afcd420bfb27848a2470895d8464d","mtime":1657420556000,"tags":[],"relations":{},"dateAdded":"2022-07-10T02:35:52Z","dateModified":"2022-07-10T02:35:56Z"}},{"key":"LTHD859E","version":1204,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LTHD859E","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LTHD859E","type":"text/html"}},"meta":{"creatorSummary":"Hu et al.","parsedDate":"2021","numChildren":1},"data":{"key":"LTHD859E","version":1204,"itemType":"conferencePaper","title":"Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image","creators":[{"creatorType":"author","firstName":"Ronghang","lastName":"Hu"},{"creatorType":"author","firstName":"Nikhila","lastName":"Ravi"},{"creatorType":"author","firstName":"Alexander C.","lastName":"Berg"},{"creatorType":"author","firstName":"Deepak","lastName":"Pathak"}],"abstractNote":"","date":"10/2021","proceedingsTitle":"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","conferenceName":"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","place":"Montreal, QC, Canada","publisher":"IEEE","volume":"","pages":"12508-12517","series":"","language":"en","DOI":"10.1109/ICCV48922.2021.01230","ISBN":"978-1-66542-812-5","shortTitle":"Worldsheet","url":"https://ieeexplore.ieee.org/document/9711329/","accessDate":"2022-07-10T02:35:56Z","archive":"","archiveLocation":"","libraryCatalog":"DOI.org (Crossref)","callNumber":"","rights":"","extra":"","tags":[],"collections":["689E8XER"],"relations":{},"dateAdded":"2022-07-10T02:35:56Z","dateModified":"2022-07-10T02:35:56Z"}},{"key":"RZ2JQ68Y","version":1202,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RZ2JQ68Y","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RZ2JQ68Y","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BRMFJCWU","type":"application/json"}},"meta":{},"data":{"key":"RZ2JQ68Y","version":1202,"parentItem":"BRMFJCWU","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-07-10T02:35:01Z","url":"https://arxiv.org/abs/2012.05903","note":"","contentType":"text/html","charset":"utf-8","filename":"2012.html","md5":"9c3cd694052fe5fb0668d6fb4ccc920c","mtime":1657420501000,"tags":[],"relations":{},"dateAdded":"2022-07-10T02:35:01Z","dateModified":"2022-07-10T02:35:01Z"}},{"key":"ZTS7VIRJ","version":1202,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ZTS7VIRJ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ZTS7VIRJ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BRMFJCWU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"ZTS7VIRJ","version":1202,"parentItem":"BRMFJCWU","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-07-10T02:34:55Z","url":"https://arxiv.org/pdf/2012.05903.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Gao et al. - 2021 - Portrait Neural Radiance Fields from a Single Imag.pdf","md5":"63ae063fad03015eea282ab524fe9c4e","mtime":1657420495000,"tags":[],"relations":{},"dateAdded":"2022-07-10T02:34:55Z","dateModified":"2022-07-10T02:34:55Z"}},{"key":"6VNT8GDV","version":1199,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/6VNT8GDV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/6VNT8GDV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/BRMFJCWU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"6VNT8GDV","version":1199,"parentItem":"BRMFJCWU","itemType":"note","note":"Comment: Project webpage: https://portrait-nerf.github.io/","tags":[],"relations":{},"dateAdded":"2022-07-10T02:34:04Z","dateModified":"2022-07-10T02:34:04Z"}},{"key":"BRMFJCWU","version":1199,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BRMFJCWU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BRMFJCWU","type":"text/html"}},"meta":{"creatorSummary":"Gao et al.","parsedDate":"2021-04-16","numChildren":3},"data":{"key":"BRMFJCWU","version":1199,"itemType":"preprint","title":"Portrait Neural Radiance Fields from a Single Image","creators":[{"creatorType":"author","firstName":"Chen","lastName":"Gao"},{"creatorType":"author","firstName":"Yichang","lastName":"Shih"},{"creatorType":"author","firstName":"Wei-Sheng","lastName":"Lai"},{"creatorType":"author","firstName":"Chia-Kai","lastName":"Liang"},{"creatorType":"author","firstName":"Jia-Bin","lastName":"Huang"}],"abstractNote":"We present a method for estimating Neural Radiance Fields (NeRF) from a single headshot portrait. While NeRF has demonstrated high-quality view synthesis, it requires multiple images of static scenes and thus impractical for casual captures and moving subjects. In this work, we propose to pretrain the weights of a multilayer perceptron (MLP), which implicitly models the volumetric density and colors, with a meta-learning framework using a light stage portrait dataset. To improve the generalization to unseen faces, we train the MLP in the canonical coordinate space approximated by 3D face morphable models. We quantitatively evaluate the method using controlled captures and demonstrate the generalization to real portrait images, showing favorable results against state-of-the-arts.","genre":"","repository":"arXiv","archiveID":"arXiv:2012.05903","place":"","date":"2021-04-16","series":"","seriesNumber":"","DOI":"10.48550/arXiv.2012.05903","citationKey":"","url":"http://arxiv.org/abs/2012.05903","accessDate":"2022-07-10T02:34:04Z","archive":"","archiveLocation":"","shortTitle":"","language":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2012.05903 [cs]","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["689E8XER"],"relations":{},"dateAdded":"2022-07-10T02:34:04Z","dateModified":"2022-07-10T02:34:04Z"}},{"key":"SATQYWBV","version":1196,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SATQYWBV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SATQYWBV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LIEZDGFX","type":"application/json"}},"meta":{},"data":{"key":"SATQYWBV","version":1196,"parentItem":"LIEZDGFX","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-06-13T16:26:36Z","url":"https://arxiv.org/abs/2106.10410","note":"","contentType":"text/html","charset":"utf-8","filename":"2106.html","md5":"242e62cdd703ef95d0aeec3c7f86b1fd","mtime":1655137596000,"tags":[],"relations":{},"dateAdded":"2022-06-13T16:26:36Z","dateModified":"2022-06-13T16:26:36Z"}},{"key":"UD53QUS3","version":1196,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UD53QUS3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UD53QUS3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LIEZDGFX","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"UD53QUS3","version":1196,"parentItem":"LIEZDGFX","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-06-13T16:26:31Z","url":"https://arxiv.org/pdf/2106.10410.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Wang et al. - 2021 - Deep Generative Learning via Schr{o}dinger Bridg.pdf","md5":"ab4378d0e7c65c99adc386578f19d13e","mtime":1655137591000,"tags":[],"relations":{},"dateAdded":"2022-06-13T16:26:31Z","dateModified":"2022-06-13T16:26:31Z"}},{"key":"LIEZDGFX","version":1193,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LIEZDGFX","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LIEZDGFX","type":"text/html"}},"meta":{"creatorSummary":"Wang et al.","parsedDate":"2021-07-30","numChildren":2},"data":{"key":"LIEZDGFX","version":1193,"itemType":"report","title":"Deep Generative Learning via Schr\\\"{o}dinger Bridge","creators":[{"creatorType":"author","firstName":"Gefei","lastName":"Wang"},{"creatorType":"author","firstName":"Yuling","lastName":"Jiao"},{"creatorType":"author","firstName":"Qian","lastName":"Xu"},{"creatorType":"author","firstName":"Yang","lastName":"Wang"},{"creatorType":"author","firstName":"Can","lastName":"Yang"}],"abstractNote":"We propose to learn a generative model via entropy interpolation with a Schr\\\"{o}dinger Bridge. The generative learning task can be formulated as interpolating between a reference distribution and a target distribution based on the Kullback-Leibler divergence. At the population level, this entropy interpolation is characterized via an SDE on $[0,1]$ with a time-varying drift term. At the sample level, we derive our Schr\\\"{o}dinger Bridge algorithm by plugging the drift term estimated by a deep score estimator and a deep density ratio estimator into the Euler-Maruyama method. Under some mild smoothness assumptions of the target distribution, we prove the consistency of both the score estimator and the density ratio estimator, and then establish the consistency of the proposed Schr\\\"{o}dinger Bridge approach. Our theoretical results guarantee that the distribution learned by our approach converges to the target distribution. Experimental results on multimodal synthetic data and benchmark data support our theoretical findings and indicate that the generative model via Schr\\\"{o}dinger Bridge is comparable with state-of-the-art GANs, suggesting a new formulation of generative learning. We demonstrate its usefulness in image interpolation and image inpainting.","reportNumber":"arXiv:2106.10410","reportType":"","seriesTitle":"","place":"","institution":"arXiv","date":"2021-07-30","pages":"","language":"","shortTitle":"","url":"http://arxiv.org/abs/2106.10410","accessDate":"2022-06-13T16:25:51Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2106.10410 [cs]\ntype: article","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-06-13T16:25:51Z","dateModified":"2022-06-13T16:25:51Z"}},{"key":"EAH9DSN3","version":1191,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/EAH9DSN3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/EAH9DSN3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/XQX5CAYV","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"EAH9DSN3","version":1191,"parentItem":"XQX5CAYV","itemType":"note","note":"Comment: 58 pages, 18 figures (correction of Proposition 5)","tags":[],"relations":{},"dateAdded":"2022-06-13T16:19:03Z","dateModified":"2022-06-13T16:20:30Z"}},{"key":"5UG8DZIM","version":1184,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5UG8DZIM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5UG8DZIM","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WU6KTIRS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5UG8DZIM","version":1184,"parentItem":"WU6KTIRS","itemType":"attachment","linkMode":"imported_url","title":"Saharia et al. - 2022 - Palette Image-to-Image Diffusion Models.pdf","accessDate":"2022-06-01T05:49:40Z","url":"https://arxiv.org/pdf/2111.05826.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Saharia et al. - 2022 - Palette Image-to-Image Diffusion Models.pdf","md5":"211b1a64d38c295ff019448886839ae6","mtime":1654062580000,"tags":[],"relations":{},"dateAdded":"2022-06-01T05:49:40Z","dateModified":"2022-06-13T16:19:37Z"}},{"key":"PK5RW79M","version":1180,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PK5RW79M","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PK5RW79M","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/M943T35K","type":"application/json"}},"meta":{},"data":{"key":"PK5RW79M","version":1180,"parentItem":"M943T35K","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-06-13T15:46:52Z","url":"https://arxiv.org/abs/2111.07243","note":"","contentType":"text/html","charset":"utf-8","filename":"2111.html","md5":"662f3f34f934b1acde77a2f6dc28fa84","mtime":1655135212000,"tags":[],"relations":{},"dateAdded":"2022-06-13T15:46:52Z","dateModified":"2022-06-13T15:46:52Z"}},{"key":"9I7J6JDB","version":1180,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9I7J6JDB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9I7J6JDB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/M943T35K","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"9I7J6JDB","version":1180,"parentItem":"M943T35K","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-06-13T15:46:47Z","url":"https://arxiv.org/pdf/2111.07243.pdf","note":"","contentType":"application/pdf","charset":"","filename":"De Bortoli et al. - 2021 - Simulating Diffusion Bridges with Score Matching.pdf","md5":"11f650960b2efd0857eb4b43dea4b285","mtime":1655135207000,"tags":[],"relations":{},"dateAdded":"2022-06-13T15:46:47Z","dateModified":"2022-06-13T15:46:47Z"}},{"key":"KGI23XC8","version":1178,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KGI23XC8","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KGI23XC8","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/M943T35K","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KGI23XC8","version":1178,"parentItem":"M943T35K","itemType":"note","note":"Comment: 20 pages, 3 figures","tags":[],"relations":{},"dateAdded":"2022-06-13T15:46:37Z","dateModified":"2022-06-13T15:46:37Z"}},{"key":"M943T35K","version":1178,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/M943T35K","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/M943T35K","type":"text/html"}},"meta":{"creatorSummary":"De Bortoli et al.","parsedDate":"2021-11-14","numChildren":3},"data":{"key":"M943T35K","version":1178,"itemType":"report","title":"Simulating Diffusion Bridges with Score Matching","creators":[{"creatorType":"author","firstName":"Valentin","lastName":"De Bortoli"},{"creatorType":"author","firstName":"Arnaud","lastName":"Doucet"},{"creatorType":"author","firstName":"Jeremy","lastName":"Heng"},{"creatorType":"author","firstName":"James","lastName":"Thornton"}],"abstractNote":"We consider the problem of simulating diffusion bridges, i.e. diffusion processes that are conditioned to initialize and terminate at two given states. Diffusion bridge simulation has applications in diverse scientific fields and plays a crucial role for statistical inference of discretely-observed diffusions. This is known to be a challenging problem that has received much attention in the last two decades. In this work, we first show that the time-reversed diffusion bridge process can be simulated if one can time-reverse the unconditioned diffusion process. We introduce a variational formulation to learn this time-reversal that relies on a score matching method to circumvent intractability. We then consider another iteration of our proposed methodology to approximate the Doob's $h$-transform defining the diffusion bridge process. As our approach is generally applicable under mild assumptions on the underlying diffusion process, it can easily be used to improve the proposal bridge process within existing methods and frameworks. We discuss algorithmic considerations and extensions, and present some numerical results.","reportNumber":"arXiv:2111.07243","reportType":"","seriesTitle":"","place":"","institution":"arXiv","date":"2021-11-14","pages":"","language":"","shortTitle":"","url":"http://arxiv.org/abs/2111.07243","accessDate":"2022-06-13T15:46:37Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2111.07243 [cs, stat]\ntype: article","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Computation","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-06-13T15:46:37Z","dateModified":"2022-06-13T15:46:37Z"}},{"key":"5D73ESD2","version":1176,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5D73ESD2","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5D73ESD2","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/R4BCUK3W","type":"application/json"}},"meta":{},"data":{"key":"5D73ESD2","version":1176,"parentItem":"R4BCUK3W","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-06-13T15:45:47Z","url":"https://arxiv.org/abs/2202.13460","note":"","contentType":"text/html","charset":"utf-8","filename":"2202.html","md5":"560434be57018f070fffb92d9342df6e","mtime":1655135147000,"tags":[],"relations":{},"dateAdded":"2022-06-13T15:45:47Z","dateModified":"2022-06-13T15:45:47Z"}},{"key":"JGNL7H5P","version":1176,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JGNL7H5P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JGNL7H5P","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/R4BCUK3W","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JGNL7H5P","version":1176,"parentItem":"R4BCUK3W","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-06-13T15:45:42Z","url":"https://arxiv.org/pdf/2202.13460.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Shi et al. - 2022 - Conditional Simulation Using Diffusion Schroding.pdf","md5":"ae543f6b762a8271112b188593d0ea99","mtime":1655135142000,"tags":[],"relations":{},"dateAdded":"2022-06-13T15:45:42Z","dateModified":"2022-06-13T15:45:42Z"}},{"key":"R4BCUK3W","version":1172,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/R4BCUK3W","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/R4BCUK3W","type":"text/html"}},"meta":{"creatorSummary":"Shi et al.","parsedDate":"2022-02-27","numChildren":2},"data":{"key":"R4BCUK3W","version":1172,"itemType":"report","title":"Conditional Simulation Using Diffusion Schr\\\"odinger Bridges","creators":[{"creatorType":"author","firstName":"Yuyang","lastName":"Shi"},{"creatorType":"author","firstName":"Valentin","lastName":"De Bortoli"},{"creatorType":"author","firstName":"George","lastName":"Deligiannidis"},{"creatorType":"author","firstName":"Arnaud","lastName":"Doucet"}],"abstractNote":"Denoising diffusion models have recently emerged as a powerful class of generative models. They provide state-of-the-art results, not only for unconditional simulation, but also when used to solve conditional simulation problems arising in a wide range of inverse problems such as image inpainting or deblurring. A limitation of these models is that they are computationally intensive at generation time as they require simulating a diffusion process over a long time horizon. When performing unconditional simulation, a Schr\\\"odinger bridge formulation of generative modeling leads to a theoretically grounded algorithm shortening generation time which is complementary to other proposed acceleration techniques. We extend here the Schr\\\"odinger bridge framework to conditional simulation. We demonstrate this novel methodology on various applications including image super-resolution and optimal filtering for state-space models.","reportNumber":"arXiv:2202.13460","reportType":"","seriesTitle":"","place":"","institution":"arXiv","date":"2022-02-27","pages":"","language":"","shortTitle":"","url":"http://arxiv.org/abs/2202.13460","accessDate":"2022-06-13T15:44:55Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2202.13460 [cs, stat]\ntype: article","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-06-13T15:44:55Z","dateModified":"2022-06-13T15:44:55Z"}},{"key":"BK3SB55M","version":1172,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/BK3SB55M","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/BK3SB55M","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IE4Z6NWF","type":"application/json"}},"meta":{},"data":{"key":"BK3SB55M","version":1172,"parentItem":"IE4Z6NWF","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-06-13T15:44:39Z","url":"https://arxiv.org/abs/2205.13699","note":"","contentType":"text/html","charset":"utf-8","filename":"2205.html","md5":"ebf8bd25e49e476b7b784d7a4e9dd154","mtime":1655135079000,"tags":[],"relations":{},"dateAdded":"2022-06-13T15:44:39Z","dateModified":"2022-06-13T15:44:39Z"}},{"key":"DTKEFQVD","version":1172,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/DTKEFQVD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/DTKEFQVD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/IE4Z6NWF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"DTKEFQVD","version":1172,"parentItem":"IE4Z6NWF","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-06-13T15:44:33Z","url":"https://arxiv.org/pdf/2205.13699.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kim et al. - 2022 - Maximum Likelihood Training of Implicit Nonlinear .pdf","md5":"87b935292aea1067d80efc42f892ab5c","mtime":1655135073000,"tags":[],"relations":{},"dateAdded":"2022-06-13T15:44:33Z","dateModified":"2022-06-13T15:44:33Z"}},{"key":"IE4Z6NWF","version":1169,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/IE4Z6NWF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/IE4Z6NWF","type":"text/html"}},"meta":{"creatorSummary":"Kim et al.","parsedDate":"2022-05-26","numChildren":2},"data":{"key":"IE4Z6NWF","version":1169,"itemType":"report","title":"Maximum Likelihood Training of Implicit Nonlinear Diffusion Models","creators":[{"creatorType":"author","firstName":"Dongjun","lastName":"Kim"},{"creatorType":"author","firstName":"Byeonghu","lastName":"Na"},{"creatorType":"author","firstName":"Se Jung","lastName":"Kwon"},{"creatorType":"author","firstName":"Dongsoo","lastName":"Lee"},{"creatorType":"author","firstName":"Wanmo","lastName":"Kang"},{"creatorType":"author","firstName":"Il-Chul","lastName":"Moon"}],"abstractNote":"Whereas diverse variations of diffusion models exist, expanding the linear diffusion into a nonlinear diffusion process is investigated only by a few works. The nonlinearity effect has been hardly understood, but intuitively, there would be more promising diffusion patterns to optimally train the generative distribution towards the data distribution. This paper introduces such a data-adaptive and nonlinear diffusion process for score-based diffusion models. The proposed Implicit Nonlinear Diffusion Model (INDM) learns the nonlinear diffusion process by combining a normalizing flow and a diffusion process. Specifically, INDM implicitly constructs a nonlinear diffusion on the \\textit{data space} by leveraging a linear diffusion on the \\textit{latent space} through a flow network. This flow network is the key to forming a nonlinear diffusion as the nonlinearity fully depends on the flow network. This flexible nonlinearity is what improves the learning curve of INDM to nearly MLE training, compared against the non-MLE training of DDPM++, which turns out to be a special case of INDM with the identity flow. Also, training the nonlinear diffusion empirically yields a sampling-friendly latent diffusion that the sample trajectory of INDM is closer to an optimal transport than the trajectories of previous research. In experiments, INDM achieves the state-of-the-art FID on CelebA.","reportNumber":"arXiv:2205.13699","reportType":"","seriesTitle":"","place":"","institution":"arXiv","date":"2022-05-26","pages":"","language":"","shortTitle":"","url":"http://arxiv.org/abs/2205.13699","accessDate":"2022-06-13T15:43:16Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2205.13699 [cs]\ntype: article","tags":[{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-06-13T15:43:16Z","dateModified":"2022-06-13T15:43:16Z"}},{"key":"UNI2TGUD","version":1172,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/UNI2TGUD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/UNI2TGUD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LN6BUNS7","type":"application/json"}},"meta":{},"data":{"key":"UNI2TGUD","version":1172,"parentItem":"LN6BUNS7","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-06-13T15:43:08Z","url":"https://arxiv.org/abs/2202.05722","note":"","contentType":"text/html","charset":"utf-8","filename":"2202.html","md5":"31345a2b49e6f8cb1a885c9bbe7b1942","mtime":1655134988000,"tags":[],"relations":{},"dateAdded":"2022-06-13T15:43:08Z","dateModified":"2022-06-13T15:43:08Z"}},{"key":"5WZR3ECU","version":1172,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/5WZR3ECU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/5WZR3ECU","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/LN6BUNS7","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"5WZR3ECU","version":1172,"parentItem":"LN6BUNS7","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-06-13T15:43:01Z","url":"https://arxiv.org/pdf/2202.05722.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Bunne et al. - 2022 - Recovering Stochastic Dynamics via Gaussian Schr.pdf","md5":"62abfc50b18e4468f3099fe6fd0c0189","mtime":1655134981000,"tags":[],"relations":{},"dateAdded":"2022-06-13T15:43:01Z","dateModified":"2022-06-13T15:43:01Z"}},{"key":"LN6BUNS7","version":1167,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/LN6BUNS7","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/LN6BUNS7","type":"text/html"}},"meta":{"creatorSummary":"Bunne et al.","parsedDate":"2022-02-11","numChildren":2},"data":{"key":"LN6BUNS7","version":1167,"itemType":"report","title":"Recovering Stochastic Dynamics via Gaussian Schr\\\"odinger Bridges","creators":[{"creatorType":"author","firstName":"Charlotte","lastName":"Bunne"},{"creatorType":"author","firstName":"Ya-Ping","lastName":"Hsieh"},{"creatorType":"author","firstName":"Marco","lastName":"Cuturi"},{"creatorType":"author","firstName":"Andreas","lastName":"Krause"}],"abstractNote":"We propose a new framework to reconstruct a stochastic process $\\left\\{\\mathbb{P}_{t}: t \\in[0, T]\\right\\}$ using only samples from its marginal distributions, observed at start and end times $0$ and $T$. This reconstruction is useful to infer population dynamics, a crucial challenge, e.g., when modeling the time-evolution of cell populations from single-cell sequencing data. Our general framework encompasses the more specific Schr\\\"odinger bridge (SB) problem, where $\\mathbb{P}_{t}$ represents the evolution of a thermodynamic system at almost equilibrium. Estimating such bridges is notoriously difficult, motivating our proposal for a novel adaptive scheme called the GSBflow. Our goal is to rely on Gaussian approximations of the data to provide the reference stochastic process needed to estimate SB. To that end, we solve the \\acs{SB} problem with Gaussian marginals, for which we provide, as a central contribution, a closed-form solution and SDE-representation. We use these formulas to define the reference process used to estimate more complex SBs, and show that this does indeed help with its numerical solution. We obtain notable improvements when reconstructing both synthetic processes and single-cell genomics experiments.","reportNumber":"arXiv:2202.05722","reportType":"","seriesTitle":"","place":"","institution":"arXiv","date":"2022-02-11","pages":"","language":"","shortTitle":"","url":"http://arxiv.org/abs/2202.05722","accessDate":"2022-06-13T15:42:35Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2202.05722 [cs, q-bio]\ntype: article","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Quantitative Biology - Quantitative Methods","type":1}],"collections":[],"relations":{},"dateAdded":"2022-06-13T15:42:35Z","dateModified":"2022-06-13T15:42:35Z"}},{"key":"J2KC9ZTS","version":1161,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/J2KC9ZTS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/J2KC9ZTS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ACIHCV9H","type":"application/json"}},"meta":{},"data":{"key":"J2KC9ZTS","version":1161,"parentItem":"ACIHCV9H","itemType":"attachment","linkMode":"imported_url","title":"Snapshot","accessDate":"2022-05-31T14:43:31Z","url":"https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14503?casa_token=nEgVQwfn6VIAAAAA%3AoBZ9vamFIAfzMJ8baQRL7vbwmrfCIOM376utYL5IIvdPjcGeSBKeSbvQ13p8zkQw-0ySYk-mc4e-Sw","note":"","contentType":"text/html","charset":"utf-8","filename":"cgf.html","md5":"37f2ef3633ce658a515cd60749270708","mtime":1654008211000,"tags":[],"relations":{},"dateAdded":"2022-05-31T14:43:31Z","dateModified":"2022-05-31T14:43:31Z"}},{"key":"GFLPC8RN","version":1161,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/GFLPC8RN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/GFLPC8RN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/ACIHCV9H","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"GFLPC8RN","version":1161,"parentItem":"ACIHCV9H","itemType":"attachment","linkMode":"imported_url","title":"Full Text PDF","accessDate":"2022-05-31T14:43:26Z","url":"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cgf.14503","note":"","contentType":"application/pdf","charset":"","filename":"Bermano et al. - 2022 - State-of-the-Art in the Architecture, Methods and .pdf","md5":"ce8e6af52192277aafa96d017525ee16","mtime":1654008206000,"tags":[],"relations":{},"dateAdded":"2022-05-31T14:43:26Z","dateModified":"2022-05-31T14:43:26Z"}},{"key":"ACIHCV9H","version":1160,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/ACIHCV9H","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/ACIHCV9H","type":"text/html"}},"meta":{"creatorSummary":"Bermano et al.","parsedDate":"2022","numChildren":2},"data":{"key":"ACIHCV9H","version":1160,"itemType":"journalArticle","title":"State-of-the-Art in the Architecture, Methods and Applications of StyleGAN","creators":[{"creatorType":"author","firstName":"A.h.","lastName":"Bermano"},{"creatorType":"author","firstName":"R.","lastName":"Gal"},{"creatorType":"author","firstName":"Y.","lastName":"Alaluf"},{"creatorType":"author","firstName":"R.","lastName":"Mokady"},{"creatorType":"author","firstName":"Y.","lastName":"Nitzan"},{"creatorType":"author","firstName":"O.","lastName":"Tov"},{"creatorType":"author","firstName":"O.","lastName":"Patashnik"},{"creatorType":"author","firstName":"D.","lastName":"Cohen-Or"}],"abstractNote":"Generative Adversarial Networks (GANs) have established themselves as a prevalent approach to image synthesis. Of these, StyleGAN offers a fascinating case study, owing to its remarkable visual quality and an ability to support a large array of downstream tasks. This state-of-the-art report covers the StyleGAN architecture, and the ways it has been employed since its conception, while also analyzing its severe limitations. It aims to be of use for both newcomers, who wish to get a grasp of the field, and for more experienced readers that might benefit from seeing current research trends and existing tools laid out. Among StyleGAN's most interesting aspects is its learned latent space. Despite being learned with no supervision, it is surprisingly well-behaved and remarkably disentangled. Combined with StyleGAN's visual quality, these properties gave rise to unparalleled editing capabilities. However, the control offered by StyleGAN is inherently limited to the generator's learned distribution, and can only be applied to images generated by StyleGAN itself. Seeking to bring StyleGAN's latent control to real-world scenarios, the study of GAN inversion and latent space embedding has quickly gained in popularity. Meanwhile, this same study has helped shed light on the inner workings and limitations of StyleGAN. We map out StyleGAN's impressive story through these investigations, and discuss the details that have made StyleGAN the go-to generator. We further elaborate on the visual priors StyleGAN constructs, and discuss their use in downstream discriminative tasks. Looking forward, we point out StyleGAN's limitations and speculate on current trends and promising directions for future research, such as task and target specific fine-tuning.","publicationTitle":"Computer Graphics Forum","volume":"41","issue":"2","pages":"591-611","date":"2022","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"10.1111/cgf.14503","ISSN":"1467-8659","shortTitle":"","url":"https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14503","accessDate":"2022-05-31T14:43:23Z","archive":"","archiveLocation":"","libraryCatalog":"Wiley Online Library","callNumber":"","rights":"","extra":"_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14503","tags":[{"tag":"CCS Concepts","type":1},{"tag":"Computer graphics","type":1},{"tag":"Image manipulation","type":1},{"tag":"Neural networks","type":1},{"tag":"• Computing methodologies → Learning latent representations","type":1}],"collections":[],"relations":{},"dateAdded":"2022-05-31T14:43:23Z","dateModified":"2022-05-31T14:43:23Z"}},{"key":"29NMKTJS","version":1158,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/29NMKTJS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/29NMKTJS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/X6VV792Y","type":"application/json"}},"meta":{},"data":{"key":"29NMKTJS","version":1158,"parentItem":"X6VV792Y","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-05-26T04:47:18Z","url":"https://arxiv.org/abs/2205.11916","note":"","contentType":"text/html","charset":"utf-8","filename":"2205.html","md5":"28f856ea23d6e1733a6e0d436bb050af","mtime":1653540438000,"tags":[],"relations":{},"dateAdded":"2022-05-26T04:47:18Z","dateModified":"2022-05-26T04:47:18Z"}},{"key":"XYA5ISPN","version":1158,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/XYA5ISPN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/XYA5ISPN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/X6VV792Y","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"XYA5ISPN","version":1158,"parentItem":"X6VV792Y","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-05-26T04:47:11Z","url":"https://arxiv.org/pdf/2205.11916.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Kojima et al. - 2022 - Large Language Models are Zero-Shot Reasoners.pdf","md5":"490cc7d5fd9384d466a7c447bcfd78a8","mtime":1653540431000,"tags":[],"relations":{},"dateAdded":"2022-05-26T04:47:11Z","dateModified":"2022-05-26T04:47:11Z"}},{"key":"X6VV792Y","version":1156,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/X6VV792Y","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/X6VV792Y","type":"text/html"}},"meta":{"creatorSummary":"Kojima et al.","parsedDate":"2022-05-24","numChildren":2},"data":{"key":"X6VV792Y","version":1156,"itemType":"journalArticle","title":"Large Language Models are Zero-Shot Reasoners","creators":[{"creatorType":"author","firstName":"Takeshi","lastName":"Kojima"},{"creatorType":"author","firstName":"Shixiang Shane","lastName":"Gu"},{"creatorType":"author","firstName":"Machel","lastName":"Reid"},{"creatorType":"author","firstName":"Yutaka","lastName":"Matsuo"},{"creatorType":"author","firstName":"Yusuke","lastName":"Iwasawa"}],"abstractNote":"Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding ``Let's think step by step'' before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with an off-the-shelf 175B parameter model. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted through simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.","publicationTitle":"arXiv:2205.11916 [cs]","volume":"","issue":"","pages":"","date":"2022-05-24","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"","ISSN":"","shortTitle":"","url":"http://arxiv.org/abs/2205.11916","accessDate":"2022-05-26T04:47:05Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv: 2205.11916","tags":[{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-05-26T04:47:05Z","dateModified":"2022-05-26T04:47:05Z"}},{"key":"KF2EM4HR","version":1221,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/KF2EM4HR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/KF2EM4HR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9L3XB3NI","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"KF2EM4HR","version":1221,"parentItem":"9L3XB3NI","itemType":"attachment","linkMode":"imported_url","title":"Full Text PDF","accessDate":"2022-07-18T06:54:10Z","url":"https://arxiv.org/pdf/2112.01523","note":"","contentType":"application/pdf","charset":"","filename":"Attal et al. - 2021 - Learning Neural Light Fields with Ray-Space Embedd.pdf","md5":"fb0e8f997e573fa6c8602a1967ca43a4","mtime":1658127250000,"tags":[],"relations":{},"dateAdded":"2022-07-18T06:54:10Z","dateModified":"2022-07-18T06:54:10Z"}},{"key":"3WNY2UKN","version":1218,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/3WNY2UKN","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/3WNY2UKN","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9L3XB3NI","type":"application/json"}},"meta":{},"data":{"key":"3WNY2UKN","version":1218,"parentItem":"9L3XB3NI","itemType":"attachment","linkMode":"imported_url","title":"Snapshot","accessDate":"2022-07-18T06:53:24Z","url":"https://arxiv.org/abs/2112.01523","note":"","contentType":"text/html","charset":"utf-8","filename":"2112.html","md5":"fa170dc7daf360edbf1aed5ec2cd617f","mtime":1658127204000,"tags":[],"relations":{},"dateAdded":"2022-07-18T06:53:24Z","dateModified":"2022-07-18T06:53:24Z"}},{"key":"9L3XB3NI","version":1217,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9L3XB3NI","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9L3XB3NI","type":"text/html"}},"meta":{"creatorSummary":"Attal et al.","parsedDate":"2021-12-02","numChildren":2},"data":{"key":"9L3XB3NI","version":1217,"itemType":"journalArticle","title":"Learning Neural Light Fields with Ray-Space Embedding Networks","creators":[{"creatorType":"author","firstName":"Benjamin","lastName":"Attal"},{"creatorType":"author","firstName":"Jia-Bin","lastName":"Huang"},{"creatorType":"author","firstName":"Michael","lastName":"Zollhoefer"},{"creatorType":"author","firstName":"Johannes","lastName":"Kopf"},{"creatorType":"author","firstName":"Changil","lastName":"Kim"}],"abstractNote":"Neural radiance fields (NeRFs) produce state-of-the-art view synthesis results. However, they are slow to render, requiring hundreds of network evaluations per pixel to approximate a volume rendering integral. Baking NeRFs into explicit data structures enables efficient rendering, but results in a large increase in memory footprint and, in many cases, a quality reduction. In this paper, we propose a novel neural light field representation that, in contrast, is compact and directly predicts integrated radiance along rays. Our method supports rendering with a single network evaluation per pixel for small baseline light field datasets and can also be applied to larger baselines with only a few evaluations per pixel. At the core of our approach is a ray-space embedding network that maps the 4D ray-space manifold into an intermediate, interpolable latent space. Our method achieves state-of-the-art quality on dense forward-facing datasets such as the Stanford Light Field dataset. In addition, for forward-facing scenes with sparser inputs we achieve results that are competitive with NeRF-based approaches in terms of quality while providing a better speed/quality/memory trade-off with far fewer network evaluations.","publicationTitle":"","volume":"","issue":"","pages":"","date":"2021/12/02","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"10.48550/arXiv.2112.01523","ISSN":"","shortTitle":"","url":"https://arxiv.org/abs/2112.01523v3","accessDate":"2022-07-18T06:53:20Z","archive":"","archiveLocation":"","libraryCatalog":"arxiv.org","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2022-07-18T06:53:20Z","dateModified":"2022-07-18T06:53:20Z"}},{"key":"RQ9SI9RS","version":1215,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/RQ9SI9RS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/RQ9SI9RS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/685J4EA6","type":"application/json"}},"meta":{},"data":{"key":"RQ9SI9RS","version":1215,"parentItem":"685J4EA6","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-07-18T06:52:14Z","url":"https://arxiv.org/abs/2105.07112","note":"","contentType":"text/html","charset":"utf-8","filename":"2105.html","md5":"2db0360d8e927011f7b0719f8da6841b","mtime":1658127134000,"tags":[],"relations":{},"dateAdded":"2022-07-18T06:52:14Z","dateModified":"2022-07-18T06:52:14Z"}},{"key":"X8GA28EA","version":1215,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/X8GA28EA","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/X8GA28EA","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/685J4EA6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"X8GA28EA","version":1215,"parentItem":"685J4EA6","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-07-18T06:52:06Z","url":"https://arxiv.org/pdf/2105.07112.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Li et al. - 2022 - NeuLF Efficient Novel View Synthesis with Neural .pdf","md5":"16c649e86f0a5066f98008e8445b0424","mtime":1658127126000,"tags":[],"relations":{},"dateAdded":"2022-07-18T06:52:06Z","dateModified":"2022-07-18T06:52:06Z"}},{"key":"48C3CCMR","version":1212,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/48C3CCMR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/48C3CCMR","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/685J4EA6","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"48C3CCMR","version":1212,"parentItem":"685J4EA6","itemType":"note","note":"Comment: get accepted by EGSR 2022","tags":[],"relations":{},"dateAdded":"2022-07-18T06:50:52Z","dateModified":"2022-07-18T06:50:52Z"}},{"key":"685J4EA6","version":1212,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/685J4EA6","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/685J4EA6","type":"text/html"}},"meta":{"creatorSummary":"Li et al.","parsedDate":"2022","numChildren":3},"data":{"key":"685J4EA6","version":1212,"itemType":"journalArticle","title":"NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field","creators":[{"creatorType":"author","firstName":"Zhong","lastName":"Li"},{"creatorType":"author","firstName":"Liangchen","lastName":"Song"},{"creatorType":"author","firstName":"Celong","lastName":"Liu"},{"creatorType":"author","firstName":"Junsong","lastName":"Yuan"},{"creatorType":"author","firstName":"Yi","lastName":"Xu"}],"abstractNote":"In this paper, we present an efficient and robust deep learning solution for novel view synthesis of complex scenes. In our approach, a 3D scene is represented as a light field, i.e., a set of rays, each of which has a corresponding color when reaching the image plane. For efficient novel view rendering, we adopt a two-plane parameterization of the light field, where each ray is characterized by a 4D parameter. We then formulate the light field as a 4D function that maps 4D coordinates to corresponding color values. We train a deep fully connected network to optimize this implicit function and memorize the 3D scene. Then, the scene-specific model is used to synthesize novel views. Different from previous light field approaches which require dense view sampling to reliably render novel views, our method can render novel views by sampling rays and querying the color for each ray from the network directly, thus enabling high-quality light field rendering with a sparser set of training images. Per-ray depth can be optionally predicted by the network, thus enabling applications such as auto refocus. Our novel view synthesis results are comparable to the state-of-the-arts, and even superior in some challenging scenes with refraction and reflection. We achieve this while maintaining an interactive frame rate and a small memory footprint.","publicationTitle":"","volume":"","issue":"","pages":"11 pages","date":"2022","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"","DOI":"10.2312/sr.20221156","ISSN":"","shortTitle":"NeuLF","url":"http://arxiv.org/abs/2105.07112","accessDate":"2022-07-18T06:50:52Z","archive":"","archiveLocation":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2105.07112 [cs]","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Graphics","type":1}],"collections":[],"relations":{},"dateAdded":"2022-07-18T06:50:52Z","dateModified":"2022-07-18T06:50:52Z"}},{"key":"HPWET3PP","version":1224,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HPWET3PP","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HPWET3PP","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PQIWW6ZU","type":"application/json"}},"meta":{},"data":{"key":"HPWET3PP","version":1224,"parentItem":"PQIWW6ZU","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-07-18T08:53:51Z","url":"https://arxiv.org/abs/2006.10739","note":"","contentType":"text/html","charset":"utf-8","filename":"2006.html","md5":"4c2a70b8efe26ac2496776e0a45b2838","mtime":1658134429000,"tags":[],"relations":{},"dateAdded":"2022-07-18T08:53:51Z","dateModified":"2022-07-18T08:53:51Z"}},{"key":"FEL7DFR3","version":1224,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/FEL7DFR3","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/FEL7DFR3","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PQIWW6ZU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"FEL7DFR3","version":1224,"parentItem":"PQIWW6ZU","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-07-18T08:53:37Z","url":"https://arxiv.org/pdf/2006.10739.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Tancik et al. - 2020 - Fourier Features Let Networks Learn High Frequency.pdf","md5":"484216db4d5573cbcc9852a4ed313e8f","mtime":1658134417000,"tags":[],"relations":{},"dateAdded":"2022-07-18T08:53:37Z","dateModified":"2022-07-18T08:53:37Z"}},{"key":"JMH8DL2K","version":1223,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/JMH8DL2K","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/JMH8DL2K","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/PQIWW6ZU","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"JMH8DL2K","version":1223,"parentItem":"PQIWW6ZU","itemType":"note","note":"Comment: Project page: https://people.eecs.berkeley.edu/~bmild/fourfeat/","tags":[],"relations":{},"dateAdded":"2022-07-18T08:52:41Z","dateModified":"2022-07-18T08:52:41Z"}},{"key":"PQIWW6ZU","version":1223,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PQIWW6ZU","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PQIWW6ZU","type":"text/html"}},"meta":{"creatorSummary":"Tancik et al.","parsedDate":"2020-06-18","numChildren":3},"data":{"key":"PQIWW6ZU","version":1223,"itemType":"preprint","title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains","creators":[{"creatorType":"author","firstName":"Matthew","lastName":"Tancik"},{"creatorType":"author","firstName":"Pratul P.","lastName":"Srinivasan"},{"creatorType":"author","firstName":"Ben","lastName":"Mildenhall"},{"creatorType":"author","firstName":"Sara","lastName":"Fridovich-Keil"},{"creatorType":"author","firstName":"Nithin","lastName":"Raghavan"},{"creatorType":"author","firstName":"Utkarsh","lastName":"Singhal"},{"creatorType":"author","firstName":"Ravi","lastName":"Ramamoorthi"},{"creatorType":"author","firstName":"Jonathan T.","lastName":"Barron"},{"creatorType":"author","firstName":"Ren","lastName":"Ng"}],"abstractNote":"We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.","genre":"","repository":"arXiv","archiveID":"arXiv:2006.10739","place":"","date":"2020-06-18","series":"","seriesNumber":"","DOI":"","citationKey":"","url":"http://arxiv.org/abs/2006.10739","accessDate":"2022-07-18T08:52:41Z","archive":"","archiveLocation":"","shortTitle":"","language":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2006.10739 [cs]","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-07-18T08:52:41Z","dateModified":"2022-07-18T08:52:41Z"}},{"key":"HBYA9STQ","version":1229,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HBYA9STQ","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HBYA9STQ","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4VTRIS8S","type":"application/json"}},"meta":{},"data":{"key":"HBYA9STQ","version":1229,"parentItem":"4VTRIS8S","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-07-19T02:16:33Z","url":"https://arxiv.org/abs/2207.07621","note":"","contentType":"text/html","charset":"utf-8","filename":"2207.html","md5":"eeb820651c637da5ebeb55341655f039","mtime":1658196993000,"tags":[],"relations":{},"dateAdded":"2022-07-19T02:16:33Z","dateModified":"2022-07-19T02:16:33Z"}},{"key":"TLPU8SHD","version":1229,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/TLPU8SHD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/TLPU8SHD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/4VTRIS8S","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"TLPU8SHD","version":1229,"parentItem":"4VTRIS8S","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-07-19T02:16:27Z","url":"https://arxiv.org/pdf/2207.07621.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Drobyshev et al. - 2022 - MegaPortraits One-shot Megapixel Neural Head Avat.pdf","md5":"3a421d63f9d9dc5d2cf2a2c3e78306b8","mtime":1658196987000,"tags":[],"relations":{},"dateAdded":"2022-07-19T02:16:27Z","dateModified":"2022-07-19T02:16:27Z"}},{"key":"4VTRIS8S","version":1226,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/4VTRIS8S","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/4VTRIS8S","type":"text/html"}},"meta":{"creatorSummary":"Drobyshev et al.","parsedDate":"2022-07-15","numChildren":2},"data":{"key":"4VTRIS8S","version":1226,"itemType":"preprint","title":"MegaPortraits: One-shot Megapixel Neural Head Avatars","creators":[{"creatorType":"author","firstName":"Nikita","lastName":"Drobyshev"},{"creatorType":"author","firstName":"Jenya","lastName":"Chelishev"},{"creatorType":"author","firstName":"Taras","lastName":"Khakhulin"},{"creatorType":"author","firstName":"Aleksei","lastName":"Ivakhnenko"},{"creatorType":"author","firstName":"Victor","lastName":"Lempitsky"},{"creatorType":"author","firstName":"Egor","lastName":"Zakharov"}],"abstractNote":"In this work, we advance the neural head avatar technology to the megapixel resolution while focusing on the particularly challenging task of cross-driving synthesis, i.e., when the appearance of the driving image is substantially different from the animated source image. We propose a set of new neural architectures and training methods that can leverage both medium-resolution video data and high-resolution image data to achieve the desired levels of rendered image quality and generalization to novel views and motion. We demonstrate that suggested architectures and methods produce convincing high-resolution neural avatars, outperforming the competitors in the cross-driving scenario. Lastly, we show how a trained high-resolution neural avatar model can be distilled into a lightweight student model which runs in real-time and locks the identities of neural avatars to several dozens of pre-defined source images. Real-time operation and identity lock are essential for many practical applications head avatar systems.","genre":"","repository":"arXiv","archiveID":"arXiv:2207.07621","place":"","date":"2022-07-15","series":"","seriesNumber":"","DOI":"10.48550/arXiv.2207.07621","citationKey":"","url":"http://arxiv.org/abs/2207.07621","accessDate":"2022-07-19T02:14:46Z","archive":"","archiveLocation":"","shortTitle":"MegaPortraits","language":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2207.07621 [cs]","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-07-19T02:14:46Z","dateModified":"2022-07-19T02:14:46Z"}},{"key":"AVVR373L","version":1239,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/AVVR373L","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/AVVR373L","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WTJ3VAXR","type":"application/json"}},"meta":{},"data":{"key":"AVVR373L","version":1239,"parentItem":"WTJ3VAXR","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-07-19T03:26:52Z","url":"https://arxiv.org/abs/2206.00927","note":"","contentType":"text/html","charset":"utf-8","filename":"2206.html","md5":"1981b52b6354b965599fe27bdd53658c","mtime":1658201212000,"tags":[],"relations":{},"dateAdded":"2022-07-19T03:26:52Z","dateModified":"2022-07-19T03:26:52Z"}},{"key":"38FZ5N8Z","version":1239,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/38FZ5N8Z","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/38FZ5N8Z","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QJSWEXRF","type":"application/json"}},"meta":{},"data":{"key":"38FZ5N8Z","version":1239,"parentItem":"QJSWEXRF","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-07-19T03:26:50Z","url":"https://arxiv.org/abs/2205.15463","note":"","contentType":"text/html","charset":"utf-8","filename":"2205.html","md5":"4019d819421644f24d3a5b8c39f41532","mtime":1658201210000,"tags":[],"relations":{},"dateAdded":"2022-07-19T03:26:50Z","dateModified":"2022-07-19T03:26:50Z"}},{"key":"NENZ26DG","version":1239,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/NENZ26DG","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/NENZ26DG","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/WTJ3VAXR","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"NENZ26DG","version":1239,"parentItem":"WTJ3VAXR","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-07-19T03:26:46Z","url":"https://arxiv.org/pdf/2206.00927.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Lu et al. - 2022 - DPM-Solver A Fast ODE Solver for Diffusion Probab.pdf","md5":"40b50d25eef3da71e69e0ccc225907ed","mtime":1658201206000,"tags":[],"relations":{},"dateAdded":"2022-07-19T03:26:46Z","dateModified":"2022-07-19T03:26:46Z"}},{"key":"8ESPAHVV","version":1238,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8ESPAHVV","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8ESPAHVV","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QJSWEXRF","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8ESPAHVV","version":1238,"parentItem":"QJSWEXRF","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-07-19T03:26:45Z","url":"https://arxiv.org/pdf/2205.15463.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Giannone et al. - 2022 - Few-Shot Diffusion Models.pdf","md5":"ba60b46b45e141cad83dfa3041662c62","mtime":1658201205000,"tags":[],"relations":{},"dateAdded":"2022-07-19T03:26:45Z","dateModified":"2022-07-19T03:26:45Z"}},{"key":"SAXGW5CD","version":1238,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SAXGW5CD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SAXGW5CD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E2NJLN7X","type":"application/json"}},"meta":{},"data":{"key":"SAXGW5CD","version":1238,"parentItem":"E2NJLN7X","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-07-19T03:26:44Z","url":"https://arxiv.org/abs/2204.11824","note":"","contentType":"text/html","charset":"utf-8","filename":"2204.html","md5":"9e4655d9bd5d24ee8dc512f91dd8a945","mtime":1658201204000,"tags":[],"relations":{},"dateAdded":"2022-07-19T03:26:44Z","dateModified":"2022-07-19T03:26:44Z"}},{"key":"SCZ9TSWB","version":1238,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/SCZ9TSWB","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/SCZ9TSWB","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E2NJLN7X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"SCZ9TSWB","version":1238,"parentItem":"E2NJLN7X","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-07-19T03:26:39Z","url":"https://arxiv.org/pdf/2204.11824.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Blattmann et al. - 2022 - Retrieval-Augmented Diffusion Models.pdf","md5":"1eb64ca0dd4724dcd141a9932972102b","mtime":1658201199000,"tags":[],"relations":{},"dateAdded":"2022-07-19T03:26:39Z","dateModified":"2022-07-19T03:26:39Z"}},{"key":"PT9AUNGD","version":1234,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/PT9AUNGD","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/PT9AUNGD","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9XR34DSM","type":"application/json"}},"meta":{},"data":{"key":"PT9AUNGD","version":1234,"parentItem":"9XR34DSM","itemType":"attachment","linkMode":"imported_url","title":"arXiv.org Snapshot","accessDate":"2022-07-19T03:25:50Z","url":"https://arxiv.org/abs/2207.06389","note":"","contentType":"text/html","charset":"utf-8","filename":"2207.html","md5":"d61545ef6df4948d9f4b7366a993bf0a","mtime":1658201150000,"tags":[],"relations":{},"dateAdded":"2022-07-19T03:25:50Z","dateModified":"2022-07-19T03:25:50Z"}},{"key":"YC9LJH9A","version":1233,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/YC9LJH9A","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/YC9LJH9A","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9XR34DSM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"YC9LJH9A","version":1233,"parentItem":"9XR34DSM","itemType":"attachment","linkMode":"imported_url","title":"arXiv Fulltext PDF","accessDate":"2022-07-19T03:25:44Z","url":"https://arxiv.org/pdf/2207.06389.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Huang et al. - 2022 - ProDiff Progressive Fast Diffusion Model For High.pdf","md5":"463344c60cfcb3bd85e3619e0dbc6df9","mtime":1658201144000,"tags":[],"relations":{},"dateAdded":"2022-07-19T03:25:44Z","dateModified":"2022-07-19T03:25:44Z"}},{"key":"8SE9B74Y","version":1233,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/8SE9B74Y","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/8SE9B74Y","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/QPSHIUPS","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"8SE9B74Y","version":1233,"parentItem":"QPSHIUPS","itemType":"attachment","linkMode":"imported_url","title":"Chung et al. - Come-Closer-Diffuse-Faster Accelerating Condition.pdf","accessDate":"2022-07-19T03:25:32Z","url":"https://openaccess.thecvf.com/content/CVPR2022/papers/Chung_Come-Closer-Diffuse-Faster_Accelerating_Conditional_Diffusion_Models_for_Inverse_Problems_Through_Stochastic_CVPR_2022_paper.pdf","note":"","contentType":"application/pdf","charset":"","filename":"Chung et al. - Come-Closer-Diffuse-Faster Accelerating Condition.pdf","md5":"9721db970044806c7be47bb30cd2ebeb","mtime":1658201134000,"tags":[],"relations":{},"dateAdded":"2022-07-19T03:25:32Z","dateModified":"2022-07-19T03:25:34Z"}},{"key":"QPSHIUPS","version":1232,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QPSHIUPS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QPSHIUPS","type":"text/html"}},"meta":{"creatorSummary":"Chung et al.","numChildren":1},"data":{"key":"QPSHIUPS","version":1232,"itemType":"journalArticle","title":"Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems Through Stochastic Contraction","creators":[{"creatorType":"author","firstName":"Hyungjin","lastName":"Chung"},{"creatorType":"author","firstName":"Byeongsu","lastName":"Sim"},{"creatorType":"author","firstName":"Jong Chul","lastName":"Ye"}],"abstractNote":"","publicationTitle":"","volume":"","issue":"","pages":"10","date":"","series":"","seriesTitle":"","seriesText":"","journalAbbreviation":"","language":"en","DOI":"","ISSN":"","shortTitle":"","url":"","accessDate":"","archive":"","archiveLocation":"","libraryCatalog":"Zotero","callNumber":"","rights":"","extra":"","tags":[],"collections":[],"relations":{},"dateAdded":"2022-07-19T03:25:34Z","dateModified":"2022-07-19T03:25:34Z"}},{"key":"QJSWEXRF","version":1232,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/QJSWEXRF","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/QJSWEXRF","type":"text/html"}},"meta":{"creatorSummary":"Giannone et al.","parsedDate":"2022-05-30","numChildren":2},"data":{"key":"QJSWEXRF","version":1232,"itemType":"preprint","title":"Few-Shot Diffusion Models","creators":[{"creatorType":"author","firstName":"Giorgio","lastName":"Giannone"},{"creatorType":"author","firstName":"Didrik","lastName":"Nielsen"},{"creatorType":"author","firstName":"Ole","lastName":"Winther"}],"abstractNote":"Denoising diffusion probabilistic models (DDPM) are powerful hierarchical latent variable models with remarkable sample generation quality and training stability. These properties can be attributed to parameter sharing in the generative hierarchy, as well as a parameter-free diffusion-based inference procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a framework for few-shot generation leveraging conditional DDPMs. FSDMs are trained to adapt the generative process conditioned on a small set of images from a given class by aggregating image patch information using a set-based Vision Transformer (ViT). At test time, the model is able to generate samples from previously unseen classes conditioned on as few as 5 samples from that class. We empirically show that FSDM can perform few-shot generation and transfer to new datasets. We benchmark variants of our method on complex vision datasets for few-shot learning and compare to unconditional and conditional DDPM baselines. Additionally, we show how conditioning the model on patch-based input set information improves training convergence.","genre":"","repository":"arXiv","archiveID":"arXiv:2205.15463","place":"","date":"2022-05-30","series":"","seriesNumber":"","DOI":"10.48550/arXiv.2205.15463","citationKey":"","url":"http://arxiv.org/abs/2205.15463","accessDate":"2022-07-19T03:25:30Z","archive":"","archiveLocation":"","shortTitle":"","language":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2205.15463 [cs, stat]","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-07-19T03:25:30Z","dateModified":"2022-07-19T03:25:30Z"}},{"key":"WTJ3VAXR","version":1231,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/WTJ3VAXR","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/WTJ3VAXR","type":"text/html"}},"meta":{"creatorSummary":"Lu et al.","parsedDate":"2022-06-02","numChildren":2},"data":{"key":"WTJ3VAXR","version":1231,"itemType":"preprint","title":"DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps","creators":[{"creatorType":"author","firstName":"Cheng","lastName":"Lu"},{"creatorType":"author","firstName":"Yuhao","lastName":"Zhou"},{"creatorType":"author","firstName":"Fan","lastName":"Bao"},{"creatorType":"author","firstName":"Jianfei","lastName":"Chen"},{"creatorType":"author","firstName":"Chongxuan","lastName":"Li"},{"creatorType":"author","firstName":"Jun","lastName":"Zhu"}],"abstractNote":"Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a $4\\sim 16\\times$ speedup compared with previous state-of-the-art training-free samplers on various datasets.","genre":"","repository":"arXiv","archiveID":"arXiv:2206.00927","place":"","date":"2022-06-02","series":"","seriesNumber":"","DOI":"10.48550/arXiv.2206.00927","citationKey":"","url":"http://arxiv.org/abs/2206.00927","accessDate":"2022-07-19T03:25:23Z","archive":"","archiveLocation":"","shortTitle":"DPM-Solver","language":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2206.00927 [cs, stat]","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":[],"relations":{},"dateAdded":"2022-07-19T03:25:23Z","dateModified":"2022-07-19T03:25:23Z"}},{"key":"E875Q4CS","version":1231,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/E875Q4CS","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/E875Q4CS","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/9XR34DSM","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"E875Q4CS","version":1231,"parentItem":"9XR34DSM","itemType":"note","note":"Comment: Accepted by ACM Multimedia 2022","tags":[],"relations":{},"dateAdded":"2022-07-19T03:25:22Z","dateModified":"2022-07-19T03:25:22Z"}},{"key":"9XR34DSM","version":1231,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/9XR34DSM","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/9XR34DSM","type":"text/html"}},"meta":{"creatorSummary":"Huang et al.","parsedDate":"2022-07-13","numChildren":3},"data":{"key":"9XR34DSM","version":1231,"itemType":"preprint","title":"ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech","creators":[{"creatorType":"author","firstName":"Rongjie","lastName":"Huang"},{"creatorType":"author","firstName":"Zhou","lastName":"Zhao"},{"creatorType":"author","firstName":"Huadai","lastName":"Liu"},{"creatorType":"author","firstName":"Jinglin","lastName":"Liu"},{"creatorType":"author","firstName":"Chenye","lastName":"Cui"},{"creatorType":"author","firstName":"Yi","lastName":"Ren"}],"abstractNote":"Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hinder their applications to text-to-speech deployment. Through the preliminary study on diffusion model parameterization, we find that previous gradient-based TTS models require hundreds or thousands of iterations to guarantee high sample quality, which poses a challenge for accelerating sampling. In this work, we propose ProDiff, on progressive fast diffusion model for high-quality text-to-speech. Unlike previous work estimating the gradient for data density, ProDiff parameterizes the denoising model by directly predicting clean data to avoid distinct quality degradation in accelerating sampling. To tackle the model convergence challenge with decreased diffusion iterations, ProDiff reduces the data variance in the target site via knowledge distillation. Specifically, the denoising model uses the generated mel-spectrogram from an N-step DDIM teacher as the training target and distills the behavior into a new model with N/2 steps. As such, it allows the TTS model to make sharp predictions and further reduces the sampling time by orders of magnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to synthesize high-fidelity mel-spectrograms, while it maintains sample quality and diversity competitive with state-of-the-art models using hundreds of steps. ProDiff enables a sampling speed of 24x faster than real-time on a single NVIDIA 2080Ti GPU, making diffusion models practically applicable to text-to-speech synthesis deployment for the first time. Our extensive ablation studies demonstrate that each design in ProDiff is effective, and we further show that ProDiff can be easily extended to the multi-speaker setting. Audio samples are available at \\url{https://ProDiff.github.io/.}","genre":"","repository":"arXiv","archiveID":"arXiv:2207.06389","place":"","date":"2022-07-13","series":"","seriesNumber":"","DOI":"10.48550/arXiv.2207.06389","citationKey":"","url":"http://arxiv.org/abs/2207.06389","accessDate":"2022-07-19T03:25:22Z","archive":"","archiveLocation":"","shortTitle":"ProDiff","language":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2207.06389 [cs, eess]","tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1}],"collections":[],"relations":{},"dateAdded":"2022-07-19T03:25:22Z","dateModified":"2022-07-19T03:25:22Z"}},{"key":"HJTYGB6P","version":1231,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/HJTYGB6P","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/HJTYGB6P","type":"text/html"},"up":{"href":"https://api.zotero.org/users/7902311/items/E2NJLN7X","type":"application/json"}},"meta":{"numChildren":0},"data":{"key":"HJTYGB6P","version":1231,"parentItem":"E2NJLN7X","itemType":"note","note":"Comment: Technical Report","tags":[],"relations":{},"dateAdded":"2022-07-19T03:25:20Z","dateModified":"2022-07-19T03:25:20Z"}},{"key":"E2NJLN7X","version":1231,"library":{"type":"user","id":7902311,"name":"supasorn","links":{"alternate":{"href":"https://www.zotero.org/supasorn","type":"text/html"}}},"links":{"self":{"href":"https://api.zotero.org/users/7902311/items/E2NJLN7X","type":"application/json"},"alternate":{"href":"https://www.zotero.org/supasorn/items/E2NJLN7X","type":"text/html"}},"meta":{"creatorSummary":"Blattmann et al.","parsedDate":"2022-04-26","numChildren":3},"data":{"key":"E2NJLN7X","version":1231,"itemType":"preprint","title":"Retrieval-Augmented Diffusion Models","creators":[{"creatorType":"author","firstName":"Andreas","lastName":"Blattmann"},{"creatorType":"author","firstName":"Robin","lastName":"Rombach"},{"creatorType":"author","firstName":"Kaan","lastName":"Oktay"},{"creatorType":"author","firstName":"Björn","lastName":"Ommer"}],"abstractNote":"Generative image synthesis with diffusion models has recently achieved excellent visual quality in several tasks such as text-based or class-conditional image synthesis. Much of this success is due to a dramatic increase in the computational capacity invested in training these models. This work presents an alternative approach: inspired by its successful application in natural language processing, we propose to complement the diffusion model with a retrieval-based approach and to introduce an explicit memory in the form of an external database. During training, our diffusion model is trained with similar visual features retrieved via CLIP and from the neighborhood of each training instance. By leveraging CLIP's joint image-text embedding space, our model achieves highly competitive performance on tasks for which it has not been explicitly trained, such as class-conditional or text-image synthesis, and can be conditioned on both text and image embeddings. Moreover, we can apply our approach to unconditional generation, where it achieves state-of-the-art performance. Our approach incurs low computational and memory overheads and is easy to implement. We discuss its relationship to concurrent work and will publish code and pretrained models soon.","genre":"","repository":"arXiv","archiveID":"arXiv:2204.11824","place":"","date":"2022-04-26","series":"","seriesNumber":"","DOI":"10.48550/arXiv.2204.11824","citationKey":"","url":"http://arxiv.org/abs/2204.11824","accessDate":"2022-07-19T03:25:20Z","archive":"","archiveLocation":"","shortTitle":"","language":"","libraryCatalog":"arXiv.org","callNumber":"","rights":"","extra":"arXiv:2204.11824 [cs]","tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":[],"relations":{},"dateAdded":"2022-07-19T03:25:20Z","dateModified":"2022-07-19T03:25:20Z"}}]